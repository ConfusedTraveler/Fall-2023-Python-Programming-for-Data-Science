{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe026a0-1dad-40ef-a202-791e9ab53609",
   "metadata": {},
   "source": [
    "# Lecture 26 - Deploying Projects to the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fe2db-d2a4-463d-86ad-d0c1aae1d7b2",
   "metadata": {},
   "source": [
    "[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2023-Python-Programming-for-Data-Science/blob/main/docs/Lectures/Theme_4-Model_Deployment/Lecture_26-Deploying_to_Cloud/Lecture_26-Deploying_to_Cloud.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2023-Python-Programming-for-Data-Science/blob/main/docs/Lectures/Theme_4-Model_Deployment/Lecture_26-Deploying_to_Cloud/Lecture_26-Deploying_to_Cloud.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b18e1-8cdf-4640-92ec-5eceafd13da0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c93718",
   "metadata": {},
   "source": [
    "- [26.1 Data Science using Cloud Computing](#26.1-data-science-using-cloud-computing)\n",
    "- [26.2 Introduction to Azure Machine Learning](#26.2-introduction-to-azure-machine-learning)\n",
    "    - [26.2.1 Azure Free Trial](#26.2.1-azure-free-trial)\n",
    "- [26.3 No-Code Azure ML](#26.3-no-code-azure-ml)\n",
    "    - [26.3.1 Creating a Workspace Resource](#26.3.1-creating-a-workspace-resource)\n",
    "    - [26.3.2 Using Azure ML Studio](#26.3.2-using-azure-ml-studio)\n",
    "    - [26.3.3 Loading the Dataset](#26.3.3-loading-the-dataset)\n",
    "    - [26.3.4 Creating a Compute Resource](#26.3.4-creating-a-compute-resource)\n",
    "    - [26.3.5 Training a Model with Auto ML](#26.3.5-training-a-model-with-auto-ml)\n",
    "    - [26.3.6 Deploying the Model](#26.3.6-deploying-the-model)\n",
    "    - [26.3.7 Consuming the Model](#26.3.7-consuming-the-model)\n",
    "- [26.4 Code-based Azure ML](#26.4-code-based-azure-ml)\n",
    "    - [26.4.1 Creating a Compute Resource](#26.4.1-creating-a-compute-resource)\n",
    "    - [26.4.2 Using Jupyter Notebooks in Azure ML](#26.4.2-using-jupyter-notebooks-in-azure-ml)\n",
    "    - [26.4.3 Training a Model](#26.4.3-training-a-model)\n",
    "    - [26.4.4 Deploying the Model](#26.4.4-deploying-the-model)\n",
    "    - [26.4.5 Consuming the Model](#26.4.5-consuming-the-model)\n",
    "- [26.5 Training Deep Learning Models with Azure ML](#26.5-training-deep-learning-models-with-azure-ml)   \n",
    "    - [26.5.1 Creating Workspace and Compute Resource](#26.5.1-creating-workspace-and-compute-resource)\n",
    "    - [26.5.2 Loading the Data and Defining the Model](#26.5.2-loading-the-data-and-defining-the-model)\n",
    "    - [26.5.3 Preparing Azure ML Job and Training the Model](#26.5.3-preparing-azure-ml-job-and-training-the-model)\n",
    "    - [26.5.4 Consuming the Model](#26.5.4-consuming-the-model)\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bed47f",
   "metadata": {},
   "source": [
    "## 26.1 Data Science using Cloud Computing <a id=\"26.1-data-science-using-cloud-computing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271810f7-6fc7-4f81-9349-7a0fea3ef7f8",
   "metadata": {},
   "source": [
    "**Cloud Computing**, also referred to as the Cloud, delivers services hosted over a network, which can include data analytics, storage, databases, networking, and other services. The service is often in the form of a *public cloud*  offered to the public over the Internet by cloud service providers, or it can be a *private cloud* that is owned by an organization that maintains the services on a private network. \n",
    "\n",
    "Public Cloud Computing services include Amazon Web Services, Google Cloud Platform, Microsoft Azure, IBM Cloud, and others.\n",
    "\n",
    "In general, Cloud Computing services can be categorized as:\n",
    "\n",
    "- *Infrastructure as a Service (IaaS)*: access to infrastructure, consisting of servers, virtual machines (VMs), storage, networks, or databases.\n",
    "- *Platform as a Service (PaaS)*: access to a platform for developing, testing, delivering, and managing software applications, using infrastructure managed by the provider. \n",
    "- *Software as a Service (SaaS)*: access to software applications, that are developed and managed by the provider using the provider's infrastructure. \n",
    "\n",
    "The advantages of using Cloud Computing include convenient access to latest computational resources (without the need to purchase hardware or software), access to structured environments (preinstalled libraries) for running tasks, pay for what you need only, ability to quickly scale projects, improved efficiency by relying on infrastructure hosted and managed by the cloud provider, etc.\n",
    "\n",
    "Cloud Computing is especially important for managing  Data Science projects, which often require access to GPUs and large compute resources, storing large amounts of data, access to databases, deploying solutions for access by end-users, and similar. In addition, most Cloud providers have developed some form of AutoML tools that enable organizations without Data Science expertise to implement data analytics workflows into their projects. \n",
    "\n",
    "This lecture is primarily based on a course [Data Science for Beginners](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/5-Data-Science-In-Cloud/18-Low-Code/README.md) by Microsoft. The course has several lectures on deploying Data Science projects to the Cloud, as well as it has other lectures on Data Science in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbf9c2",
   "metadata": {},
   "source": [
    "## 26.2 Introduction to Azure Machine Learning <a id=\"26.2-introduction-to-azure-machine-learning\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db493fb",
   "metadata": {},
   "source": [
    "**Microsoft's Azure Machine Learning** is a cloud platform that provides a large number of products and services designed for handling various phases of Data Science projects. This includes capabilities for preparing and preprocessing data, training models, deploying models, and monitoring models in production. These capabilities can help to increase the efficiency of data scientists by automating many tasks and project pipelines. Understandably, the availability of cloud computing resources allows to easily scale projects and handle efficiently challenges related to processing big data and serving large number of customers. \n",
    "\n",
    "Important tools and services provided by Azure ML include:\n",
    "\n",
    "- *Azure Machine Learning Studio*: no-code framework for data engineering, model training, and deployment. \n",
    "- *Azure Machine Learning Designer*: low-code ML framework that allows to drag-and-drop modules for building data science pipelines.\n",
    "- *Azure Machine Learning SDK*: code-based environment for data science projects. \n",
    "- *Data Labelling*: tools for automatic data labeling.\n",
    "- *Machine Learning CLI (Command-Line Interface)*: allows managing Azure ML resources from the command line.\n",
    "- *Automated Machine Learning (AutoML) User Interface*: tools to automate tasks in data science projects. \n",
    "- *MLflow*: framework for tracking the performance of deployed models, and logging metrics and relevant indicators. \n",
    "\n",
    "Azure ML allows using Jupyter Notebooks and has built-in integration with popular ML libraries like Scikit-Learn, TensorFlow, PyTorch, and others.\n",
    "\n",
    "In this lecture, we will explore the different levels of functionality of Azure ML, ranging from the no-code AutoML, to full-code SDK, and working with our own custom models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0960c63",
   "metadata": {},
   "source": [
    "### 26.2.1 Azure Free Trial <a id=\"26.2.1-azure-free-trial\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bba6b6",
   "metadata": {},
   "source": [
    "Microsoft Azure has 30 days of free trial, which also can come with $200 Azure credit that can be used within the 30 days. \n",
    "\n",
    "Also, Azure offers $100 yearly Azure credit to students. \n",
    "\n",
    "In addition, the other Cloud providers typically offer some amount of credit to new users and students.\n",
    "\n",
    "Follow the link to the [Microsoft Azure webpage](https://azure.microsoft.com/en-us/free/) and select the `Start free` button. This will prompt you to create an Azure account, and if you wish you can use your University of Idaho account to get access to Azure. \n",
    "\n",
    "<img src=\"images/Azure_free_trial.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e898607",
   "metadata": {},
   "source": [
    "Once you create an account and get the subscription with $200 Azure credits, the home page should look similar to the following.\n",
    "\n",
    "<img src=\"images/Azure_homepage.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1dedc1-7725-4b23-8c89-d789b59772e0",
   "metadata": {},
   "source": [
    "## 26.3 No-Code Azure ML <a id=\"26.3-no-code-azure-ml\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a5e50",
   "metadata": {},
   "source": [
    "### 26.3.1 Creating a Workspace Resource<a id=\"26.3.1-creating-a-workspace-resource\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7cbd1",
   "metadata": {},
   "source": [
    "From the home page, we will need to first create a new **Resource** that will indicate what type of tools and services we will be using. \n",
    "\n",
    "- Select `+ Create a resource`.\n",
    "\n",
    "<img src=\"images/Create_resource.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11a1ab-a88e-4df5-8d52-fff3a1837d4a",
   "metadata": {},
   "source": [
    "Azure will next display many popular services and resources.\n",
    "\n",
    "- In the search box write `Azure Machine Learning` and select it.\n",
    "\n",
    "<img src=\"images/Resource_search.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07222355-e1ed-4feb-bae7-5caadbd6318c",
   "metadata": {},
   "source": [
    "This will load the web page of Azure Machine Learning. \n",
    "\n",
    "- Select `Create`. \n",
    "\n",
    "It will open a new page for `Azure ML Workspace resource`. The **Workspace** provides a place to work with machine learning models, and allows access to tools for training and deploying models. For instance, the Workspace will store information about training runs, such as logs of various metrics, it will provide access to the data and scripts, etc. And note also that when we are done with using Azure resources such as workspaces, we need to delete the resources, otherwise some costs can be incurred (e.g., even if we don't use the workspace to run a model, Azure may charge a fee for storing the data).\n",
    "\n",
    "<img src=\"images/Create_workspace.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ceaef",
   "metadata": {},
   "source": [
    "When we create a new Workspace, we need to fill in the information shown in the screenshot below. \n",
    "\n",
    "- Subscription: Azure subscription, e.g., the $200 Azure credits obtained with the free trial. \n",
    "- Resource group: Assign a name for the resource group.\n",
    "- Workspace name: Assign a name for the workspace (e.g., perhaps a name that is related to the project).\n",
    "- Region: Select the geographical region.\n",
    "- Storage account: A new storage account will be created for the workspace for storing the data.\n",
    "- Key vault: A new key vault will be created for the workspace for storing sensitive information.\n",
    "- Application insights: A new application insights resource will be created for the workspace to store information about deployed models.\n",
    "- Container registry: Leave it as None (it will be created automatically the first time the model is deployed).\n",
    "\n",
    "After the information in all fields is entered, select `Review & create`. \n",
    "\n",
    "<img src=\"images/Workspace_form.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958001f2-75eb-4c92-afca-689510ac3586",
   "metadata": {},
   "source": [
    "The next page will show the information that we entered and we will need to confirm that everything is correct.\n",
    "\n",
    "- Select `Create`.\n",
    "\n",
    "<img src=\"images/Workspace_confirm.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ace3e2-c832-4058-8ea0-d1d1eb20f2ff",
   "metadata": {},
   "source": [
    "It may take a few minutes for the Workspace to be created. Once it is ready, the page will show that it is completed. \n",
    "\n",
    "<img src=\"images/Workspace_complete.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac8375-b80c-4e6d-83e7-c92ad3407b90",
   "metadata": {},
   "source": [
    "### 26.3.2 Using Azure ML Studio<a id=\"26.3.2-using-azure-ml-studio\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d8a53-4230-4a12-9055-0f81a30dbc63",
   "metadata": {},
   "source": [
    "As we mentioned in the introductory section, **Azure Machine Learning Studio** is a no-code framework for data engineering, model training, and deployment. \n",
    "\n",
    "- Click on the following [link](https://ml.azure.com) to navigate to Azure ML Studio. \n",
    "\n",
    "The interface of Azure ML Studio is shown below. On the top of the page, our workspace should be listed. In this case, the workspace that we just created and named `My_workspace_1` is shown.\n",
    "\n",
    "<img src=\"images/ML_Studio.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55d6ec-871f-4487-acbb-1beb6d05785d",
   "metadata": {},
   "source": [
    "The various modules that are available in Azure ML Studio are listed in the left-side menu. To see the names of the modules, click on the three horizontal lines in the upper left corner. A brief description of the modules is shown in the next figure. The modules allow to conveniently apply tools for managing different phases of data science projects from a single place. \n",
    "\n",
    "<img src=\"images/ML_modules.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db941ecf",
   "metadata": {},
   "source": [
    "### 26.3.3 Loading the Dataset<a id=\"26.3.3-loading-the-dataset\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bb07e",
   "metadata": {},
   "source": [
    "For demonstration purposes, we will use the Heart Failure Dataset, which is publicly available, and contains 13 columns with information about 300 patients who may or may not have risk of heart failure. The csv file with the records is available in the `data` folder with the other files for this lecture.\n",
    "\n",
    "- Click on the `Data` module in the left-side menu in Azure ML Studio.\n",
    "\n",
    "The Data section provides various tools for data management, and it allows to upload files or folders with data from a local machine, or provide links to web files (e.g., data from GitHub or Google Drive), load data from a list of open datasets collected by Microsoft (check [Azure Open Datasets](https://learn.microsoft.com/en-us/azure/open-datasets/dataset-catalog)), or use files from a datastore. Datastores allow organizations that have many data files in different locations in Azure to link them together and organize them in a single view.\n",
    "\n",
    "- Select `Create` to load the dataset.\n",
    "\n",
    "Azure ML Studio will guide us through several steps for creating the dataset.\n",
    "\n",
    "<img src=\"images/Create_dataset.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aceffa-3f65-4f64-986d-5dfd9f50361e",
   "metadata": {},
   "source": [
    "On the first page, we will need to enter a name for the dataset, a brief description, and also indicate whether the data is in tabular or other formats. After the information is entered, select `Next`.\n",
    "\n",
    "<img style=\"float: center; height:350px;\" src=\"images/dataset_1.png\"> \n",
    "\n",
    "<img src=\"images/dataset_1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3852c-4611-4feb-912a-b9625e65eb15",
   "metadata": {},
   "source": [
    "Next, we will indicate that the dataset is saved on our computer and we will upload the dataset from local files.\n",
    "\n",
    "<img src=\"images/dataset_2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f44fd-f98c-49f6-b9c1-f6bddf0a4db0",
   "metadata": {},
   "source": [
    "Select `Upload` - `Upload files` and navigate to the csv file containing the heart failure records.\n",
    "\n",
    "The next page will show the columns in the dataset. Select `Next` to define the Schema for the data.\n",
    "\n",
    "<img src=\"images/dataset_3.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310cc04-d55a-46cc-8e92-8bfcad4b03be",
   "metadata": {},
   "source": [
    "In the Schema page, we will change the data type to Boolean for the columns `anemia`, `diabetes`, `high blood pressure`, `sex`, `smoking`, and `DEATH_EVENT`.\n",
    "\n",
    "Afterward, click `Next` and select `Create` to complete the creation of the dataset. \n",
    "\n",
    "<img src=\"images/dataset_4.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000ee08-def7-4da8-9c00-a86894986f6f",
   "metadata": {},
   "source": [
    "Now we can see that the dataset `heart-failure-data` is listed under the `Data assets` in our workspace.\n",
    "\n",
    "<img src=\"images/data_created.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50b7f8-a477-4284-adda-eebe8d503296",
   "metadata": {},
   "source": [
    "### 26.3.4 Creating a Compute Resource<a id=\"26.3.4-creating-a-compute-resource\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579db70a-1b6b-42dc-9478-421b1c489482",
   "metadata": {},
   "source": [
    "We also need to use Compute Resources for our project to perform data preparation and processing, and to run the models. \n",
    "To create a compute resource, we will select `Compute` from the left-side menu. \n",
    "\n",
    "We can see that the compute resources are categorized into four tabs:\n",
    "\n",
    "- Compute Instances, are workstations for data and models; they involve creating a Virtual Machine (VM) and launching a notebook instance (e.g., compute resources to train a model are requested from the notebook). \n",
    "- Compute Clusters, VMs for on-demand code processing (e.g., training a model using AutoML).\n",
    "- Inference Clusters, clusters for inference using trained models.\n",
    "- Attached Compute, links to existing Azure compute resources, such as Virtual Machines or Azure Databricks clusters.\n",
    "\n",
    "<img src=\"images/compute_module.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6d946-861b-4b8d-aa34-0bd590057848",
   "metadata": {},
   "source": [
    "For this task, we will need a compute cluster, so let's select that tab.\n",
    "\n",
    "- Click on `New` to create a new compute resource.\n",
    "\n",
    "<img src=\"images/new_compute.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb84afa-9271-4245-95c7-9161a3fd2a1a",
   "metadata": {},
   "source": [
    "Selecting adequate compute resources for a project depends on several factors, which impose trade-offs between speed and cost. \n",
    "\n",
    "1. *CPU versus GPU*: CPUs are less expensive, but also less powerful especially for training deep learning models. GPUs are more expensive, but they provide efficient parallel computing, and are often necessary for training deep learning models. \n",
    "2. *Cluster size*: larger clusters are more expensive, but faster in completing tasks. For smaller tasks that don't take too long, it may be better to select a small compute cluster. \n",
    "3. *VM size*: similar to the cluster size, increasing the amount of RAM, number of cores, and processing speed of the VMs will reduce the computational time, but it will be more expensive.\n",
    "4. *Dedicated versus low-priority resources*: dedicated resources are non-interruptible, while low-priority instances can be assigned by Azure to other tasks and interrupt the job. \n",
    "\n",
    "For this project, we will select a VM with a dedicated CPU, and in fact only one option is listed as available for our region, which seems suitable. Let's select it and click on `Next`. \n",
    "\n",
    "<img src=\"images/VM.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3211ff8-ef2a-4578-b656-5974f808ac97",
   "metadata": {},
   "source": [
    "On the next page, we need to assign a name to the compute cluster. \n",
    "We will leave the `minimum number of nodes` to 0, which means that when the cluster is idle we don't need to pay for the nodes. \n",
    "Regarding the `maximum number of nodes`, the higher it is, the shorter the training time will be, but it will be more costly. In this case, we are not allowed to select more than 1 node. \n",
    "The option `Idle seconds before scale down` defines how long to wait when the cluster is idle before changing it to the minimum number of nodes. \n",
    "\n",
    "- Let's click on `Create` to create the compute cluster. \n",
    "\n",
    "It can take a few minutes for the cluster to be created.\n",
    "\n",
    "<img src=\"images/cluster_2.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2a3ae-926a-424d-a7f0-bfbd73b7e4cd",
   "metadata": {},
   "source": [
    "### 26.3.5 Training a Model with Auto ML <a id=\"26.3.5-training-a-model-with-auto-ml\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42be31d-435b-496c-982f-9c7cd6e36a65",
   "metadata": {},
   "source": [
    "In a previous tutorial on AutoML in this course, we learned how to train a model in Hugging Face without writing code. Similarly, AutoML in Azure ML Studio allows to build and deploy ML models without writing code. \n",
    "\n",
    "- Select `Automated ML` from the modules in the left-side panel.\n",
    "- Select `+ New Automated ML job`.\n",
    "\n",
    "<img src=\"images/autoML.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e3f82b-6704-4f90-b26e-03e6c6ad3ee4",
   "metadata": {},
   "source": [
    "The first step requires to add a dataset for the job.\n",
    "- Select the heart failure dataset that we uploaded, and click on `Next`.\n",
    "\n",
    "<img src=\"images/autoML_data.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaadede-2e09-43ab-ab91-d5de19a4acb2",
   "metadata": {},
   "source": [
    "In the next step, we need to fill in information about the *job*, i.e., *experiment* (in the previous versions of Azure ML, this module was called Experiments; it was later named Jobs, but some fields still use the name Experiment).\n",
    "\n",
    "- Assign a name for the experiment.\n",
    "- Select a target column in the data: in this case it is `DEATH_EVENT`.\n",
    "- Use the drop-down menu to assign a compute resource, e.g., select the `heart-failure-cluster` that we created.\n",
    "\n",
    "<img src=\"images/autoML_experiment.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49e910-5a20-4fa2-a384-e3b7f6cf2d50",
   "metadata": {},
   "source": [
    "Afterward, we need to indicate the *task*, that is, whether the goal is to perform classification, regression, time-series forecasting, etc. In this case, `classification` was preselected, and we can confirm that this is indeed the task. Note that there is a checkmark about enabling deep learning. \n",
    "\n",
    "<img src=\"images/autoML_task.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae900b-74ff-4944-a11b-c475867f1600",
   "metadata": {},
   "source": [
    "Finally, we need to specify the validation type, e.g., whether we would like to use k-fold cross-validation, or whether to split the training data into train and validation sets, etc. Also, the test data asset field allows to upload a test dataset or specify how to evaluate the model. We can leave these fields at their defaults, and click `Finish` to complete the setup.\n",
    "\n",
    "<img src=\"images/autoML_validate.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeeeaee-0ba7-49dd-bd09-17ed8cea2507",
   "metadata": {},
   "source": [
    "Now the setup is complete and the experiment will begin running. This means that Azure ML will train many different models, and explore different hyperparameters for the models. \n",
    "\n",
    "On the home page, we will see a summary of the entered information about the experiment, and we will also see that the status of the experiment is `Running - Model training`. It took about 1 hour to complete this experiment. \n",
    "\n",
    "<img src=\"images/running.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ecbf0-4f1a-4e58-8bd0-fff038d062f8",
   "metadata": {},
   "source": [
    "When the experiment is completed, in the `Best model summary` we can see that the highest performance was obtained by a Voting Ensemble model, which achieved 92% AUC. \n",
    "\n",
    "<img src=\"images/autoML_summary.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159734c-a22e-4ff3-9f21-c30becf5af8e",
   "metadata": {},
   "source": [
    "Also, let's select the `Models` tab to get more information about the training. We can see that 66 models were trained in total, including Random Forest, Gradient Boosting, XGBoost, and running most of the models took under 1 minute. We can also see that different scaling methods were used with different algorithms (MinMaxScaler, RobustScaler, StandardScaler).\n",
    "\n",
    "We can also select `View explanation` to see which features contributed the most to the predictions, and also other details are provided about the models. \n",
    "\n",
    "<img src=\"images/models.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0853eb0-3828-4335-bde5-e47bd4d10b56",
   "metadata": {},
   "source": [
    "### 26.3.6 Deploying the Model <a id=\"26.3.6-deploying-the-model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d08d37-eb58-4fd8-aa5d-c5cedbe02b1f",
   "metadata": {},
   "source": [
    "To deploy the model as a web service, we will select the Voting Ensemble as the best model, and from the drop-down menu under the `Deploy` tab select `Deploy to web service`. \n",
    "\n",
    "<img src=\"images/deploy.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d56eca-cb3a-4b95-9f85-8dad1280ec95",
   "metadata": {},
   "source": [
    "In the newly opened form, we need to assign a name for the deployed model, a brief description, and compute type for the deployed model. In this case, we selected Azure Container Instance, which is suitable for low-scale CPU-based workloads, as is the model for this project. To deploy models that require large computational resources can require to select other compute type. \n",
    "\n",
    "Next, let's click on `Deploy` to initialize this step. It took about 15 minutes for this project. When it is completed, the `Deploy Status` on the dashboard will change from Running to Succeeded. \n",
    "\n",
    "<img src=\"images/deploy_form.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b01538-0376-4056-9f5d-d23747381159",
   "metadata": {},
   "source": [
    "### 26.3.7 Consuming the Model <a id=\"26.3.7-consuming-the-model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f03693-4a4a-428e-98b0-020dd0fcf649",
   "metadata": {},
   "source": [
    "After the model is deployed, we can find the summarized information in the `Endpoints` module in the left-hand menu. \n",
    "- Select the `Consume` tab to access the script for consuming the model.\n",
    "\n",
    "<img src=\"images/endpoint.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf9556e-2572-4478-a4f3-8e6a5516d4df",
   "metadata": {},
   "source": [
    "The opened page will provide the REST endpoint, and a script for consuming the model from a local machine. The script is available in C#, Python, and R. \n",
    "\n",
    "<img src=\"images/consume.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540e232-7442-4297-b901-21841e8716af",
   "metadata": {},
   "source": [
    "The Python script is shown below. The `data` section represents a dictionary where the users enter information for the input features. In this script, all values are set either to 0 or False. Then, `url` below is the address for the REST endpoint from the above figure, and `api_key` is the primary authentication key in the above figure. The last code section makes a prediction for the DEATH_EVENT, and the result is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f753760f-6172-462b-bd19-e5494ec762ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "data =  {\n",
    "  \"Inputs\": {\n",
    "    \"data\": [\n",
    "      {\n",
    "        \"age\": 0.0,\n",
    "        \"anaemia\": False,\n",
    "        \"creatinine_phosphokinase\": 0,\n",
    "        \"diabetes\": False,\n",
    "        \"ejection_fraction\": 0,\n",
    "        \"high_blood_pressure\": False,\n",
    "        \"platelets\": 0.0,\n",
    "        \"serum_creatinine\": 0.0,\n",
    "        \"serum_sodium\": 0,\n",
    "        \"sex\": False,\n",
    "        \"smoking\": False,\n",
    "        \"time\": 0\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"GlobalParameters\": {\n",
    "    \"method\": \"predict\"\n",
    "  }\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'http://a836b469-9573-4a63-bd31-90e8205ae13c.westus2.azurecontainer.io/score'\n",
    "api_key = 'QYEGDiZFpL5OECR2aZEhhNfSfYhPvgVn' # Replace this with the API key for the web service\n",
    "\n",
    "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
    "# Remove this header to have the request observe the endpoint traffic rules\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a8728b-0802-4489-a0ae-f3150a5b0582",
   "metadata": {},
   "source": [
    "To consume the model, we just need to save the script to our local machine and execute it. The output is shown below. For this set of input parameters, the result for DEATH_EVENT is `True`.\n",
    "\n",
    "<img src=\"images/result_1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c0795-8129-4776-9d7e-fa3311711f04",
   "metadata": {},
   "source": [
    "Let's check the model prediction for the last record in the dataset. The input features should be as follow, and the expected output is `False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c257286-d055-4e99-ad96-078b4caf8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"age\": 50.0,\n",
    "\"anaemia\": False,\n",
    "\"creatinine_phosphokinase\": 196,\n",
    "\"diabetes\": False,\n",
    "\"ejection_fraction\": 45,\n",
    "\"high_blood_pressure\": False,\n",
    "\"platelets\": 395000.0,\n",
    "\"serum_creatinine\": 1.6,\n",
    "\"serum_sodium\": 136,\n",
    "\"sex\": True,\n",
    "\"smoking\": True,\n",
    "\"time\": 285"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623994c-63e8-431e-ac5d-2026fea4654c",
   "metadata": {},
   "source": [
    "The prediction by the model is `False` as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a4a2bc-5742-4c94-a924-b3a1666b1aa3",
   "metadata": {},
   "source": [
    "<img src=\"images/result_2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced66a4-95a9-430c-a109-dfc360d96623",
   "metadata": {},
   "source": [
    "## 26.4 Code-based Azure ML <a id=\"26.4-code-based-azure-ml\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5501753a-5821-4b99-aac0-b4565390954f",
   "metadata": {},
   "source": [
    "In this section, we will use the **Azure Machine Learning SDK** to manage Data Science projects in a Python environment, that can include Jupyter Notebooks, VS Code, or other Python IDEs. Differently from the previous section which focused on No-Code environment with Azure ML Studio, this section focuses on Code-based environment with Azure ML SDK. \n",
    "\n",
    "We will explain the basic concepts using again the Heart Failure dataset from the previous section. Since we have already created a Workspace in which this dataset is loaded, we can use the same Workspace in this section as well. \n",
    "\n",
    "- Let's follow again the [link](https://ml.azure.com) to log into Azure ML and get access to the Workspace. \n",
    "\n",
    "The workspace is shown below, where we can see that the job using the AutoML module from the previous section is listed.\n",
    "\n",
    "<img src=\"images/workspace_2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6c461-6921-423e-816a-e68b54a08b81",
   "metadata": {},
   "source": [
    "### 26.4.1 Creating a Compute Resource<a id=\"26.4.1-creating-a-compute-resource\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f3efe-27d2-4a01-b2ac-93405f65a905",
   "metadata": {},
   "source": [
    "For this task we need to create a new compute resource. Similar to the previous section, we select `Compute` module from the left-side menu. Only this time we will select the `Compute instances` tab and click on `+ New`. \n",
    "\n",
    "Let's name it `computeinstance2` and select the same parameters as in the previous section, that is, CPU standard instance. It will take a few minutes for the instance to be created. \n",
    "\n",
    "<img src=\"images/resources_2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ea5ac-a120-47ed-b233-3a0d300fef44",
   "metadata": {},
   "source": [
    "### 26.4.2 Using Jupyter Notebooks in Azure ML<a id=\"26.4.2-using-jupyter-notebooks-in-azure-ml\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce5d14-1a63-443c-b858-9fdb3cfc3f38",
   "metadata": {},
   "source": [
    "In the homepage, the created compute instance will be listed.\n",
    "\n",
    "To use Jupyter Notebooks, we will select `Jupyter` from the Applications tab for the newly created compute instance. \n",
    "\n",
    "<img src=\"images/jupyter_1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c473604-1e69-4763-b3cc-5ea0387f7899",
   "metadata": {},
   "source": [
    "The Jupyter Notebook environment will open, so let's create a New notebook for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042de23a-6285-4a20-9b94-d560a7f44f6f",
   "metadata": {},
   "source": [
    "### 26.4.3 Training a Model<a id=\"26.4.3-training-a-model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406f877e-7422-42e4-bdcf-f616fcc0f548",
   "metadata": {},
   "source": [
    "The first step is to load the workspace from the configuration file. This is accomplished with the code in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447005a9-7a01-4edc-be33-2b63ff05fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c3b72-079d-4ec8-a396-0c581af5d417",
   "metadata": {},
   "source": [
    "Next, we will create a new experiment, which we can name `aml-experiment`, and we will associate it with the workspace variable `ws`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e134345-12fd-4a7e-b1bc-f6f87d1a2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'aml-experiment'\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d92b9-9007-4947-9c04-a6b363b6d50e",
   "metadata": {},
   "source": [
    "In the following step, we will assign a compute resource to the workspace. The code will find the existing compute resource that we just created and named it `computeinstance2`, and will link it to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3cc438-7efd-45ff-a4cd-40bb7ec47a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "\n",
    "aml_name = \"computeinstance2\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_name)\n",
    "    print('Found existing AML compute context.')\n",
    "except:\n",
    "    print('Creating new AML compute context.')\n",
    "    aml_config = AmlCompute.provisioning_configuration(vm_size = \"Standard_D2_v2\", min_nodes=1, max_nodes=3)\n",
    "    aml_compute = AmlCompute.create(ws, name = aml_name, provisioning_configuration = aml_config)\n",
    "    aml_compute.wait_for_completion(show_output = True)\n",
    "\n",
    "cts = ws.compute_targets\n",
    "compute_target = cts[aml_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a97bde-cd2c-43aa-a03a-eb8e62cf7e54",
   "metadata": {},
   "source": [
    "Similarly, the code in the next cell specifies that we will use the `heart-failure-data` that we loaded in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0308a-924c-421f-a485-eca92cca2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ws.datasets['heart-failure-data']\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0102750-ea86-4937-a0ba-2643d3f4ddfd",
   "metadata": {},
   "source": [
    "Training the model for this case will be set up using the `AutoMLConfig` class. The code is shown below, where the parameters include:  `experiment_timeout_minutes` (maximum minutes to run the experiment), `max_concurrent_iterations` (number of concurrent training iterations for the experiment), `primary_metric` (AUC evaluation metric), `debug_log` (is the log file to write debug information), and most of the other parameters are the same as in the settings in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12dcbf-82bc-4a2e-931d-867586389689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "project_folder = './aml-project'\n",
    "\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 90,\n",
    "    \"max_concurrent_iterations\": 3,\n",
    "    \"primary_metric\" : 'AUC_weighted'\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(compute_target=compute_target,\n",
    "                             task = \"classification\",\n",
    "                             training_data=dataset,\n",
    "                             label_column_name=\"DEATH_EVENT\",\n",
    "                             path = project_folder,  \n",
    "                             enable_early_stopping= True,\n",
    "                             featurization= 'auto',\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581fa53e-4467-4801-a689-3884b69511bd",
   "metadata": {},
   "source": [
    "Next, the experiment is submitted and it is run. The training will take some time to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2999426-0046-4f85-ae7b-d6c8c33d7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1df100-8539-4ac6-b0fb-ec72a230e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa429b4-440f-4a1d-bf4d-5ed74de796ed",
   "metadata": {},
   "source": [
    "The training results are concurrently displayed during the training. After the training is completed, the best model was a StackingEnsemble and achieved 91.8% AUC. \n",
    "\n",
    "<img src=\"images/training_output.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a7c98-79ad-4f6e-91c4-af4aa75cdfec",
   "metadata": {},
   "source": [
    "The code in the next cell retrieves the best trained model that will be used or deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34278ce2-7f27-4427-bf3d-5e52469d0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74db16a-a165-4471-89e9-cf201009aa8d",
   "metadata": {},
   "source": [
    "### 26.4.4 Deploying the Model<a id=\"26.4.4-deploying-the-model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f5bdae-537a-4d67-a037-c71734faa382",
   "metadata": {},
   "source": [
    "The first step of deploying the model is to register the model so that it can be reused later, which is achieved by the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84106ca0-b621-4507-b76d-b38111ccfc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = best_run.properties['model_name']\n",
    "script_file_name = 'inference/score.py'\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', 'inference/score.py')\n",
    "description = \"aml heart failure project sdk\"\n",
    "model = best_run.register_model(model_name = model_name,\n",
    "                                model_path = './outputs/',\n",
    "                                description = description,\n",
    "                                tags = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683b942-94af-4cb7-9110-b1e70a732705",
   "metadata": {},
   "source": [
    "To deploy the best model we will use the `InferenceConfig` class, which creates an environment for the deployment. The `AciWebservice` class is used to create an endpoint for a web API service. Finally, the model is deployed using the `deploy` method, which takes as arguments the workspace AciWebservice, the best model, and the inference configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95bfb0-29c7-4525-8c62-8c4d1959c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=script_file_name, environment=best_run.get_environment())\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                               memory_gb = 1,\n",
    "                                               tags = {'type': \"automl-heart-failure-prediction\"},\n",
    "                                               description = 'Sample service for AutoML Heart Failure Prediction')\n",
    "\n",
    "aci_service_name = 'automl-hf-sdk-1'\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8f66c-9400-4fc2-8360-929783ee2745",
   "metadata": {},
   "source": [
    "### 26.4.5 Consuming the Model<a id=\"26.4.5-consuming-the-model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be3137-35d1-4674-8b4f-789192025fac",
   "metadata": {},
   "source": [
    "The resulting endpoint is then consumed by entering a set of input values, and it returns the predicted output by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fba26-2eb2-4d61-9baf-a44ac49ca15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"data\":\n",
    "    [\n",
    "        {\n",
    "            'age': \"60\",\n",
    "            'anaemia': \"false\",\n",
    "            'creatinine_phosphokinase': \"500\",\n",
    "            'diabetes': \"false\",\n",
    "            'ejection_fraction': \"38\",\n",
    "            'high_blood_pressure': \"false\",\n",
    "            'platelets': \"260000\",\n",
    "            'serum_creatinine': \"1.40\",\n",
    "            'serum_sodium': \"137\",\n",
    "            'sex': \"false\",\n",
    "            'smoking': \"false\",\n",
    "            'time': \"130\",\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "test_sample = str.encode(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b38e8-2850-4fbe-9308-18009fee4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = aci_service.run(input_data=test_sample)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828f13d-9186-415d-9c7d-0cf20578e239",
   "metadata": {},
   "source": [
    "The expected prediction by the model is `{\"result\": [false]}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90639dca-12ec-42e7-8ec3-0acad68c736d",
   "metadata": {},
   "source": [
    "## 26.5 Training Deep Learning Models with Azure ML <a id=\"26.5-training-deep-learning-models-with-azure-ml\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b0b78-d336-4dc3-8768-100710de9ff1",
   "metadata": {},
   "source": [
    "In the previous sections we let Azure ML explore multiple models and select the best performing model for classification of the heart failure datasets.\n",
    "\n",
    "In this section, we will learn how to use Azure ML to train our own custom model. We will define a deep learning model for classification of the MNIST dataset, which will be trained and evaluated using Azure resources.\n",
    "\n",
    "For instance, if we didn't have access to GPU, we could use the GPUs provided by Azure ML to train our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb993c4-d5bd-467a-b3a5-0eb1c5e79329",
   "metadata": {},
   "source": [
    "### 26.5.1 Create Workspace and Compute Resource<a id=\"26.5.1-creating-workspace-and-compute-resource\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ada0b-95ba-4837-a7ef-9ff58f7f4bf1",
   "metadata": {},
   "source": [
    "Let's log in to [Microsoft Azure webpage](https://azure.microsoft.com/en-us/free/) and create a new workspace named Workspace_2, by following the steps listed in Section 26.3.\n",
    "\n",
    "Afterward, we will navigate to the [Azure ML Studio website](https://ml.azure.com), and in the newly created Workspace 2, we will create a new compute resource from the `Compute` module in the left-side menu. As explained earlier, we need to use a `Compute Instance` when working with our own Python code, and we can select a CPU VM since MNIST is a relatively small dataset. \n",
    "\n",
    "<img src=\"images/mnist_resource.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41da268-8a5c-4a3d-bfc5-013c7dcd8f70",
   "metadata": {},
   "source": [
    "### 26.5.2 Loading the Data and Defining the Model<a id=\"26.5.2-loading-the-data-and-defining-the-model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad059b-fc03-46c9-ab94-a4dfda7ed8d7",
   "metadata": {},
   "source": [
    "In the created compute resource, we will select `Jupyter` to use a Jupyter Notebook (as in the previous section), and select `New` to create a new notebook using `Python 3.8 - PyTorch and TensorFlow` kernel. Let's rename the notebook to `mnist-demo`. \n",
    "\n",
    "The code in the next cells imports libraries and loads the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4c1de-16e5-4c05-9dab-77a00e376942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97b140-b819-4f52-ab0f-f602da445623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the data\n",
    "DATA_FOLDER = os.path.join(os.getcwd(), 'datasets/mnist-data')\n",
    "DATASET_BASE_URL = 'https://azureopendatastorage.blob.core.windows.net/mnist/'\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    os.path.join(DATASET_BASE_URL, 'train-images-idx3-ubyte.gz'),\n",
    "    filename=os.path.join(DATA_FOLDER, 'train-images.gz'))\n",
    "urllib.request.urlretrieve(\n",
    "    os.path.join(DATASET_BASE_URL, 'train-labels-idx1-ubyte.gz'),\n",
    "    filename=os.path.join(DATA_FOLDER, 'train-labels.gz'))\n",
    "urllib.request.urlretrieve(\n",
    "    os.path.join(DATASET_BASE_URL, 't10k-images-idx3-ubyte.gz'),\n",
    "    filename=os.path.join(DATA_FOLDER, 'test-images.gz'))\n",
    "urllib.request.urlretrieve(\n",
    "    os.path.join(DATASET_BASE_URL, 't10k-labels-idx1-ubyte.gz'),\n",
    "    filename=os.path.join(DATA_FOLDER, 'test-labels.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646eef36-6038-4270-8ed4-35d825657e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    def unpack_mnist_data(filename: str, label=False):\n",
    "        with gzip.open(filename) as gz:\n",
    "            struct.unpack('I', gz.read(4))\n",
    "            n_items = struct.unpack('>I', gz.read(4))\n",
    "            if not label:\n",
    "                n_rows = struct.unpack('>I', gz.read(4))[0]\n",
    "                n_cols = struct.unpack('>I', gz.read(4))[0]\n",
    "                res = np.frombuffer(gz.read(n_items[0] * n_rows * n_cols), dtype=np.uint8)\n",
    "                res = res.reshape(n_items[0], n_rows * n_cols) / 255.0\n",
    "            else:\n",
    "                res = np.frombuffer(gz.read(n_items[0]), dtype=np.uint8)\n",
    "                res = res.reshape(-1)\n",
    "        return res\n",
    "    \n",
    "    X_train = unpack_mnist_data(os.path.join(dataset_path, 'train-images.gz'), False)\n",
    "    y_train = unpack_mnist_data(os.path.join(dataset_path, 'train-labels.gz'), True)\n",
    "    X_test = unpack_mnist_data(os.path.join(dataset_path, 'test-images.gz'), False)\n",
    "    y_test = unpack_mnist_data(os.path.join(dataset_path, 'test-labels.gz'), True)\n",
    "\n",
    "    return X_train.reshape(-1,28,28,1), y_train, X_test.reshape(-1,28,28,1), y_test\n",
    "\n",
    "# load the data\n",
    "X_train, y_train, X_test, y_test = load_dataset(DATA_FOLDER)\n",
    "\n",
    "print('Data shape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e279c-3a47-4e65-b4c4-aa0a36eb13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, labels):\n",
    "    images_cnt = len(images)\n",
    "    assert images_cnt <= 10, f\"Number of images cannot exceed 10. The provided list has {images_cnt} elements.\"\n",
    "    assert images_cnt == len(labels), f\"Number of images ({images_cnt}) should be equal to number of labels ({len(labels)})\"\n",
    "    f, axarr = plt.subplots(nrows=1, ncols=images_cnt, figsize=(16,16))\n",
    "    for idx in range(images_cnt):\n",
    "        img = images[idx]\n",
    "        lab = labels[idx]\n",
    "        axarr[idx].imshow(img, cmap='gray_r')\n",
    "        axarr[idx].title.set_text(lab)\n",
    "        axarr[idx].axis('off')\n",
    "\n",
    "    plt.show()   \n",
    "    \n",
    "show_images(X_train[:10], y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e29d7-4206-4811-9fea-6d792985f249",
   "metadata": {},
   "source": [
    "<img src=\"images/show_images_1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f8412b-d8e6-4138-bcd1-77465782a46c",
   "metadata": {},
   "source": [
    "We will use TensorFlow Keras library to define a simple Convolutional Neural Network for MNIST classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914f2bd-d93e-4711-b406-1e3f7b6861f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "        [   tf.keras.layers.Conv2D(filters=10, kernel_size=5, input_shape=(28,28,1), activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Conv2D(filters=20, kernel_size=5, activation='relu'),\n",
    "            tf.keras.layers.Dropout(rate=0.2),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(320, activation='relu'),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10, activation='relu')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e9f43-2c2c-4293-8cdb-de8894a87c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0272715b-a235-49e5-9cd7-63d7d7a6170c",
   "metadata": {},
   "source": [
    "### 26.5.3 Preparing Azure ML Job and Training the Model<a id=\"26.5.3-preparing-azure-ml-job-and-training-the-model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd524a6a-fe6b-47f6-b5c5-03ff24c18541",
   "metadata": {},
   "source": [
    "Next, we will create the Azure ML job (experiment). As in the previous section, we will assign the workspace name `Workspace_2` using the information about our `subscription` and `resource group`. Afterward, we will create a new experiment name `demo-mnist-training` that will use the created Azure ML workspace and resources to train the model. Also, we will use `MLflow` to track the experiment and automatically log the TensorFlow training progress by using the `autolog()` method. \n",
    "\n",
    "The model was trained for 5 epochs, and it achieved close to 99% train accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c36b8c-c09e-41ef-9753-e6c9e4b5d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "import mlflow, mlflow.tensorflow\n",
    "\n",
    "SUBSCRIPTION=\"88963609-f846-4520-a908-e222ae55cc5d\"\n",
    "GROUP=\"My_resource_group_1\"\n",
    "WORKSPACE=\"Workspace_2\"\n",
    "\n",
    "ws = Workspace(\n",
    "    subscription_id=SUBSCRIPTION,\n",
    "    resource_group=GROUP,\n",
    "    workspace_name=WORKSPACE,\n",
    ")\n",
    "\n",
    "experiment = Experiment(ws, \"demo-mnist-training\")\n",
    "\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "mlflow.start_run(experiment_id=experiment.id)\n",
    "mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87470a79-acad-4ef0-990d-7924b26c8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a01f8-2b21-4395-8e3c-df8082c73fe2",
   "metadata": {},
   "source": [
    "<img style=\"float: center; height:150px;\" src=\"images/mnist_fit.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a6f797-3bf3-4a69-b4cc-8f4205399eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30caaa0c-a0cb-4a5d-9831-57ce62342b9a",
   "metadata": {},
   "source": [
    "### 26.5.4 Consuming the Model<a id=\"26.5.4-consuming-the-model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739edd0a-594e-4723-abc6-c7398ce687df",
   "metadata": {},
   "source": [
    "To consume the model, first we will register the model with Azure ML so that it can be used for inference in the future.\n",
    "\n",
    "Afterward, we will load the registered model and use it to predict the classes for several images. The accuracy of the model on the test dataset is about 99%, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a2974-039d-4cbb-bbcd-7c015f17ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the model\n",
    "from azureml.core.model import Model\n",
    "\n",
    "registered_model = Model.register(\n",
    "    workspace=ws,\n",
    "    model_name='mnist-tf-model',\n",
    "    model_path='mnist-tf-model.h5',\n",
    "    model_framework=Model.Framework.TENSORFLOW,\n",
    "    model_framework_version=tf.__version__)\n",
    "\n",
    "registered_model\n",
    "\n",
    "# load the registered model\n",
    "aml_model = Model(workspace=ws, name='mnist-tf-model', version=registered_model.version)\n",
    "\n",
    "downloaded_model_filename = aml_model.download(exist_ok=True)\n",
    "print(downloaded_model_filename)\n",
    "\n",
    "downloaded_model = tf.keras.models.load_model(downloaded_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14181ce-04ab-45d0-b8f7-7dd08cfefecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model \n",
    "preds = downloaded_model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f9fab-6cf4-40ed-95c2-1fa30b72204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(X_test[:10], preds[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e5bb4-02e7-4503-97ea-e409c3649837",
   "metadata": {},
   "source": [
    "<img src=\"images/show_images_2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c48066-2f42-4580-be83-3167566c4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b05d1-c468-456c-bae4-0c44afb0806c",
   "metadata": {},
   "source": [
    "<img style=\"float: center; height:50px;\" src=\"images/test_accuracy.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b40f82-ad52-407a-bf41-302ecb9a50e5",
   "metadata": {},
   "source": [
    "As we mentioned earlier, always remember to release the used resources after training or predicting with a model. One alternative is to `Stop` the current resource from running if we would like to reuse it later, or `Delete` the resources if it is not needed for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df52424-cb59-4655-ad29-7ce40fb1787c",
   "metadata": {},
   "source": [
    "<img src=\"images/clean_resource.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389368f9",
   "metadata": {},
   "source": [
    "## References <a id=\"references\"/>\n",
    "\n",
    "1. Microsoft course - Data Science for Beginners, available at [https://github.com/microsoft/Data-Science-For-Beginners](https://github.com/microsoft/Data-Science-For-Beginners).\n",
    "2. From No-Code to Code in Azure Machine Learning, by William VanBuskirk, available at: [https://levelup.gitconnected.com/from-no-code-to-code-in-azure-machine-learning-38ee6b556de2](https://levelup.gitconnected.com/from-no-code-to-code-in-azure-machine-learning-38ee6b556de2).\n",
    "3. Creating a TensorFlow Model with Python and Azure ML Studio, by Jarek Szczegielniak, available at [https://www.codeproject.com/Articles/5321728/Python-Machine-Learning-on-Azure-Part-3-Creating-a](https://www.codeproject.com/Articles/5321728/Python-Machine-Learning-on-Azure-Part-3-Creating-a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12fc40",
   "metadata": {},
   "source": [
    "[BACK TO TOP](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
