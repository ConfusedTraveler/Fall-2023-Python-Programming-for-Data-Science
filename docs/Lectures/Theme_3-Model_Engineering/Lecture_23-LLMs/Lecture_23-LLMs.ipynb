{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Zc3eqC2hVQCg-97uT5BZZR-lWutdK5jP","timestamp":1699675698515},{"file_id":"1WlJSR0FBSQtTqM8AVpW48zbvAIyj5d7F","timestamp":1699662628845}],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyO1KP29miP40u0ezzZXt8yy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ba72e11438d8446d9e1b2cd169220f0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_627aef8b3a6c40da97dab6c9bbe99fc5","IPY_MODEL_6f8d1618e15a4eb0bf83b4edd29989bb","IPY_MODEL_0f5cb80b42634b09be30c702e6e17095"],"layout":"IPY_MODEL_eda9aa26238b4544b15a162fee6a1d0c"}},"627aef8b3a6c40da97dab6c9bbe99fc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9620b61435bb418aafbe207b8d4d74d7","placeholder":"​","style":"IPY_MODEL_569993f427074883b555ba26ddae7448","value":"(…)ma-2-7b-chat-hf/resolve/main/config.json: 100%"}},"6f8d1618e15a4eb0bf83b4edd29989bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b08e4ad5aa7414ab6809e3ba2079f91","max":583,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3fcd6ffc5270494ea2e6a7335a926bb9","value":583}},"0f5cb80b42634b09be30c702e6e17095":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b482ab3671de4a0ab4f9a7d829c2c94c","placeholder":"​","style":"IPY_MODEL_929a333de35c4e96a5fc0d65a3685f0c","value":" 583/583 [00:00&lt;00:00, 49.7kB/s]"}},"eda9aa26238b4544b15a162fee6a1d0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9620b61435bb418aafbe207b8d4d74d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"569993f427074883b555ba26ddae7448":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b08e4ad5aa7414ab6809e3ba2079f91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fcd6ffc5270494ea2e6a7335a926bb9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b482ab3671de4a0ab4f9a7d829c2c94c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"929a333de35c4e96a5fc0d65a3685f0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ddf7263214834f13a3606e8e9b9a7c52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1d3487e70744a0d9849dd2e1a52badf","IPY_MODEL_fdecf49240ea4914b8bb6181fd2d9883","IPY_MODEL_83f101bc175d4d1ebd5e94c2eb6f1b6f"],"layout":"IPY_MODEL_51c424ae8df842f4a8ef73222ac3189e"}},"e1d3487e70744a0d9849dd2e1a52badf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f482de776a7f43bb9ae13d5ff33bdde3","placeholder":"​","style":"IPY_MODEL_ae8623f861e246a5a1af404ebbc425bd","value":"(…)esolve/main/model.safetensors.index.json: 100%"}},"fdecf49240ea4914b8bb6181fd2d9883":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea221cf68bc64c92a4ef5d530e00c242","max":26788,"min":0,"orientation":"horizontal","style":"IPY_MODEL_963d5c1e17664ca6b5da87dc84e05fcb","value":26788}},"83f101bc175d4d1ebd5e94c2eb6f1b6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a159c4cfd0742a4b1e63467e3af8456","placeholder":"​","style":"IPY_MODEL_b6d4404a0bc0472b950bdeb28f639e3b","value":" 26.8k/26.8k [00:00&lt;00:00, 2.35MB/s]"}},"51c424ae8df842f4a8ef73222ac3189e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f482de776a7f43bb9ae13d5ff33bdde3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae8623f861e246a5a1af404ebbc425bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea221cf68bc64c92a4ef5d530e00c242":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"963d5c1e17664ca6b5da87dc84e05fcb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a159c4cfd0742a4b1e63467e3af8456":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6d4404a0bc0472b950bdeb28f639e3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c10ed969a43a4e8d86d0f110168cc9c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48a906d7e2e84a6282ddcdf65f0da3c3","IPY_MODEL_3b1c21e641304f99b80d36fb2696d25b","IPY_MODEL_d88463eabfd04a8b9f3c3ea8f027df43"],"layout":"IPY_MODEL_bf032afbc9a44bdea7058181eacaf71a"}},"48a906d7e2e84a6282ddcdf65f0da3c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3132764ed3f44f01b2e026319a0adb92","placeholder":"​","style":"IPY_MODEL_355d05a94a0846a9a7d51776d81698be","value":"Downloading shards: 100%"}},"3b1c21e641304f99b80d36fb2696d25b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0e67b8ff9064dfaa5a54c6ed8c00c07","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d45d2390a6445ca8cc0f08308442f6d","value":2}},"d88463eabfd04a8b9f3c3ea8f027df43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e77e96c9d174ecc98723a23ba663dfc","placeholder":"​","style":"IPY_MODEL_b95e57356538430ebc1eacb05879c8cd","value":" 2/2 [00:50&lt;00:00, 22.92s/it]"}},"bf032afbc9a44bdea7058181eacaf71a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3132764ed3f44f01b2e026319a0adb92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"355d05a94a0846a9a7d51776d81698be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0e67b8ff9064dfaa5a54c6ed8c00c07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d45d2390a6445ca8cc0f08308442f6d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e77e96c9d174ecc98723a23ba663dfc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b95e57356538430ebc1eacb05879c8cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccc6d2e83ef14dbfb451223ef1300659":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b4ef4f5287d4046abe9596e8c5d3b6e","IPY_MODEL_10be3982a6684e8f84f5766268192f29","IPY_MODEL_025a2b4f401a4f59b98206859bd40aeb"],"layout":"IPY_MODEL_1a40ade6cd2049b78a0ae697d151a590"}},"5b4ef4f5287d4046abe9596e8c5d3b6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0f63453902342fb9f3ece638c430933","placeholder":"​","style":"IPY_MODEL_17eb83d2bb05478d8192f79e96c8ebcc","value":"model-00001-of-00002.safetensors: 100%"}},"10be3982a6684e8f84f5766268192f29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3de4ac1270474f05afb60cede7c439f0","max":9976576152,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f2bfd4e86d24208b8fed8e9428264e2","value":9976576152}},"025a2b4f401a4f59b98206859bd40aeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_610e994dd1ae4c77af05ecc2da6eaa4c","placeholder":"​","style":"IPY_MODEL_4f5722f604dd427b8448b7382ded1007","value":" 9.98G/9.98G [00:37&lt;00:00, 303MB/s]"}},"1a40ade6cd2049b78a0ae697d151a590":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0f63453902342fb9f3ece638c430933":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17eb83d2bb05478d8192f79e96c8ebcc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3de4ac1270474f05afb60cede7c439f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f2bfd4e86d24208b8fed8e9428264e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"610e994dd1ae4c77af05ecc2da6eaa4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f5722f604dd427b8448b7382ded1007":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f784f5c94a3b4f65b2bf55f8ca5daf68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1d53672b7474be5b3564bce6d0426e1","IPY_MODEL_f6728c6bfd3e4cf392405a17d70b7969","IPY_MODEL_47427458d7a14270a22686d2a7d597e4"],"layout":"IPY_MODEL_79dfaa325b5d471f925cd5a3d851003e"}},"e1d53672b7474be5b3564bce6d0426e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa097e4e3ce74ecbbb8baf6aa26685bb","placeholder":"​","style":"IPY_MODEL_bf35ff4f7d1e4de6a057746784e8f60f","value":"model-00002-of-00002.safetensors: 100%"}},"f6728c6bfd3e4cf392405a17d70b7969":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_938e49a468104054bb48d3026ab13bb5","max":3500296424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ccf924e9fe045cb94d0c3b7aad9d01a","value":3500296424}},"47427458d7a14270a22686d2a7d597e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb3b48d108b5403ba190fa298b677f2c","placeholder":"​","style":"IPY_MODEL_f23b2118649c4b98a69af926b0433acc","value":" 3.50G/3.50G [00:12&lt;00:00, 223MB/s]"}},"79dfaa325b5d471f925cd5a3d851003e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa097e4e3ce74ecbbb8baf6aa26685bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf35ff4f7d1e4de6a057746784e8f60f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"938e49a468104054bb48d3026ab13bb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ccf924e9fe045cb94d0c3b7aad9d01a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb3b48d108b5403ba190fa298b677f2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f23b2118649c4b98a69af926b0433acc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15047b8f493d40c080640a43bd3c0b5a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_453e4e0ac7b04416aa8c00adaf91ba7f","IPY_MODEL_72819384d8d24d4c8c6d77aebc677c29","IPY_MODEL_1aa98be8007547eca2e3c1fd2c7d9904"],"layout":"IPY_MODEL_76a8df3d695a4eba81a78ee29913fbd9"}},"453e4e0ac7b04416aa8c00adaf91ba7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_472958b9400f45c4bf63e6c402ea5cb5","placeholder":"​","style":"IPY_MODEL_71cc2de571ab44feb0f84e54c42483b6","value":"Loading checkpoint shards: 100%"}},"72819384d8d24d4c8c6d77aebc677c29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b1d71b29c93436d91d1903ca192252f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04ccad6fccc347649a20fa995300005d","value":2}},"1aa98be8007547eca2e3c1fd2c7d9904":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45a6097305f04c9faff3ab3115ffb500","placeholder":"​","style":"IPY_MODEL_1d3a7922cd1f45a3b086fbbe20f72e0f","value":" 2/2 [00:04&lt;00:00,  2.25s/it]"}},"76a8df3d695a4eba81a78ee29913fbd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"472958b9400f45c4bf63e6c402ea5cb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71cc2de571ab44feb0f84e54c42483b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b1d71b29c93436d91d1903ca192252f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04ccad6fccc347649a20fa995300005d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45a6097305f04c9faff3ab3115ffb500":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d3a7922cd1f45a3b086fbbe20f72e0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bee0be9368f44982b77d529f3b3e29e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26a146244d1d4df1aa9f93243c70a45f","IPY_MODEL_dc89bb3461584bbf99e82ab24ebb0be4","IPY_MODEL_934b03488ccc47e8a3c3a2a287b6b09e"],"layout":"IPY_MODEL_6a6d808090b74e388dfaf66d6f91740f"}},"26a146244d1d4df1aa9f93243c70a45f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_170937a8c16849ab8b5dda837944c7ad","placeholder":"​","style":"IPY_MODEL_83a14999f0f1453ba1bea154123edf73","value":"(…)t-hf/resolve/main/generation_config.json: 100%"}},"dc89bb3461584bbf99e82ab24ebb0be4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb02ac15fd804bee99167b68c353d342","max":179,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18f809855924429a8227d5551a03fc4f","value":179}},"934b03488ccc47e8a3c3a2a287b6b09e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38011661e9624110b1c4911501c101ea","placeholder":"​","style":"IPY_MODEL_60885df14ee6458985a50716d6876cd3","value":" 179/179 [00:00&lt;00:00, 15.5kB/s]"}},"6a6d808090b74e388dfaf66d6f91740f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"170937a8c16849ab8b5dda837944c7ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83a14999f0f1453ba1bea154123edf73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb02ac15fd804bee99167b68c353d342":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18f809855924429a8227d5551a03fc4f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38011661e9624110b1c4911501c101ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60885df14ee6458985a50716d6876cd3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2b2b220b2964992819a94f731005d4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a1b2b2b0c6b4bcab243efbb356bdef8","IPY_MODEL_3825daecd7a14afe999ff5c7c7bb7408","IPY_MODEL_385cb35ccfae41f5b222029eb792e19b"],"layout":"IPY_MODEL_2ed0a1fd68774e4e99919f0f3424e1d8"}},"8a1b2b2b0c6b4bcab243efbb356bdef8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_555f53ea3ca14c608a1191ed3f4c1871","placeholder":"​","style":"IPY_MODEL_93f2996092014c71b9cbfe84e347deef","value":"(…)at-hf/resolve/main/tokenizer_config.json: 100%"}},"3825daecd7a14afe999ff5c7c7bb7408":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c34e4f7f5edc40a4aeec13ec3959ec52","max":746,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c301584a72443ea9e8322b28e8f6328","value":746}},"385cb35ccfae41f5b222029eb792e19b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6c5f827a1a44440ba642b9204cf06db","placeholder":"​","style":"IPY_MODEL_d5232c6268024c6182799d88cd33a944","value":" 746/746 [00:00&lt;00:00, 63.2kB/s]"}},"2ed0a1fd68774e4e99919f0f3424e1d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"555f53ea3ca14c608a1191ed3f4c1871":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93f2996092014c71b9cbfe84e347deef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c34e4f7f5edc40a4aeec13ec3959ec52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c301584a72443ea9e8322b28e8f6328":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6c5f827a1a44440ba642b9204cf06db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5232c6268024c6182799d88cd33a944":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b89e7d1386644f269002b8238b89411c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3afa4bf72c1440fb1cc326d980579c7","IPY_MODEL_e31c78c1ec314458b825db9d35ff3caf","IPY_MODEL_ded84d46e3714562ac4fbcf83d591583"],"layout":"IPY_MODEL_25027c8a955f46b480a02e5522277e3f"}},"b3afa4bf72c1440fb1cc326d980579c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8d50e28b58944deb82ca7853ff53f47","placeholder":"​","style":"IPY_MODEL_2652888e07dd432e96e588e6388c6626","value":"tokenizer.model: 100%"}},"e31c78c1ec314458b825db9d35ff3caf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eabb4fbab4404a2998ed7a95055fa2a3","max":499723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f795f50806ff42529beb5944f5e9290a","value":499723}},"ded84d46e3714562ac4fbcf83d591583":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45445d38f4bc4dc3864e25d59cb7c792","placeholder":"​","style":"IPY_MODEL_05d5e23a65ba4187b371fcef07014593","value":" 500k/500k [00:00&lt;00:00, 31.1MB/s]"}},"25027c8a955f46b480a02e5522277e3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8d50e28b58944deb82ca7853ff53f47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2652888e07dd432e96e588e6388c6626":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eabb4fbab4404a2998ed7a95055fa2a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f795f50806ff42529beb5944f5e9290a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45445d38f4bc4dc3864e25d59cb7c792":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05d5e23a65ba4187b371fcef07014593":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97286f5a60b9404c822d164942dce6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03792c4761a24610ab9732ff6b11febd","IPY_MODEL_d949783e1aad45988713728d30e65ae1","IPY_MODEL_535cfe5b95694fa89555c946254d45f2"],"layout":"IPY_MODEL_f32974944b6841038ec22909354646f3"}},"03792c4761a24610ab9732ff6b11febd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_730e87c2ea7544dab003504b04993b3f","placeholder":"​","style":"IPY_MODEL_4136e10ab4144cd9ace1660bb45ec406","value":"(…)2-7b-chat-hf/resolve/main/tokenizer.json: 100%"}},"d949783e1aad45988713728d30e65ae1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd53bfcf7e2449f2ad94c86d904da827","max":1842764,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7743a5ec34284157a3df7483e14b7391","value":1842764}},"535cfe5b95694fa89555c946254d45f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fe64be0436d41e8916ba8223925afcd","placeholder":"​","style":"IPY_MODEL_90f3e3e5c53f4787a49bd3b740602da7","value":" 1.84M/1.84M [00:00&lt;00:00, 3.77MB/s]"}},"f32974944b6841038ec22909354646f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"730e87c2ea7544dab003504b04993b3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4136e10ab4144cd9ace1660bb45ec406":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd53bfcf7e2449f2ad94c86d904da827":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7743a5ec34284157a3df7483e14b7391":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6fe64be0436d41e8916ba8223925afcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90f3e3e5c53f4787a49bd3b740602da7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fdf378cb7af4bc587150a39d2729c2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0df311ae50084910affa4ceedfcd2067","IPY_MODEL_ed48e33823e245f880d5df8e1aeb220a","IPY_MODEL_0d6c08e8ef8942d29fef33535f4088b8"],"layout":"IPY_MODEL_409804729a6545369497fd1630a4eb97"}},"0df311ae50084910affa4ceedfcd2067":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39cb2525e7414e1c810be62903b03c51","placeholder":"​","style":"IPY_MODEL_2adfdd193d7242b9a32a7063c80c069a","value":"(…)b-chat-hf/resolve/main/added_tokens.json: 100%"}},"ed48e33823e245f880d5df8e1aeb220a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0637d34b75214e608920e9d022d154e0","max":21,"min":0,"orientation":"horizontal","style":"IPY_MODEL_253b491236cb4f798820efe4fd70a4c9","value":21}},"0d6c08e8ef8942d29fef33535f4088b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1e14e8eadfc469da211286dada3cbce","placeholder":"​","style":"IPY_MODEL_374b3d5b5feb4625a84a8b76992f586a","value":" 21.0/21.0 [00:00&lt;00:00, 1.91kB/s]"}},"409804729a6545369497fd1630a4eb97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39cb2525e7414e1c810be62903b03c51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2adfdd193d7242b9a32a7063c80c069a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0637d34b75214e608920e9d022d154e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"253b491236cb4f798820efe4fd70a4c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1e14e8eadfc469da211286dada3cbce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"374b3d5b5feb4625a84a8b76992f586a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4706fc3caa24426099090a61a2a07564":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_baafec8e0be94c53b05702115ab560fc","IPY_MODEL_093f8f593d364d9e8ba6b21ce9246c3b","IPY_MODEL_84733b50276c4e4cbf8be11b254a9168"],"layout":"IPY_MODEL_995113971eda4ab590b1587e8abe9a9f"}},"baafec8e0be94c53b05702115ab560fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca8060bd081e462ebaf0c65028c44e8c","placeholder":"​","style":"IPY_MODEL_76b0fd3600524c98b8694eb88aebea9a","value":"(…)-hf/resolve/main/special_tokens_map.json: 100%"}},"093f8f593d364d9e8ba6b21ce9246c3b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_26a489f71fdc4ca49f0c764413668099","max":435,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e28a9bfe7719428aa083db5b49f5daae","value":435}},"84733b50276c4e4cbf8be11b254a9168":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81a55df60a154a86bf391b9d47b9c5ac","placeholder":"​","style":"IPY_MODEL_c124158d4327457f81aff65a3b8e8b99","value":" 435/435 [00:00&lt;00:00, 35.3kB/s]"}},"995113971eda4ab590b1587e8abe9a9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca8060bd081e462ebaf0c65028c44e8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76b0fd3600524c98b8694eb88aebea9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26a489f71fdc4ca49f0c764413668099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e28a9bfe7719428aa083db5b49f5daae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81a55df60a154a86bf391b9d47b9c5ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c124158d4327457f81aff65a3b8e8b99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e75b433cf78a404ca6943e5cdf7d88cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8d778ce336140758678cedcf510f32e","IPY_MODEL_5443ebf1314f406eb100631a9a8f1dc0","IPY_MODEL_631da6bcc6fe40159f12c52828c59ee8"],"layout":"IPY_MODEL_b402de4764764109a45faf2a20f06fcf"}},"c8d778ce336140758678cedcf510f32e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1781b82de0b43d09b4a082e758b4b18","placeholder":"​","style":"IPY_MODEL_b571c326c92d4a57ac50398518b8848a","value":"Downloading data files: 100%"}},"5443ebf1314f406eb100631a9a8f1dc0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc63063787bd4f9189710c840ea0b6af","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7948aed2e9934e34bbec71dba9e2a8c7","value":1}},"631da6bcc6fe40159f12c52828c59ee8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6f3542e07074b57baedddf91b62d8fd","placeholder":"​","style":"IPY_MODEL_6cc931cc9ec54ccb80b23bbb873bcb95","value":" 1/1 [00:00&lt;00:00,  1.72it/s]"}},"b402de4764764109a45faf2a20f06fcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1781b82de0b43d09b4a082e758b4b18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b571c326c92d4a57ac50398518b8848a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc63063787bd4f9189710c840ea0b6af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7948aed2e9934e34bbec71dba9e2a8c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6f3542e07074b57baedddf91b62d8fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cc931cc9ec54ccb80b23bbb873bcb95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdf4f78b64904f818f62fa05fa992efa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b297a74ea364411180ab939f9f69c136","IPY_MODEL_639fdc4677674c2594987a971103457e","IPY_MODEL_695730779c9c460faa532b0efffc96e7"],"layout":"IPY_MODEL_e76a60882ef940c785b100a32a7b7bb4"}},"b297a74ea364411180ab939f9f69c136":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e558f154c58f4e1088a3f4e7e67fc2a0","placeholder":"​","style":"IPY_MODEL_68dc34bac4004b1ca6167c7be77f5ef0","value":"Downloading data: 100%"}},"639fdc4677674c2594987a971103457e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_99d47b84c5c848c6b4b344bda78edd44","max":1144845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44a8dbe2251d45a08294c439da64a490","value":1144845}},"695730779c9c460faa532b0efffc96e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d7f6ae08199406fa6f165cd66dc8bf9","placeholder":"​","style":"IPY_MODEL_4695ccb174fe4eb1aa7ebc59b508dea2","value":" 1.14M/1.14M [00:00&lt;00:00, 2.03MB/s]"}},"e76a60882ef940c785b100a32a7b7bb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e558f154c58f4e1088a3f4e7e67fc2a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68dc34bac4004b1ca6167c7be77f5ef0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99d47b84c5c848c6b4b344bda78edd44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44a8dbe2251d45a08294c439da64a490":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d7f6ae08199406fa6f165cd66dc8bf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4695ccb174fe4eb1aa7ebc59b508dea2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8576ef20fad54ad18ee01ea5b10fef29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ab5a88a88f04ac2af012ae9fc23d099","IPY_MODEL_e32bca7c9ad34c24aa2f3b2d46f719d9","IPY_MODEL_390a8690216f4e54841852f93b743b5c"],"layout":"IPY_MODEL_6f35c35da9cf49a78abd9bf27353aaf5"}},"4ab5a88a88f04ac2af012ae9fc23d099":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6adee3257e7b4e09987d25d4dff1cdd0","placeholder":"​","style":"IPY_MODEL_a0f51fd20fd945a8b85d35753ab93b37","value":"Extracting data files: 100%"}},"e32bca7c9ad34c24aa2f3b2d46f719d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_64ace85a98f24d0fa8dcf0d393b497ed","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3134e73cc8eb40a4b746b96c484f4292","value":1}},"390a8690216f4e54841852f93b743b5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6429018dd8204099b6e4f02a2d7717b9","placeholder":"​","style":"IPY_MODEL_8fc59936ca944dac8406209c7caae7a0","value":" 1/1 [00:00&lt;00:00, 54.67it/s]"}},"6f35c35da9cf49a78abd9bf27353aaf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6adee3257e7b4e09987d25d4dff1cdd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0f51fd20fd945a8b85d35753ab93b37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64ace85a98f24d0fa8dcf0d393b497ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3134e73cc8eb40a4b746b96c484f4292":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6429018dd8204099b6e4f02a2d7717b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fc59936ca944dac8406209c7caae7a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b078716400684b469c6164aaec77c7d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b1b8cd74bde44fc9c3a83c0e15e638d","IPY_MODEL_5df204701b894f62877392a6f422eba9","IPY_MODEL_ed02f7a5fd96426597287efb56b7ec9e"],"layout":"IPY_MODEL_d925d0fa3dc24634aaf756954f218716"}},"9b1b8cd74bde44fc9c3a83c0e15e638d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9de0eb9270c1492ea3a1322e3630e4bc","placeholder":"​","style":"IPY_MODEL_5c339deb173c427f87a10bc05f4a25e0","value":"Generating train split: "}},"5df204701b894f62877392a6f422eba9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd4a602275154af991e750c8bfa82230","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61e740e754db43619869e4893bc21c1d","value":1}},"ed02f7a5fd96426597287efb56b7ec9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_007272e865c54712837cfb7d6c8161ea","placeholder":"​","style":"IPY_MODEL_f3dc9be162d3435cac98aa8ced7349b5","value":" 1260/0 [00:00&lt;00:00, 16840.09 examples/s]"}},"d925d0fa3dc24634aaf756954f218716":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9de0eb9270c1492ea3a1322e3630e4bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c339deb173c427f87a10bc05f4a25e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd4a602275154af991e750c8bfa82230":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"61e740e754db43619869e4893bc21c1d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"007272e865c54712837cfb7d6c8161ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3dc9be162d3435cac98aa8ced7349b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8084c0307104313af95fa54354f812c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_06c1d6dce31a4d04b35b866fa41b72f7","IPY_MODEL_0eeb4fcac89e4d71b60d210c958e872b","IPY_MODEL_ed4c4e3e35b747ef8e3fece1b785d6cd"],"layout":"IPY_MODEL_f5523cfc843744a28c812d3d5ce0a0dc"}},"06c1d6dce31a4d04b35b866fa41b72f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a78b373f9ac94b0f9327d845b7e76112","placeholder":"​","style":"IPY_MODEL_ddabba530bcf4517b7abf3f94960a292","value":"Map: 100%"}},"0eeb4fcac89e4d71b60d210c958e872b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b5ccd8d524a466790e306f9e8974b5a","max":1260,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0abafd95a440464ead8f9287cf356b69","value":1260}},"ed4c4e3e35b747ef8e3fece1b785d6cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddc6751f6a394dc7851b7f300ec62d96","placeholder":"​","style":"IPY_MODEL_81f1d6762eee44b692870c07610560db","value":" 1260/1260 [00:00&lt;00:00, 4046.12 examples/s]"}},"f5523cfc843744a28c812d3d5ce0a0dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a78b373f9ac94b0f9327d845b7e76112":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddabba530bcf4517b7abf3f94960a292":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b5ccd8d524a466790e306f9e8974b5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0abafd95a440464ead8f9287cf356b69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddc6751f6a394dc7851b7f300ec62d96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81f1d6762eee44b692870c07610560db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Lecture 23 - Large Language Models"],"metadata":{"id":"xoYnii0YEv9n"}},{"cell_type":"markdown","source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2023-Python-Programming-for-Data-Science/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_23-LLMs/Lecture_23-LLMs.ipynbb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2023-Python-Programming-for-Data-Science/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_23-LLMs/Lecture_23-LLMs.ipynb)"],"metadata":{"id":"QSUTHCD9lnyf"}},{"cell_type":"markdown","source":["<a id='top'></a>"],"metadata":{"id":"iGbTNIZlln1J"}},{"cell_type":"markdown","source":["- [23.1 Introduciton to LLMs](#23.1-introduciton-to-llms)\n","  - [23.1.1 Architecture of Large Language Models](#23.1.1-architecture-of-large-language-models)\n","  - [23.1.2 Variants of Transformer Network Architectures](#23.1.2-variants-of-transformer-network-architectures)\n","- [23.2 Creating LLMs](#23.2-creating-llms)\n","  - [23.2.1 Pretraining](#23.2.1-pretraining)\n","  - [23.2.2 Supervised Finetuning](#23.2.2-supervised-finetuning)\n","  - [23.3.3 Alignment with Reinforcement Learning from Human Feedback (RLHF)](#23.3.3-alignment-with-reinforcement-learning-from-human-feedback-(rlhf))\n","- [23.3 Finetuning LLMs](#23.3-finetuning-llms)\n","  - [23.3.1 Parameter-Efficient Finetuning (PEFT)](#23.3.1-parameter-efficient-finetuning-(peft))\n","  - [23.3.2 Low-Rank Adaptation (LoRA)](#23.3.2-low-rank-adaptation-(lora))\n","  - [23.3.3 Quanitized LoRA (QLoRA)](#23.3.3-quanitized-lora-(qlora))\n","- [23.4 Prompt Engineering](#23.4-prompt-engineering)\n","- [23.5 Limitations and Ethical Considerations of LLMs](#23.5-limitations-and-ethical-considerations-of-llms)\n","- [23.6 LLMs and Foundation Models](#23.6-llms-and-foundation-models)\n","- [References](#references)"],"metadata":{"id":"sMS08x8Rltiw"}},{"cell_type":"markdown","source":["## 23.1 Introduction to LLMs <a name='23.1-introduciton-to-llms'></a>"],"metadata":{"id":"KrZV8AcvUPqZ"}},{"cell_type":"markdown","source":["Large Language Models (LLMs) are  a class of Deep Neural Networks designed to understand and generate natural human language. These models have achieved state-of-the-art performance across various NLP tasks.\n","\n","LLMs are a result of many years of research and advancement in NLP and Machine Learning. Important phases in the development include:\n","\n","- Statistical language models (1980s-2000s): developed to predict the probability of a word in a text sequence based on the preceding words. Examples include bag-of-words models based on N-grams. These models were widely used in tasks like speech recognition and machine translation, but struggled with capturing long-range dependencies in text.\n","- Neural network models (2003-2017): fully-connected NNs and recurrent NNs  emerged as an alternative to statistical language models. Long Short-Term Memory (LSTM) RNN models were used for sequence-to-sequence tasks like machine translation and they formed the basis for several early LLMs.\n","- Transformer models (2017-present): transformer architecture replaced the recurrent layers in RNNs with self-attention mechanisms. This breakthrough enabled the development of more powerful and efficient LLMs, laying the foundation for BERT, GPT, and other LLMs."],"metadata":{"id":"yut8E4WoRpVv"}},{"cell_type":"markdown","source":["### 23.1.1 Architecture of Large Language Models <a name='23.1.1-architecture-of-large-language-models'></a>"],"metadata":{"id":"v5uvsID2UMCT"}},{"cell_type":"markdown","source":["The architecture of LLMs is based on Transformer Networks, which we covered in Lecture 20. Transformer Networks rely on the self-attention mechanism to process and generate text sequences. The main components of the Transformer Networks architecture include:\n","\n","- **Input embeddings**, are fixed-size continuous vector embeddings that represent tokens in input text.\n","- **Positional encodings**, are fixed-size continuous vectors that are added to the input embeddings to provide information about the relative positions of tokens in the input text sequence.\n","- **Encoder**, composed of a stack of multi-head attention modules and fully-connected (feed-forward) modules. The encoder block also includes dropout layers, residual connections, and applies layer normalization.\n","- **Decoder**: composed of a stack of multi-head self-attention modules and fully-connected (feed-forward) modules, with an additional masked multi-head  attention module.\n","- **Output fully-connected layer**: the output of the decoder is passed through a fully-connected (dense, linear) layer to produce the next token in the text sequence.\n","\n","<img src=\"images/transformer.jpg\" width=\"450\">\n","\n","*Figure: Pretraining LLMs.* Source: [2]."],"metadata":{"id":"aP-g3MgRUYUI"}},{"cell_type":"markdown","source":["Transformer Networks comprise a stack of multiple encoder and decoder blocks to create deep networks with many layers that allow learning complex patterns in input text. For example, the original Transformer Network has 6 encoder and 6 decoder blocks, as shown in the above figure.\n","\n","The **self-attention mechanism** is a key component of the Transformer Network architecture that enables the model to weigh the importance of each token with respect to other tokens in a sequence. It allows to capture long-range dependencies and relationships between the tokens (words) and helps the model to understand the context and structure of the input text sequence."],"metadata":{"id":"5eNRiCQhWXlN"}},{"cell_type":"markdown","source":["### 23.1.2 Variants of Transformer Network Architectures <a name='23.1.2-variants-of-transformer-network-architectures'></a>"],"metadata":{"id":"NtZrLHZdewMk"}},{"cell_type":"markdown","source":["Various LLMs are built on top of the Transformer Network architecture. The popular variants include:\n","\n","- Decoder-only models: are autoregressive models that utilize only the decoder part of the Transformer architecture. These models are particularly suitable for generating text and content. Examples are the family of GPT (Generative Pretrained Transformer) models.\n","- Encoder-only models: use only the encoder part of the Transformer architecture, and perform well on tasks related to language understanding, such as classification and sentiment analysis. An example is BERT, which is pretrained with masked language modeling and next sentence prediction.\n","- Encoder-Decoder models: employ the original Transformer architecture and combine encoder and decoder sub-networks, enabling to both understand language and generate content. These models can be used for various NLP tasks with minimal task-specific modifications. An example of this class of models is T5 (Text-to-Text Transfer Transformer)."],"metadata":{"id":"FevKIGPne1iW"}},{"cell_type":"markdown","source":["#### List of LLMs\n","\n","A large number of LLMs were developed in the last several years. Some of the most well-known LLMs include:\n","\n","- GPT (Generative pre-trained transformers): Developed by OpenAI, the GPT family are the best-known LLMs. They include GPT 1, 2, 3, 3.5 (initial ChatGPT), and 4 (current ChatGPT).\n","- BERT (Bidirectional Encoder Representations from Transformers): Developed by Google in 2018, BERT is an early LLM with 345M parameters that can understand natural language and answer questions.\n","- Llama: Developed by Meta AI, Llama is an open-source LLM, which can be used for both research and commercial uses. It consists of several models including Llama base model, Llama-chat, and Code-Llama.  \n","- Falcon: Developed by UAE's Technology Innovation Institute (TII), it is an open-source family of models with 1.3B, 7.5B, 40B, and 180B parameters.\n","- Bard, developed by Google, Bard is an LLM based on the LaMDA model.\n","- Claude: developed by Anthropic AI, Claude is an LLM with 137B parameters.\n","- PaLM Pathways Language Model (PaLM): Developed by Google, PaLM  is an LLM with 540B parameters capable of common-sense and arithmetic reasoning, code generation, and translation.\n","- Cohere LLM: Developed by Cohere, it is a family of 6B, 13B, and 52B parameters LLMs, designed for enterprise use cases.\n","- Vicuna: Developed by LMSYS, Vicuna is a 13B parameters chat assistant fine-tuned from LLaMA on user-shared conversations.\n","- Alpaca: Developed by Stanford, it is an LLM finetuned from instruction-following demonstrations by LLaMA.\n","- Dolly: Developed by Databricks, it is an open-source instruction-tuned LLM language model with 12B parameters."],"metadata":{"id":"40129R8npOLm"}},{"cell_type":"markdown","source":["## 23.2 Creating LLMs <a name='23.2-creating-llms'></a>"],"metadata":{"id":"s3o9DjItXUxt"}},{"cell_type":"markdown","source":["Creating modern LLMs such as ChatGPT or Llama 2, typically involves three main phases:\n","\n","1. **Pretraining**, the model extracts knowledge from large unlabeled text datasets.\n","2. **Supervised finetuning**, the model is refined to improve the quality of generated responses.\n","3. **Alignment**, the model is further refined to generate safe and helpful responses that are aligned with human preferences."],"metadata":{"id":"LaZLtPXyXfrK"}},{"cell_type":"markdown","source":["### 23.2.1 Pretraining <a name='23.2.1-pretraining'></a>\n","\n","The first step in creating LLMs is **pretraining** the model on massive amounts of text data. The datasets usually consist of a large collection of web pages or e-books comprising billions or trillions of tokens, and ranging from gigabytes to terabytes of text. During pretraining, the model learns the structure of the language, grammar rules, facts about the world, and reasoning abilities. And, it also learns biases and harmful content present in the training data.\n","\n"," Pretraining is performed using unsupervised learning techniques. Two common pretraining tasks are:\n","\n","- **Causal Language Modeling**, also known as autoregressive language modeling, involves training the model to predict the next token in the sequence given the previous tokens. This approach is used for pretraining ChatGPT, Llama 2, and it is more common with modern LLMs.\n","- **Masked Language Modeling**, where a certain percentage of input tokens are randomly masked, and the model is trained to predict the masked tokens based on the surrounding context. BERT and earlier LLMs were pretrained with masked language modeling.\n","\n","The following figure depicts the pretraining phase with causal language modeling, where the model learns to predict the next word in a sentence given the previous words.\n","\n","<img src=\"images/pretraining.jpg\" width=\"450\">\n","\n","*Figure: Pretraining LLMs.* Source: [3].\n","\n","Pretraining allows to extract knowledge from very large unlabeled datasets, without the need for manual labeling. Or, to be more precise, the \"label\" in model pretraining is the next word in the text, to which we already have access since it is part of the training text. Such pretraining approach is also called self-supervised training, since the model uses each next word in the text to self-supervise the training.\n","\n","Note that pretraining LLMs from scratch is computationally expensive and time-consuming. As we stated before, the pretraining phase can cost millions of dollars (e.g., estimated cost for training GPT-4 is $100 million). Also, pretraining LLMs requires access to large datasets and technical expertise with strong understanding of deep learning workflows, distributed software and hardware, and managing the training with thousands of GPUs simultaneously."],"metadata":{"id":"wJ5rp47DXf2w"}},{"cell_type":"markdown","source":["### 23.2.2 Supervised Finetuning <a name='23.2.2-supervised-finetuning'></a>\n","\n","After the pretraining phase, the model is finetuned on a much smaller dataset, which is carefully generated with human supervision. This dataset consists of samples where AI trainers provide both user queries (instructions) and model responses (outputs), as depicted in the following figure. That is, *instruction* is the input given to the model, and *output* is the desired response by the model. The model takes the instruction text as input (e.g., \"Write a limerick about a pelican\") and uses next-token prediction to generate the output text (e.g., \"There once was a pelican so fine ...\").\n","\n","The finetuning process involves updating the model's weights using supervised learning techniques. To compile datasets for supervised finetuning, AI trainers need to write the desired instructions and responses, which is a laborious process. Typical datasets include between 1K and 100K instruction-output pairs. Based on the provided instruction-output pairs, the model is finetuned to generate responses that are similar to those provided by AI trainers.\n","\n","<img src=\"images/finetuning.jpg\" width=\"500\">\n","\n","*Figure: Finetuning a pretrained LLM.* Source: [3]."],"metadata":{"id":"LnAjaqBPXlJC"}},{"cell_type":"markdown","source":["### 23.3.3 Alignment with Reinforcement Learning from Human Feedback (RLHF) <a name='23.3.3-alignment-with-reinforcement-learning-from-human-feedback-(rlhf)'></a>\n","\n","To further improve the performance and align the model responses with human preferences, LLMs typically use **Reinforcement Learning from Human Feedback (RLHF)**. This process is depicted in the figure below and involves the following steps:\n","\n","1. *Collect human feedback*. For this step a new dataset is created by collecting sample prompts from a database or by creating a set of new prompts. For each prompt, multiple responses are generated by the finetuned model. Next, AI trainers are asked to rank by quality all responses generated by the model for the same prompt from best to worst. Such feedback is used to define the human preferences and expectations about the responses by the model. Although this ranking process is time-consuming, it is usually less labor-intensive than creating the dataset for supervised finetuning, since ranking the responses is faster than writing the responses.\n","2. *Create a reward model*. The collected data with human feedback containing the prompts and the ranking scores of the different responses is used to train a reward model (denoted with RM in the figure). The task for the Reward Model is to predict the quality of the different responses to a given prompt and output a ranking score. The ranking scores provided by AI trainers are used to establish a ground-truth for training the Reward Model. Note that the Reward Model is a different model than the LLM that is being finetuned, and it only needs to rank the generated responses by the LLM.\n","3. *Finetune the LLM with RL*. The LLM is finetuned using a Reinforcement Learning algorithm, and for this step typically the Proximal Policy Optimization (PPO) algorithm is used. For a new prompt, the original LLM generates a response, which the Reward Model evaluates and calculates a reward score. Next, the PPO algorithm uses the reward score to finetune the LLM so that the total rewards for the generated responses by the LLM are maximized. I.e., the goal is to generate responses by the LLM that maximize the predicted rewards, and by that, are aligned with human preferences and are more useful to human users.\n","4. *Iterative improvement*. The RLHF process is performed iteratively, with multiple rounds of collecting additional feedback from human labelers, re-training the Reward Model, and applying Reinforcement Learning. This allows for continuous refinement and improvement of the LLM's performance.\n","\n","<img src=\"images/RLHF.jpg\" width=\"600\">\n","\n","*Figure: Reinforcement Learning from Human Feedback.* Source: [4].\n","\n","The RLHF approach creates a reward system that is augmented by human feedback and is used to teach LLMs which responses are more aligned with human preferences. Through these iterations, LLMs can learn to better align with human values and preferences, which can lead to higher-quality outputs and improved performance on specific tasks. RLHF has been successfully applied to finetune models like ChatGPT and Llama models.\n","\n","There are several variants of the RLFH approach for finetuning LLMs. For example, Llama 2 employs two reward models, one based on the ranks of helpfulness of the responses, and another based on the ranks of safety of the responses. The final reward score is obtained as a combination of the helpfulness and safety scores."],"metadata":{"id":"0s2BsqGjXlLg"}},{"cell_type":"markdown","source":["## 23.3 Finetuning LLMs <a name='23.3-finetuning-llms'></a>"],"metadata":{"id":"HY3OHi4TlRAw"}},{"cell_type":"markdown","source":["Finetuning LLMs involves updating the weights of a pretrained model on new data to improve its performance on a specific task and make the model more suitable for a specific use case. It involves additional training of the model on a new dataset that is specific to that task. To adapt pretrained LLMs to a custom task, different finetuning techniques have been applied. In this section, we will demonstrate how to finetune a pretrained LLM using a custom dataset.\n","\n","Full model finetuning is a method that finetunes all the parameters of all the layers of a pretrained model. Full model finetuning typically can achieve the best performance but it is also the most resource-intensive and time-consuming. Performance-efficient finetuning involves updating only a small number of parameters to reduce the required computational resources and costs.\n","\n","We will work with **Llama 2**, which is an open-source LLM developed by Meta AI and released in July 2023. Llama 2 is the first LLM that is open for both research and commercial use. Llama 2 is a successor model to the original Llama developed by Meta AI as well. Llama 2 has three variants with 7B, 13B, and 70B parameters. It has been trained on 2 trillion tokens, and it has a context window of 4,096 tokens enabling to process large documents.\n","\n","Furthermore, several specialized versions of Llama 2 were recently released, including Llama-2-Chat optimized for dialog generation, and Code Llama optimized for coding tasks."],"metadata":{"id":"gHwycTGfVUN_"}},{"cell_type":"markdown","source":["### 23.3.1 Parameter-Efficient Finetuning (PEFT) <a name='23.3.1-parameter-efficient-finetuning-(peft)'></a>"],"metadata":{"id":"rie71lFEWtQ_"}},{"cell_type":"markdown","source":["Finetuning LLMs is challenging because their large size requires substantial computational resources and it can be prohibitively expensive for most users. For instance, full model finetuning of the largest version of the Llama 2 model with 70 billion parameters requires 780 GB of GPU memory, which is equivalent to ten A100 80 GB GPUs.\n","\n","Fortunately, several Parameter-Efficient FineTuning (PEFT) techniques have been introduced recently, which focus on updating only a small number of model weights. Consequently, these techniques allow to finetune LLMs using lower computational resources by reducing memory usage and speeding up the training process. PEFT techniques include prompt tuning, prefix tuning, adding additional adapter layers in the transformer block, and low-rank adaptation (LoRA). Among these techniques, LoRA finetuning has been the most popular, since it allows to train LLMs with a single GPU.\n","\n","Hugging Face has developed a [PEFT library](https://huggingface.co/docs/peft/index) that contains implementations of common finetuning techniques, such as weight quantization, LoRA, QLoRA, prefix tuning, and others. We will use the PEFT library to finetune Llama 2 on a custom dataset."],"metadata":{"id":"jprNHaStWtS7"}},{"cell_type":"markdown","source":["### 23.3.2 Low-Rank Adaptation (LoRA) <a name='23.3.2-low-rank-adaptation-(lora)'></a>\n","\n","**Low-Rank Adaptation (LoRA)** involves freezing the pretrained model and finetuning a small number of additional weights. After the additional weights are updated, these weights are merged with the weights of the original model. Finetuning LLMs by updating only a small number of weights in comparison to updating all weights of the original model allows to complete the finetuning with lower computational resources.\n","\n","This is depicted in the following figure, where regular finetuning is shown in the left figure, and it involves updating all weights $W$ in a pretrained model. As we know, the weight update matrix $\\nabla{W}$ is calculated based on the negative gradient of the loss function. Finetuning with LoRA is shown in the right figure, where the weight update matrix $\\nabla{W}$ is decomposed into two smaller matrices, $\\nabla{W}=W_A*W_B$, with size $W_A \\in \\mathbb{R}^{A \\times r}$ and $W_B \\in \\mathbb{R}^{r \\times B}$. The matrices $W_A$ and $W_B$ are called low-rank adapters, since they have lower rank $r$ in comparison to the original weight matrix, i.e., they have fewer number of columns or rows, respectively. During training, gradients are backpropagated only through the matrices $W_A$ and $W_B$, while the pretrained weights $W$ remain frozen.\n","\n","For instance, if the full weight matrix $W$ is of size $100 \\times 100$, this is equal to $10,000$ elements (network weights). If we decompose the weight update matrix $\\nabla{W}$ by using rank $r=5$, the total number of elements of $W_A \\in \\mathbb{R}^{100 \\times 5}$ and $W_B \\in \\mathbb{R}^{5 \\times 100}$ will be $500 + 500 =  1,000$. Hence, with LoRA the number of elements was reduced from $10,000$ to $1,000$.\n","\n","<img src=\"images/LoRA.png\" width=\"600\">\n","\n","*Figure: Regular finetuning versus LoRA finetuning .* Source: [5]."],"metadata":{"id":"XS2wNFlwlgD1"}},{"cell_type":"markdown","source":["### 23.3.3 Quanitized LoRA (QLoRA) <a name='23.3.3-quanitized-lora-(qlora)'></a>\n","\n","**Quanitized LoRA (QLoRA)** is a modified version of LoRA that uses 4-bit quantized weights. *Quantization* reduces the precision of the values of the network weights. In TensorFlow and PyTorch, the network weights by default are stored with 32-bit floating-point precision. With quantization techniques, the network weights are stored with lower precision, such as 16-bit, 8-bit, or 4-bit floating-point precision.\n","\n","QLoRA introduces a new 4-bit quantization format called \"nf4\" (normalized float 4) that ensures an equal number of values in each quantization bin, to reduce  computational issues related to outlier values. In addition, QLoRA combines 4-bit quantization of the model weights in the pretrained model and LoRA with adding low-rank trainable weights. The benefits of QLoRA with 4-bit quantization of the model weights include reduced size of the model and increased inference speed, while having a modest decrease in the overall model performance.\n","\n","For example, with QLoRA a 70B parameter model can be finetuned with 48 GB VRAM, in comparison to 780 GB VRAM required for finetuning all weights of the original model (with 32-bit floating-point precision). Similarly, QLoRA enables to train the smaller version of Llama 2 with 7B parameters on a T4 GPU (provided by Google Colab) that has 16 GB VRAM. In cases when only a single GPU is available, using quantization is necessary for inetuning LLMs."],"metadata":{"id":"SfoWFu-JjxUr"}},{"cell_type":"markdown","source":["#### Import Libraries\n","\n","We will begin by installing the required libraries and importing modules from these packages. These include `accelerate` (for training on GPUs), `peft` (for Parameter-Efficient Fine-Tuning), `bitsandbytes` (to quantize the Llama model to 4-bit precision), `transformers` (for working with Transformer Networks), and `trl` (for supervised finetuning, where trl stands for Transformer Reinforcement Learning)."],"metadata":{"id":"kC7SG1ZO5L2y"}},{"cell_type":"code","source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"],"metadata":{"id":"ZJrRD657TXc1","executionInfo":{"status":"ok","timestamp":1699711190324,"user_tz":420,"elapsed":18400,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e82d9415-be84-4c3c-9868-5b95ce8e906c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n","    HfArgumentParser, TrainingArguments, pipeline, logging)\n","from peft import LoraConfig, PeftModel, get_peft_model\n","from trl import SFTTrainer"],"metadata":{"id":"-gVbHOdK5hdU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Load the Model\n","\n","We will download the smallest version of Llama-2-Chat model with 7B parameters from Hugging Face. Understandably, the larger Llama 2 models with 35B and 70B parameters require larger memory resources and computational resources for finetuning.\n","\n","Also, we will use the BitsAndBytes library to apply quantization with 4-bit precision format for loading the model weights. Loading a quantized model reduces the GPU memory requirement and makes it possible to train the model with a single GPU, as a tradeoff for some loss in precision. In the next cell we define the configuration for BitsAndBytes, and afterward we will use the configuration in the `from_pretrained` function to load the Llama 2 model. The parameters in BitsAndBytes configuration are described in the commented code below. The compute type can be either \"float16\", \"bfloat16\", or \"float32\" because computations are performed in either 16 or 32-bit precision. In this case, we specified to use `\"torch. float16\"` compute data type (i.e., 16-bit floating-point numbers) for memory-saving purposes."],"metadata":{"id":"DqCbomvC5L49"}},{"cell_type":"code","source":["# The model is Llama 2 from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\""],"metadata":{"id":"gChA0lhb7MpV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BitsAndBytes configuraton\n","bnb_config = BitsAndBytesConfig(\n","    # Load the model using 4-bit precision\n","    load_in_4bit=True,\n","    # Quantization type (fp4 or nf4)\n","    # nf4 is \"normalized float 4\" format, uses a symmetric quantization scheme with 4-bit precision\n","    bnb_4bit_quant_type=\"nf4\",\n","    # Compute dtype for 4-bit models\n","    bnb_4bit_compute_dtype= torch.float16,\n","    # Use double quantization for 4-bit models\n","    # Double quantization applies quantization to both weights and activations\n","    bnb_4bit_use_double_quant=False,\n",")"],"metadata":{"id":"9vqy2DA-7Igl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will use `AutoModelForCausalLM` to load the model with the `from_pretrained` function, and we will use the above BitesAndBytes configuration to load the model parameters with 4-bit precision.\n","\n","In the following cell we will load the corresponding tokenizer for Llama 2 by using `AutoTokenizer` and `from_pretrained`."],"metadata":{"id":"NjKpW3FwVOsN"}},{"cell_type":"code","source":["# Load Llama 2 model from Hugging Face\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    use_cache=False,\n","    pretraining_tp=1,\n","    # Load the entire model on the GPU if available\n","    device_map=\"auto\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241,"referenced_widgets":["ba72e11438d8446d9e1b2cd169220f0e","627aef8b3a6c40da97dab6c9bbe99fc5","6f8d1618e15a4eb0bf83b4edd29989bb","0f5cb80b42634b09be30c702e6e17095","eda9aa26238b4544b15a162fee6a1d0c","9620b61435bb418aafbe207b8d4d74d7","569993f427074883b555ba26ddae7448","1b08e4ad5aa7414ab6809e3ba2079f91","3fcd6ffc5270494ea2e6a7335a926bb9","b482ab3671de4a0ab4f9a7d829c2c94c","929a333de35c4e96a5fc0d65a3685f0c","ddf7263214834f13a3606e8e9b9a7c52","e1d3487e70744a0d9849dd2e1a52badf","fdecf49240ea4914b8bb6181fd2d9883","83f101bc175d4d1ebd5e94c2eb6f1b6f","51c424ae8df842f4a8ef73222ac3189e","f482de776a7f43bb9ae13d5ff33bdde3","ae8623f861e246a5a1af404ebbc425bd","ea221cf68bc64c92a4ef5d530e00c242","963d5c1e17664ca6b5da87dc84e05fcb","3a159c4cfd0742a4b1e63467e3af8456","b6d4404a0bc0472b950bdeb28f639e3b","c10ed969a43a4e8d86d0f110168cc9c1","48a906d7e2e84a6282ddcdf65f0da3c3","3b1c21e641304f99b80d36fb2696d25b","d88463eabfd04a8b9f3c3ea8f027df43","bf032afbc9a44bdea7058181eacaf71a","3132764ed3f44f01b2e026319a0adb92","355d05a94a0846a9a7d51776d81698be","c0e67b8ff9064dfaa5a54c6ed8c00c07","2d45d2390a6445ca8cc0f08308442f6d","7e77e96c9d174ecc98723a23ba663dfc","b95e57356538430ebc1eacb05879c8cd","ccc6d2e83ef14dbfb451223ef1300659","5b4ef4f5287d4046abe9596e8c5d3b6e","10be3982a6684e8f84f5766268192f29","025a2b4f401a4f59b98206859bd40aeb","1a40ade6cd2049b78a0ae697d151a590","a0f63453902342fb9f3ece638c430933","17eb83d2bb05478d8192f79e96c8ebcc","3de4ac1270474f05afb60cede7c439f0","2f2bfd4e86d24208b8fed8e9428264e2","610e994dd1ae4c77af05ecc2da6eaa4c","4f5722f604dd427b8448b7382ded1007","f784f5c94a3b4f65b2bf55f8ca5daf68","e1d53672b7474be5b3564bce6d0426e1","f6728c6bfd3e4cf392405a17d70b7969","47427458d7a14270a22686d2a7d597e4","79dfaa325b5d471f925cd5a3d851003e","aa097e4e3ce74ecbbb8baf6aa26685bb","bf35ff4f7d1e4de6a057746784e8f60f","938e49a468104054bb48d3026ab13bb5","2ccf924e9fe045cb94d0c3b7aad9d01a","cb3b48d108b5403ba190fa298b677f2c","f23b2118649c4b98a69af926b0433acc","15047b8f493d40c080640a43bd3c0b5a","453e4e0ac7b04416aa8c00adaf91ba7f","72819384d8d24d4c8c6d77aebc677c29","1aa98be8007547eca2e3c1fd2c7d9904","76a8df3d695a4eba81a78ee29913fbd9","472958b9400f45c4bf63e6c402ea5cb5","71cc2de571ab44feb0f84e54c42483b6","5b1d71b29c93436d91d1903ca192252f","04ccad6fccc347649a20fa995300005d","45a6097305f04c9faff3ab3115ffb500","1d3a7922cd1f45a3b086fbbe20f72e0f","bee0be9368f44982b77d529f3b3e29e2","26a146244d1d4df1aa9f93243c70a45f","dc89bb3461584bbf99e82ab24ebb0be4","934b03488ccc47e8a3c3a2a287b6b09e","6a6d808090b74e388dfaf66d6f91740f","170937a8c16849ab8b5dda837944c7ad","83a14999f0f1453ba1bea154123edf73","eb02ac15fd804bee99167b68c353d342","18f809855924429a8227d5551a03fc4f","38011661e9624110b1c4911501c101ea","60885df14ee6458985a50716d6876cd3"]},"id":"6PGB9V7H8KNk","executionInfo":{"status":"ok","timestamp":1699711271583,"user_tz":420,"elapsed":68033,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"b9a17645-6d9a-4f0e-d507-cea18387c566"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["(…)ma-2-7b-chat-hf/resolve/main/config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba72e11438d8446d9e1b2cd169220f0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["(…)esolve/main/model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf7263214834f13a3606e8e9b9a7c52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c10ed969a43a4e8d86d0f110168cc9c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccc6d2e83ef14dbfb451223ef1300659"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f784f5c94a3b4f65b2bf55f8ca5daf68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15047b8f493d40c080640a43bd3c0b5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["(…)t-hf/resolve/main/generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee0be9368f44982b77d529f3b3e29e2"}},"metadata":{}}]},{"cell_type":"code","source":["# Load tokenizer from Hugging Face\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","# Needed for LLaMA tokenizer\n","tokenizer.pad_token = tokenizer.eos_token\n","# Fix an overflow issue with fp16 training\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"QQhVU0lh8n2j","executionInfo":{"status":"ok","timestamp":1699711275292,"user_tz":420,"elapsed":3718,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["e2b2b220b2964992819a94f731005d4b","8a1b2b2b0c6b4bcab243efbb356bdef8","3825daecd7a14afe999ff5c7c7bb7408","385cb35ccfae41f5b222029eb792e19b","2ed0a1fd68774e4e99919f0f3424e1d8","555f53ea3ca14c608a1191ed3f4c1871","93f2996092014c71b9cbfe84e347deef","c34e4f7f5edc40a4aeec13ec3959ec52","9c301584a72443ea9e8322b28e8f6328","f6c5f827a1a44440ba642b9204cf06db","d5232c6268024c6182799d88cd33a944","b89e7d1386644f269002b8238b89411c","b3afa4bf72c1440fb1cc326d980579c7","e31c78c1ec314458b825db9d35ff3caf","ded84d46e3714562ac4fbcf83d591583","25027c8a955f46b480a02e5522277e3f","b8d50e28b58944deb82ca7853ff53f47","2652888e07dd432e96e588e6388c6626","eabb4fbab4404a2998ed7a95055fa2a3","f795f50806ff42529beb5944f5e9290a","45445d38f4bc4dc3864e25d59cb7c792","05d5e23a65ba4187b371fcef07014593","97286f5a60b9404c822d164942dce6e3","03792c4761a24610ab9732ff6b11febd","d949783e1aad45988713728d30e65ae1","535cfe5b95694fa89555c946254d45f2","f32974944b6841038ec22909354646f3","730e87c2ea7544dab003504b04993b3f","4136e10ab4144cd9ace1660bb45ec406","dd53bfcf7e2449f2ad94c86d904da827","7743a5ec34284157a3df7483e14b7391","6fe64be0436d41e8916ba8223925afcd","90f3e3e5c53f4787a49bd3b740602da7","9fdf378cb7af4bc587150a39d2729c2a","0df311ae50084910affa4ceedfcd2067","ed48e33823e245f880d5df8e1aeb220a","0d6c08e8ef8942d29fef33535f4088b8","409804729a6545369497fd1630a4eb97","39cb2525e7414e1c810be62903b03c51","2adfdd193d7242b9a32a7063c80c069a","0637d34b75214e608920e9d022d154e0","253b491236cb4f798820efe4fd70a4c9","c1e14e8eadfc469da211286dada3cbce","374b3d5b5feb4625a84a8b76992f586a","4706fc3caa24426099090a61a2a07564","baafec8e0be94c53b05702115ab560fc","093f8f593d364d9e8ba6b21ce9246c3b","84733b50276c4e4cbf8be11b254a9168","995113971eda4ab590b1587e8abe9a9f","ca8060bd081e462ebaf0c65028c44e8c","76b0fd3600524c98b8694eb88aebea9a","26a489f71fdc4ca49f0c764413668099","e28a9bfe7719428aa083db5b49f5daae","81a55df60a154a86bf391b9d47b9c5ac","c124158d4327457f81aff65a3b8e8b99"]},"outputId":"ba08488c-ef92-434c-a193-81f8bb32637b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["(…)at-hf/resolve/main/tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2b2b220b2964992819a94f731005d4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b89e7d1386644f269002b8238b89411c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["(…)2-7b-chat-hf/resolve/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97286f5a60b9404c822d164942dce6e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["(…)b-chat-hf/resolve/main/added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fdf378cb7af4bc587150a39d2729c2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["(…)-hf/resolve/main/special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4706fc3caa24426099090a61a2a07564"}},"metadata":{}}]},{"cell_type":"markdown","source":["#### Define LoRA Configuration"],"metadata":{"id":"gEBqQTb8vXy9"}},{"cell_type":"markdown","source":["Next, the model will be packed into the LoRA format, which will introduce additional weights and keep the original weights frozen. The parameters in the LoRA configuration include:\n","\n","- `r`, determines the rank of update matrices, where lower rank results in smaller update matrices with fewer trainable parameters, and greater rank (not more than 32) results in more trainable parameters but more robust model.\n","- `lora_alpha`, controls the LoRA scaling factor.\n","- `lora_dropout`, is the dropout rate for LoRA layers.\n","- `bias`, specifies if the bias parameters should be trained.\n","- `task_type`, is Causal LLM for the considered task."],"metadata":{"id":"lyBp3pMo5L7F"}},{"cell_type":"code","source":["# LoRA configuration\n","peft_config = LoraConfig(\n","    # LoRA rank dimension\n","    r=64,\n","    # Alpha parameter for LoRA scaling\n","    lora_alpha=16,\n","    # Dropout rate for LoRA layers\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")"],"metadata":{"id":"jFZQWZhZVTsw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to understand how LoRA impacts the finetuning of Llama 2 model, let's compare the total number of trainable parameters in LLama 2 and the trainable parameters for the LoRA model. As we can note in the cell below, the LoRA model has about 33M trainable parameters, which is about 1% of the total trainable parameters in LlaMA 2. This makes it possible to finetune the model on a single GPU."],"metadata":{"id":"z6sBPVs-dIkx"}},{"cell_type":"code","source":["def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    print(f\"Total model parameters: {all_model_params:,d}. Trainable model parameters: {trainable_model_params:,d}. Percent of trainable parameters: {100 * trainable_model_params/ all_model_params:4.2f} %\")"],"metadata":{"id":"0_yK0mSQvWwv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compare the number of trainable parameters to QLoRA model\n","qlora_model = get_peft_model(model, peft_config)\n","\n","# print trainable parameters\n","print_number_of_trainable_model_parameters(qlora_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dxCd5Vxb0mv","executionInfo":{"status":"ok","timestamp":1699711283509,"user_tz":420,"elapsed":8225,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6ea671c3-da85-4e99-ba6d-6eaa1dea336f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total model parameters: 3,533,967,360. Trainable model parameters: 33,554,432. Precent of trainable parameters: 0.95 %\n"]}]},{"cell_type":"markdown","source":["### Load the Dataset\n","\n","We will use the [Lamini docs](https://huggingface.co/datasets/lamini/lamini_docs) dataset, which contains questions and answers about the framework Lamini for training and developing Language Models. The dataset contains 1,260 question/answer pairs. Here are a few samples from the dataset.\n","\n","|Question |Answer\n","| :---- | :---\n","|Does Lamini support generating code|Yes, Lamini supports generating code through its API.\n","|How do I report a bug or issue with the Lamini documentation?| You can report a bug or issue with the Lamini documentation by submitting an issue on the Lamini GitHub page.\n","|Can Lamini be used in an online learning setting, <br /> where the model is updated continuously as new data becomes available?|It is possible to use Lamini in an online learning setting where the model is updated continuously as new data becomes available. <br /> However, this would require some additional implementation and configuration to ensure that the model is updated appropriately and efficiently."],"metadata":{"id":"bNUSM8ckMwMG"}},{"cell_type":"markdown","source":["A preprocessed version of the dataset in a format that matches the instruction-output pairs for Llama 2 is available on Hugging Face, and we will directly load the preprocessed version of the dataset."],"metadata":{"id":"gHiyZE0Qwtny"}},{"cell_type":"code","source":["# Lamini dataset\n","dataset = load_dataset(\"mwitiderrick/llamini_llama\", split=\"train\")"],"metadata":{"id":"up6AcG5hMqhO","executionInfo":{"status":"ok","timestamp":1699711293810,"user_tz":420,"elapsed":10313,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["e75b433cf78a404ca6943e5cdf7d88cd","c8d778ce336140758678cedcf510f32e","5443ebf1314f406eb100631a9a8f1dc0","631da6bcc6fe40159f12c52828c59ee8","b402de4764764109a45faf2a20f06fcf","f1781b82de0b43d09b4a082e758b4b18","b571c326c92d4a57ac50398518b8848a","fc63063787bd4f9189710c840ea0b6af","7948aed2e9934e34bbec71dba9e2a8c7","f6f3542e07074b57baedddf91b62d8fd","6cc931cc9ec54ccb80b23bbb873bcb95","cdf4f78b64904f818f62fa05fa992efa","b297a74ea364411180ab939f9f69c136","639fdc4677674c2594987a971103457e","695730779c9c460faa532b0efffc96e7","e76a60882ef940c785b100a32a7b7bb4","e558f154c58f4e1088a3f4e7e67fc2a0","68dc34bac4004b1ca6167c7be77f5ef0","99d47b84c5c848c6b4b344bda78edd44","44a8dbe2251d45a08294c439da64a490","6d7f6ae08199406fa6f165cd66dc8bf9","4695ccb174fe4eb1aa7ebc59b508dea2","8576ef20fad54ad18ee01ea5b10fef29","4ab5a88a88f04ac2af012ae9fc23d099","e32bca7c9ad34c24aa2f3b2d46f719d9","390a8690216f4e54841852f93b743b5c","6f35c35da9cf49a78abd9bf27353aaf5","6adee3257e7b4e09987d25d4dff1cdd0","a0f51fd20fd945a8b85d35753ab93b37","64ace85a98f24d0fa8dcf0d393b497ed","3134e73cc8eb40a4b746b96c484f4292","6429018dd8204099b6e4f02a2d7717b9","8fc59936ca944dac8406209c7caae7a0","b078716400684b469c6164aaec77c7d6","9b1b8cd74bde44fc9c3a83c0e15e638d","5df204701b894f62877392a6f422eba9","ed02f7a5fd96426597287efb56b7ec9e","d925d0fa3dc24634aaf756954f218716","9de0eb9270c1492ea3a1322e3630e4bc","5c339deb173c427f87a10bc05f4a25e0","bd4a602275154af991e750c8bfa82230","61e740e754db43619869e4893bc21c1d","007272e865c54712837cfb7d6c8161ea","f3dc9be162d3435cac98aa8ced7349b5"]},"outputId":"66955246-2813-4178-8658-76a5a43db4f7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e75b433cf78a404ca6943e5cdf7d88cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdf4f78b64904f818f62fa05fa992efa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8576ef20fad54ad18ee01ea5b10fef29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b078716400684b469c6164aaec77c7d6"}},"metadata":{}}]},{"cell_type":"code","source":["print(f'Number of prompts: {len(dataset)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-wVL7a4WAgJ","executionInfo":{"status":"ok","timestamp":1699711293810,"user_tz":420,"elapsed":20,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"c6af0a26-8a61-468c-da57-f91db3546dcf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of prompts: 1260\n"]}]},{"cell_type":"markdown","source":["#### Model Training\n","\n","The next cell defines the training arguments, and the commented notes describe the arguments. Note that we will finetune the model for only 1 epoch (if we finetune for more than 1 epoch it will take longer but it will probably result in improved performance)."],"metadata":{"id":"IGWUw7QSlREb"}},{"cell_type":"code","source":["# Set training parameters\n","training_arguments = TrainingArguments(\n","    # Output directory where the model predictions and checkpoints will be stored\n","    output_dir=\"./results\",\n","    # Number of training epochs\n","    num_train_epochs=1,\n","    # Batch size per GPU for training\n","    per_device_train_batch_size=4,\n","    # Number of update steps to accumulate the gradients for\n","    gradient_accumulation_steps=1,\n","    # Optimizer to use\n","    optim=\"paged_adamw_32bit\",\n","    # Save checkpoint every number of steps\n","    save_steps=0,\n","    # Log updates every number of steps\n","    logging_steps=25,\n","    # Initial learning rate (AdamW optimizer)\n","    learning_rate=2e-4,\n","    # Weight decay to apply\n","    weight_decay=0.001,\n","    # Enable fp16/bf16 training (set bf16 to True with an A100)\n","    fp16=False,\n","    bf16=False,\n","    # Maximum gradient normal (gradient clipping)\n","    max_grad_norm=0.3,\n","    # Group sequences into batches with same length\n","    # Saves memory and speeds up training considerably\n","    group_by_length=True,\n","    # Learning rate schedule\n","    lr_scheduler_type=\"constant\"\n",")"],"metadata":{"id":"xCpu1K5fGct0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will use the `SFTTrainer` class in Hugging Face to create an instance of the model by passing the loaded LlaMA 2 model, training dataset, PeFT configuration, tokenizer, and the training arguments. `SFTTrainer` stands for Supervised Fine-Tuning Trainer."],"metadata":{"id":"_KYUP14kOnKn"}},{"cell_type":"code","source":["# Set supervised finetuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    # Column in the dataset that contains the data\n","    dataset_text_field=\"text\",\n","    # Maximum sequence length to use\n","    max_seq_length=None,\n","    # Pack multiple short examples in the same input sequence to increase efficiency\n","    packing=False,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["e8084c0307104313af95fa54354f812c","06c1d6dce31a4d04b35b866fa41b72f7","0eeb4fcac89e4d71b60d210c958e872b","ed4c4e3e35b747ef8e3fece1b785d6cd","f5523cfc843744a28c812d3d5ce0a0dc","a78b373f9ac94b0f9327d845b7e76112","ddabba530bcf4517b7abf3f94960a292","7b5ccd8d524a466790e306f9e8974b5a","0abafd95a440464ead8f9287cf356b69","ddc6751f6a394dc7851b7f300ec62d96","81f1d6762eee44b692870c07610560db"]},"id":"2bB03IouIdxV","executionInfo":{"status":"ok","timestamp":1699711293811,"user_tz":420,"elapsed":15,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"12b1fb92-6642-4f19-8b87-244880e7ae2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1260 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8084c0307104313af95fa54354f812c"}},"metadata":{}}]},{"cell_type":"markdown","source":["Finally, we can train the model with the `train()` function in Hugging Face. In the output of the cell we can see the loss for every 25 training steps, based on `logging_steps=25` in the training arguments.\n","\n","The training took about 15 minutes on a T4 GPU with High-RAM memory on Google Clab Pro."],"metadata":{"id":"X0zwKCAxPGwB"}},{"cell_type":"code","source":["# Train the model\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":558},"id":"qzYKR_FwJDSe","executionInfo":{"status":"ok","timestamp":1699712252689,"user_tz":420,"elapsed":958493,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6a2fcabe-f82d-4b8c-a939-ef11cfd14336"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [315/315 15:47, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>1.923200</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.571700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.656000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.454400</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.581900</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.436300</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.624500</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.404100</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.579500</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.438100</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.563900</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.397300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=315, training_loss=0.6264339901152112, metrics={'train_runtime': 958.0585, 'train_samples_per_second': 1.315, 'train_steps_per_second': 0.329, 'total_flos': 5614695674511360.0, 'train_loss': 0.6264339901152112, 'epoch': 1.0})"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["#### Generate Text\n","\n","To generate text with the trained model we will use the Hugging Face `pipeline` with the task set to `\"text-generation\"`. We can set the length of the generated text tokens with the `max_length` argument.\n","\n","The output displays the start `<s>[INST]` and end `[/INST]` of the instruction prompt, followed by the generated output by the model."],"metadata":{"id":"gnLrKvauJV4z"}},{"cell_type":"code","source":["# Run text generation pipeline with the finetuned model\n","prompt = \"What are Lamini models?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","output = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"471QJli9KEo4","executionInfo":{"status":"ok","timestamp":1699712306059,"user_tz":420,"elapsed":53406,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"27b38810-36ef-4e17-c200-cb9a75d3bcc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n","pip install xformers.\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["<s>[INST] What are Lamini models? [/INST]  Lamini is a language model training platform that allows developers to train and fine-tune their own custom language models using their own data. everybody can train a language model with Lamini. Lamini models are trained on a specific dataset and can be used for a variety of natural language processing tasks such as text classification, sentiment analysis, and language translation.\n","\n","Lamini models are trained using a combination of machine learning algorithms and statistical techniques, such as gradient descent and maximum likelihood estimation. The model is trained on a large dataset of text, and the training process involves adjusting the model's parameters to minimize the error between the model's predictions and the actual labels.\n","\n","Lamini models can be trained on a variety of datasets, including text from the internet, social media, and other sources. The model can be fine-tuned on specific tasks, such as sentiment analysis or\n"]}]},{"cell_type":"code","source":["# Run text generation pipeline with the finetuned model\n","prompt = \"How to evaluate the quality of the generated text with Lamini models\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n","output = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWj51bGaL1m8","executionInfo":{"status":"ok","timestamp":1699712522319,"user_tz":420,"elapsed":216264,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a66f1054-ea8f-44ea-c5c5-139595da70ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s>[INST] How to evaluate the quality of the generated text with Lamini models [/INST]  Lamini offers a Python library called Lamini Python Library that allows you to train and use LLMs. everybody can use Lamini's python library to train and use LLMs.\n","\n","Here are some ways to evaluate the quality of the generated text with Lamini models:\n","\n","1. Perplexity: Measure the perplexity of the generated text by comparing it to a reference text. Lower perplexity indicates better quality.\n","2. BLEU score: Use the BLEU (Bilingual Evaluation Understudy) score to evaluate the quality of the generated text. BLEU measures the similarity between the generated text and a reference text.\n","3. ROUGE score: Use the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score to evaluate the quality of the generated text. ROUGE measures the similarity between the generated text and a reference text.\n","4. METEOR score: Use the METEOR (Metric for Evaluation of Translation with Explicit ORdering) score to evaluate the quality of the generated text. METEOR measures the similarity between the generated text and a reference text.\n","5. Human evaluation: Conduct human evaluations of the generated text to assess its quality. This can be done by having human evaluators read and rate the generated text on a scale of quality.\n","6. Automatic metrics: Use automatic metrics such as perplexity, entropy, and coherence to evaluate the quality of the generated text. These metrics can provide insights into the quality of the generated text without requiring human evaluations.\n","7. Contextual coherence: Evaluate the contextual coherence of the generated text by comparing it to the context in which it is being used. This can help identify any inconsistencies or inaccuracies in the generated text.\n","8. Fluency: Evaluate the fluency of the generated text by comparing it to the fluency of the reference text. This can help identify any areas where the generated text may be less fluent or less natural.\n","9. Coherence: Evaluate the coherence of the generated text by comparing it to the coherence of the reference text. This\n"]}]},{"cell_type":"code","source":["# Run text generation pipeline with the finetuned model\n","prompt = \"Write a poem about Data Science\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n","output = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQxWZNnDKrAy","executionInfo":{"status":"ok","timestamp":1699712603878,"user_tz":420,"elapsed":81583,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"24b2aec1-fb53-428b-b094-7eeef0ea1d2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s>[INST] Write a poem about Data Science [/INST]  In the realm of code and algorithms,\n"," nobody knows like Data Science\n","With a dash of math and a pinch of creativity,\n","She weaves a tale of insights and clarity.\n","\n","With a click of her mouse, she uncovers secrets,\n","Beneath the surface of numbers and facts,\n","She finds the hidden patterns and stories,\n","That reveal the truth and bring new facts.\n","\n","She's a detective of data, a master of the craft,\n","With a keen eye for detail and a heart of gold,\n","She unravels the mysteries of the past,\n","And brings them to life with a tale to be told.\n","\n","With a passion for learning and a drive to succeed,\n","She's a force to be reckoned with, indeed,\n","She's a data scientist, a wizard of the code,\n","A master of the craft, a true data goddess.\n","\n","So let her analyze your data with grace,\n","And uncover the secrets that lie within the space,\n","For she's a data scientist, a true data queen,\n","With a heart of gold and a mind that's keen.\n"]}]},{"cell_type":"markdown","source":["## 23.4 Prompt Engineering <a name='23.4-prompt-engineering'></a>"],"metadata":{"id":"nT8PV_WTlAaJ"}},{"cell_type":"markdown","source":["**Prompt engineering** is a technique for improving the performance of LLMs by providing them with more context and information about a specific task. It involves creating text prompts that provide additional information or guidance to the model, such as the topic of the generated response. By using prompts, the model can better understand the kind of excepted output and produce more accurate and relevant results.\n","\n","Crafting effective prompts is an important part of prompt engineering. The following tips for creating prompts can improve the performance of LLMs:\n","\n","- Be clear and concise: The prompt should be easy to understand and provide enough information for the model to generate relevant output. Avoid using jargon or technical terms.\n","- Use specific examples: Providing specific examples can help the model better understand the expected output. For example, if you want the model to generate a story about a particular topic, include a few sentences about the setting, characters, and plot.\n","- Vary the prompts: Using prompts with different styles, tones, and formats can result in more diverse outputs.\n","- Test and refine: Test the prompts on the model and try refining them by adding more detail or adjusting the tone and style.\n","- Use feedback: Use feedback from users or other sources to identify areas where the model needs more guidance and make adjustments accordingly.\n","\n","*Chain-of-thought technique* involves providing the LLM with a series of prompts or questions to help guide its thinking and generate a more coherent and relevant response. This technique can be useful for generating more thoughtful and well-reasoned responses from language models.\n","\n","Example of chain-of-thought prompt: You are a virtual tour guide from 1901. You have tourists visiting Eiffel Tower. Describe Eiffel Tower to your audience. Begin with (1) why it was built, (2) how long it took them to build, (3) where were the materials sourced to build, (4) number of people it took to build, and (5) number of people visiting the Eiffel tour annually in the 1900's, the amount of time it completes a full tour and why so many people visit this place each year. Make your tour funny by including 1 or 2 funny jokes at the end of the tour."],"metadata":{"id":"25kN3VP7lApv"}},{"cell_type":"markdown","source":["#### Retrieval Augmented Generation (RAG)\n","\n","For more complex tasks that require external knowledge, it's possible to build a language model-based system that accesses external knowledge sources to complete the required tasks. This enables more factual accuracy, and helps to mitigate the problem of \"hallucination\".\n","\n","With RAG, instead of using LLMs to access its internal knowledge, we use the LLM as a natural language interface to external knowledge. The first step is to convert the documents and any user queries into a compatible format to perform relevancy search (convert text into vectors, or embeddings). The original user prompt is then appended with relevant / similar documents within the external knowledge source (as a context). The model then answers the questions based on the provided external context."],"metadata":{"id":"MBvMcfQekDZq"}},{"cell_type":"markdown","source":["## 23.5 Limitations and Ethical Considerations of LLMs <a name='23.5-limitations-and-ethical-considerations-of-llms'></a>"],"metadata":{"id":"RUNB0noQXlQz"}},{"cell_type":"markdown","source":["Although LLMs have demonstrated impressive performance across a wide range of  tasks, there are several limitations and ethical considerations that raise concerns.\n","\n","Limitations:\n","\n","- *Computational resources*: Training LLMs requires significant computational resources, making it difficult for researchers with limited access to GPUs or specialized hardware to develop and finetune these models.\n","- *Data bias*: LLMs are trained on vast amounts of data from the internet, which often contain biases present in the real world. As a result, the models may unintentionally learn and reproduce biases in their predictions and generated text.\n","- *Lack of understanding*: LLMs may not truly \"understand\" language in the way humans do. They are often sensitive to small perturbations in the inputs and can generate plausible-sounding but nonsensical text.\n","- *Inability to explain*: LLMs are inherently black-box models, making it challenging to explain their reasoning or decision-making processes, which is essential in certain applications like healthcare, finance, and legal domains.\n","\n","Hallucinations: A hallucination is when a LLM produces an output that is false, or that does not match the user's intent. For example, claiming that it is human, that it has emotions, or that it is in love with the user. Because large language models predict the next syntactically correct word or phrase, they can't wholly interpret human meaning. The result can sometimes be what is referred to as a \"hallucination.\"\n","\n","Ethical Considerations:\n","\n","- *Privacy concerns*: LLMs memorize information from their training data, and can potentially reveal sensitive information or violate user privacy.\n","- *Misinformation and manipulation*: LLMs can generate coherent and contextually relevant text, which can be exploited to create disinformation, fake news, or deepfake content that manipulates public opinion and undermines trust.\n","- *Accessibility and fairness*: The computational resources and expertise required to train LLMs may lead to an unequal distribution of benefits, with only a few organizations having the resources to develop and control these powerful models.\n","- *Environmental impact*: The large-scale training of LLMs consumes a significant amount of energy, contributing to the carbon footprint and raising concerns about the environmental sustainability of these models.\n","\n","It is important to encourage transparency, collaboration, and responsible AI practices to ensure that LLMs benefit all members of society without causing harm."],"metadata":{"id":"WHbgR7cBc7jQ"}},{"cell_type":"markdown","source":["## 23.6 LLMs and Foundation Models <a name='23.6-llms-and-foundation-models'></a>\n","\n","A foundation model refers to any model trained on broad data that can be adapted to a wide range of downstream tasks. These models are typically created using deep neural networks and trained using self-supervised learning on many unlabeled data. Foundation models are typically finetuned with further training for various downstream cognitive tasks.\n","\n","LLMs are examples of foundation models, since they can be adapted for various NLP tasks. The term foundation model is more general, and includse large models trained on multimodal data, such as a mix of text, images, audio, etc."],"metadata":{"id":"BnFBFVAEe1my"}},{"cell_type":"markdown","source":["## References <a name='references'></a>\n","\n","1. Introduction to Large Language Models, by Bernhard Mayrhofer, available at [https://github.com/datainsightat/introduction_llm](https://github.com/datainsightat/introduction_llm).\n","2. Understanding Encoder and Decoder LLMs, by Sebastian Raschka, available at [https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder).\n","3. LLM Training: RLHF and Its Alternatives, by Sebastian Raschka, available at [https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives).\n","4. Training Language Models to Follow Instructions with Human Feedback, by Long Ouyang et al., available at [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155).\n","5. Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA), by Sebastian Raschka, available at [https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html).\n","6. How to Fine-tune Llama 2 With LoRA, by Derrick Mwiti, available at [https://www.mldive.com/p/how-to-fine-tune-llama-2-with-lora](https://www.mldive.com/p/how-to-fine-tune-llama-2-with-lora).\n","7. Fine-Tuning Llama 2.0 with Single GPU Magic, by Chee Kean, available at [https://ai.plainenglish.io/fine-tuning-llama2-0-with-qloras-single-gpu-magic-1b6a6679d436](https://ai.plainenglish.io/fine-tuning-llama2-0-with-qloras-single-gpu-magic-1b6a6679d436).\n","8. Fine-Tuning LLaMA 2 Models using a single GPU, QLoRA and AI Notebooks, by Mathieu Busquet, available at [https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/](https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/).\n","9. Getting started with Llama, by Meta AI, available at [https://ai.meta.com/llama/get-started/](https://ai.meta.com/llama/get-started/)."],"metadata":{"id":"Hlg6Y1COc8U_"}},{"cell_type":"markdown","source":["[BACK TO TOP](#top)"],"metadata":{"id":"gw7AvgH2nS41"}}]}