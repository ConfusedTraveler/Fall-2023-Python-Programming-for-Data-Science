{"cells":[{"cell_type":"markdown","metadata":{"id":"3JhHIQqNe4Qs"},"source":["# Lecture 19 - Natural Language Processing"]},{"cell_type":"markdown","metadata":{"id":"kPG5INqg-_qn"},"source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2023-Python-Programming-for-Data-Science/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_19-Natural_Language_Processing/Lecture_19-NLP.ipynb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2023-Python-Programming-for-Data-Science/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_19-Natural_Language_Processing/Lecture_19-NLP.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"oR4oHH5b5fP1"},"source":["<a id='top'></a>"]},{"cell_type":"markdown","metadata":{"id":"iEkmemKte4Qv"},"source":["- [19.1 Introduction to NLP](#19.1-introduction-to-nlp)\n","- [19.2 Preprocessing Text Data](#19.2-preprocessing-text-data)\n","- [19.3 Text Tokenization](#19.3-text-tokenization)\n","    - [19.3.1 Character-level Tokens](#19.3.1-character-level-tokens)\n","    - [19.3.2 Word-level Tokens](#19.3.2-word-level-tokens)\n","    - [19.3.3 Padding Word Sequences](#19.3.3-padding-word-sequences)\n","- [19.4 Representation of Groups of Words](#19.4-representation-of-groups-of-words)\n","- [19.5 Sequence Models Approach](#19.5-sequence-models-approach)\n","    - [19.5.1 Word Embeddings](#19.5.1-word-embeddings)\n","    - [19.5.2 Using TextVectorization Layer](#19.5.2-using-textvectorization-layer)\n","    - [19.5.3 Sequence Modeling with Recurrent Neural Networks](#19.5.3-sequence-modeling-with-recurrent-neural-networks)\n","- [References](#references)"]},{"cell_type":"markdown","metadata":{"id":"gTFgm_bJe4Qw"},"source":["## 19.1 Introduction to NLP <a name='19.1-introduction-to-nlp'></a>"]},{"cell_type":"markdown","metadata":{"id":"UFMFGCdwiw5k"},"source":["**Natural Language Processing (NLP)** is a branch of Computer Science (and more broadly, a branch of Artificial Intelligence) that is concerned with providing computers with the ability to understand texts and human language.\n","\n","Common tasks in NLP include:\n","\n","- *Text classification* — assign a class label to text based on the topic discussed in the text, e.g., sentiment analysis (positive or negative movie review), spam detection, content filtering (detect abusive content).\n","- *Text summarization/reading comprehension* — summarize a long input document with a shorter text.\n","- *Speech recognition* — convert spoken language to text.\n","- *Machine translation* — convert text in a source language to a target language.\n","- *Part of Speech (PoS) tagging* — mark up words in text as nouns, verbs, adverbs, etc.\n","- *Question answering* — output an answer to an input question.\n","- *Dialog generation* — generate the next reply in a conversation given the history of the conversation.\n","- *Text generation/language modeling* — generate text to complete the sentence or to complete the paragraph.\n"]},{"cell_type":"markdown","metadata":{"id":"GF5bogcC7mUa"},"source":["## 19.2 Preprocessing Text Data <a name='19.2-preprocessing-text-data'></a>"]},{"cell_type":"markdown","metadata":{"id":"rWMSWyTqjNym"},"source":["In order to perform operations with text data, they first need to be converted into a numerical representation.\n","\n","Converting text data into numerical form for processing by ML models typically involves the following steps:\n","\n","- *Standardization* - remove punctuation, convert the text to lowercase.\n","- *Tokenization* - break up the text into tokens (e.g., tokens can be individual words, several consecutive words (N-grams), or individual characters).\n","- *Indexing* - assign a numerical index to each token in the training set (i.e., vocabulary). In modern ML models, the step of indexing is typically replaced with the step of *embedding* which involves assigning a numerical vector to each token (e.g., one-hot encoding and word-embedding are explained in Section 19.5 below).\n"]},{"cell_type":"markdown","metadata":{"id":"cc6LF4pHjNyn"},"source":["### Text Standardization\n","\n","**Text standardization** usually includes some or all of the following steps, depending on the application:\n","\n","- Remove punctuation marks (such as comma, period) or non-alphabetic characters (@, #, {, ]).\n","- Change all words to lower-case letters, since ML models should consider *Text* and *text* as the same word.\n","\n","Some NLP tasks can apply additional steps, such as:\n","\n","- Correct spelling errors or replace abbreviations with full words.\n","- Remove stop words, such as *for*, *the*, *is*, *to*, *some*, etc.; if the task is text classification, these words are not relevant to the meaning of the text.\n","- Apply stemming and lemmatization, which transforms words to their base form, such as changing the word *changing* to *change*, or *grilled* to *grill* since they have a common root.\n","\n","Applying text standardization is helpful for training ML models, because the models do not need to consider *Text* and *text* as two different words, which reduces the requirements for large training datasets. However, depending on the application, text standardization may remove information that can be important for some tasks, and this should always be considered when performing text preprocessing."]},{"cell_type":"markdown","metadata":{"id":"k7RF_VR3jNyo"},"source":["### Tokenization\n","\n","**Tokenization** is breaking up a sequence of text into individual units called *tokens*.\n","\n","Tokenization can be performed at different levels:\n","\n","- *Character-level tokenization* - where the text is divided into individual characters, and each character is a token, including letters, digits, punctuation marks, and symbols. One of the traditional character encoding techniques is ASCII (American Standard Code Information Interchange). ASCII allows to convert any character to a numerical value. One disadvantage of this type of tokenization is that antigrams (words with same letters in different order, such as *silent* and *listen*) can have the same numerical encoding, which can affect the performance of ML models. Consequently, character-level tokenization is not widely used in practice.\n","- *Word-level tokenization* - where each word is a token. It provides a natural representation of the text with the words as building blocks of language. This type of tokenization is the most commonly used.\n","- *Subword-level tokenization* - where the words are divided into smaller units (e.g., tokenizing the word \"unhappiness\" into two tokens \"un\" + \"happiness\"). In some languages with complex word structures, subword-level tokenization is more suitable.\n","- *N-gram tokenization* - where N consecutive words represent a token. For instance, N-grams consisting of two adjacent words are called bigrams, or three words constitute a trigram, etc. N-gram tokens preserve the words order and can potentially capture more information in the text. For instance, for spam filtering using  bigram tokens such as *mailing list* or *bank account* may provide more helpful information than using word-level tokens.\n","\n","For some NLP tasks, tokenization can also be performed at other levels, such as sentence-level tokenizaiton for document segmentation task."]},{"cell_type":"markdown","metadata":{"id":"NbD7810XjNyp"},"source":["An example of text standardization, word-level tokenization, and indexing is shown in the next figure.\n","\n","<img src=\"images/tokenization.png\" width=\"500\">\n","\n","*Figure: Text standardization, word-level tokenization, and indexing.*"]},{"cell_type":"markdown","metadata":{"id":"u2SDR8h32fE9"},"source":["## 19.3 Text Tokenization <a name='19.3-text-tokenization'></a>"]},{"cell_type":"markdown","metadata":{"id":"9tDuayp80W1M"},"source":["Keras provides a text preprocessing function `Tokenizer` for converting raw text into sequences of tokens. The `Tokenizer` performs text standardization, tokenization, and indexing.\n","\n","The Kears `Tokenizer` has the following arguments:\n","\n","- *num_words*: the maximum number of words to keep in the input text. It is better to set a high number if we are not sure, because if we set a number less than the words in the text, some words will not be tokenized.\n","- *filters*: by default, all punctuations and special characters in the text will be removed. If we want to change that, we can provide a list of punctuations and characters to keep.\n","- *lower*: can be True or False. By default, it is True, and that means all texts will be converted to lowercase.\n","- *split*: separator for splitting words. A default separator is a space `(\" \")`.  \n","- *char_level*: can be True or False. By default, it is False and will perform word-level tokenization. If it is True, the function will perform character-level tokenization.\n","- *oov_token*: oov stands for Out Of Vocabulary, and it denotes a special token that will replace tokens that are not present in the input text."]},{"cell_type":"markdown","metadata":{"id":"hA9dZFJejNys"},"source":["### 19.3.1 Character-level Tokens <a name='19.3.1-character-level-tokens'></a>"]},{"cell_type":"markdown","metadata":{"id":"eP3igoipEMDW"},"source":["To use the `Tokenizer` for character-level tokenization, we need to set `char_level` to `True`. Let's set the number of tokens to 1,000.\n","\n","Let's apply it to the following sentence by using  the method `fit_on_texts()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4226,"status":"ok","timestamp":1698691435407,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"alBwiPau5LC9","outputId":"004bb44a-0b7f-481a-cedb-8230ef81ffa7","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version:2.14.0\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","\n","# Print the version of tf\n","print(\"TensorFlow version:{}\".format(tf.__version__))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNUzPUWIDfGi","tags":[]},"outputs":[],"source":["# A sample sentence\n","sentence = ['TensorFlow is a Machine Learning framework']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vr496JvAEEIN","tags":[]},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer(num_words=1000, char_level=True)\n","\n","# Fitting tokenizer on sentences\n","tokenizer.fit_on_texts(sentence)"]},{"cell_type":"markdown","metadata":{"id":"_n39kkPvGQV7"},"source":["When the `Tokenizer` separates the characters in text, it creates a  dictionary that maps each character to an integer index. We can inspect the dictionary by using the attribute `word_index`, although since we have set `char_level` to `True` in this case it is the character index.\n","\n","Note that the start index is 1. By default, all letters are converted to lowercase. The first token is an empty space `' '`, the second is the letter `'e'`, etc. There are 17 unique characters in the sentence, including the empty space."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698691435732,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"R7ztDhDIGPuo","outputId":"88948c92-6790-40e5-c459-419e74bbbee7","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["{' ': 1, 'e': 2, 'n': 3, 'r': 4, 'a': 5, 'o': 6, 'i': 7, 's': 8, 'f': 9, 'l': 10, 'w': 11, 'm': 12, 't': 13, 'c': 14, 'h': 15, 'g': 16, 'k': 17}\n"]}],"source":["char_index = tokenizer.word_index\n","print(char_index)"]},{"cell_type":"markdown","metadata":{"id":"I2L349BGjNyu"},"source":["The method `text_to_sequences` outputs the indices for the text. You can check that the word `TensorFlow` has the indices 13, 2, 3, 8, 6, 4, 9, 10, 6, 11, where each index corresponds to the letters listed in `char_index`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698691435732,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"JAPxvWUljNyv","outputId":"1645fe8a-176c-4b2b-81f1-8a238e9464a4","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[13, 2, 3, 8, 6, 4, 9, 10, 6, 11, 1, 7, 8, 1, 5, 1, 12, 5, 14, 15, 7, 3, 2, 1, 10, 2, 5, 4, 3, 7, 3, 16, 1, 9, 4, 5, 12, 2, 11, 6, 4, 17]]\n"]}],"source":["print(tokenizer.texts_to_sequences(sentence))"]},{"cell_type":"markdown","metadata":{"id":"MzhZW63ljNyv"},"source":["As we mentioned earlier, character-level tokenization is rarely used, because it does not capture semantic meaning of words as effectively as word-level tokens."]},{"cell_type":"markdown","metadata":{"id":"mkfbQe9XjNyv"},"source":["### 19.3.2 Word-level Tokens <a name='19.3.2-word-level-tokens'></a>"]},{"cell_type":"markdown","metadata":{"id":"uemMRwYrGwpb"},"source":["To use the `Tokenizer` for tokenizing words instead of characters, we need to just change the argument `char_level` to `False`, which is the default setting, so we may as well just omit it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CmuIbnlKjNyv","tags":[]},"outputs":[],"source":["# Sample sentences\n","sentences = ['TensorFlow is a Machine Learning framework.',\n","             'Keras is a well designed deep learning API!',\n","             'Keras is built on top of TensorFlow!']"]},{"cell_type":"markdown","metadata":{"id":"vk1fTO2yjNyw"},"source":["After the text is broken down into individual words, the `Tokenizer` builds a *vocabulary* of all words that are found in the input text, and assigns a unique integer index to each word in the vocabulary. We can inspect the words by using again the attribute `word_index`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1698691436031,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"tpIE4S3PHCbY","outputId":"35a08973-1555-4c58-8bcf-0c86ab857ed4","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["{'is': 1, 'tensorflow': 2, 'a': 3, 'learning': 4, 'keras': 5, 'machine': 6, 'framework': 7, 'well': 8, 'designed': 9, 'deep': 10, 'api': 11, 'built': 12, 'on': 13, 'top': 14, 'of': 15}\n"]}],"source":["tokenizer = Tokenizer(num_words=1000)\n","\n","# Fitting tokenizer on sentences\n","tokenizer.fit_on_texts(sentences)\n","\n","word_index = tokenizer.word_index\n","print(word_index)"]},{"cell_type":"markdown","metadata":{"id":"NGmvr6I2GwwJ"},"source":["There are 15 unique words in the above sentences. By default, all punctuations are removed and all letters are converted to lowercase.\n","\n","The indices for the above three sentences are shown below. For instance, the first list `[2, 1, 3, 6, 4, 7]` represents the first sentence in the text `TensorFlow is a Machine Learning framework`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1698691436031,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"9Xz_E4PXjNyx","outputId":"6b3b223c-d6ad-4fb4-d85d-b3967681ade8","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[2, 1, 3, 6, 4, 7], [5, 1, 3, 8, 9, 10, 4, 11], [5, 1, 12, 13, 14, 15, 2]]\n"]}],"source":["print(tokenizer.texts_to_sequences(sentences))"]},{"cell_type":"markdown","metadata":{"id":"ZAJYp5sAIstj"},"source":["Also, `word_counts` can return the number of times each word appears in the sentences."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1698691436032,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"XOb-6DGTIIDp","outputId":"869f6aa3-519f-40b8-c26a-58ba7b7a9f50","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('tensorflow', 2),\n","             ('is', 3),\n","             ('a', 2),\n","             ('machine', 1),\n","             ('learning', 2),\n","             ('framework', 1),\n","             ('keras', 2),\n","             ('well', 1),\n","             ('designed', 1),\n","             ('deep', 1),\n","             ('api', 1),\n","             ('built', 1),\n","             ('on', 1),\n","             ('top', 1),\n","             ('of', 1)])"]},"metadata":{},"execution_count":9}],"source":["word_counts = tokenizer.word_counts\n","word_counts"]},{"cell_type":"markdown","metadata":{"id":"LZE0QY8NjNyx"},"source":["#### Out of Vocabulary Words"]},{"cell_type":"markdown","metadata":{"id":"e4832s6thIYV"},"source":["To handle the case when the Tokenizer is applied to text that contains words that were not present in the original documents, we can define a special token `oov_token`. This token will be used to replace these words that are Out Of Vocabulary (OOV).\n","\n","In the example below, we set the `oov_token`, which has been assigned the index `1`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1698691436032,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"EHy4egiNh1dg","outputId":"6dadfdf1-28e9-4208-a58e-6a31932c76cc","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["{'Word Out of Vocab': 1, 'is': 2, 'tensorflow': 3, 'a': 4, 'learning': 5, 'keras': 6, 'machine': 7, 'framework': 8, 'well': 9, 'designed': 10, 'deep': 11, 'api': 12, 'built': 13, 'on': 14, 'top': 15, 'of': 16}\n"]}],"source":["tokenizer = Tokenizer(num_words=1000, oov_token='Word Out of Vocab')\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1698691436032,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"9tT78VLKjNyy","outputId":"9ee45fea-cfc7-47b6-e58b-435576e32ecf","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[3, 2, 4, 7, 5, 8], [6, 2, 4, 9, 10, 11, 5, 12], [6, 2, 13, 14, 15, 16, 3]]\n"]}],"source":["# Converting text to sequences\n","print(tokenizer.texts_to_sequences(sentences))"]},{"cell_type":"markdown","metadata":{"id":"2Bnw7dUui5Kv"},"source":["Next, if we pass text with new words that the tokenizer was not fit to, the new words will be replaced with the `oov_token`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1698691436032,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"x7mHGlEjjOG_","outputId":"0a1b5b20-56fc-481d-8ad0-326ffe7e0e52","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 1, 3], [6, 2, 4, 1, 11, 5, 12]]\n"]}],"source":["new_sentences = ['I like TensorFlow', # 'I' and 'like' are new words\n","                'Keras is a superb deep learning API'] # 'superb' is a new word\n","\n","print(tokenizer.texts_to_sequences(new_sentences))"]},{"cell_type":"markdown","metadata":{"id":"jj4rtaDbjNyz"},"source":["And also, if we work with a large dataset that contains many documents, we can limit the number of words in the vocabulary to 20,000 or 30,000, and consider the rare words as out-of-vocabulary words. This can reduce the input space of the model, by ignoring those words that are present only once or twice in the large database."]},{"cell_type":"markdown","metadata":{"id":"zt7W6Gdflok5"},"source":["### 19.3.3 Padding Word Sequences <a name='19.3.3-padding-word-sequences'></a>"]},{"cell_type":"markdown","metadata":{"id":"lUfBkgJimHpo"},"source":["Most machine learning models require the input samples to have the same length/size. In Keras, the function `pad_sequences()` can be used to pad the text sequences with predefined values, so that they have the same length.\n","\n","The function `pad_sequences()` accepts the following arguments:\n","\n","- *sequence*: a list of integer indices (i.e., tokenized text).\n","- *maxlen*: maximum length of all sequences; if not provided, sequences will be padded to the length of the longest sequence.\n","- *padding*: 'pre' (default) or 'post', whether to pad before the sequence or after the sequence.\n","- *truncating*: 'pre' (default) or post', whether to remove the values from sequences larger than maxlen at the beginning or at the end of the sequences.\n","- *value*: a float or a string to use as a padding value. By default, the sequences are padded with 0.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1698691436032,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"-gtdBQdVRHEc","outputId":"2c6e2579-5844-4493-c55d-4a7dd88bd77d","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[3, 2, 4, 7, 5, 8], [6, 2, 4, 9, 10, 11, 5, 12], [6, 2, 13, 14, 15, 16, 3]]\n"]}],"source":["tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n","\n","print(tokenized_sentences)"]},{"cell_type":"markdown","metadata":{"id":"qgLaHzlYrf3j"},"source":["The next cell shows the above sequences pre-padded with 0 to sequences with length 10."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1698691436032,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"7i5cMbLQrthF","outputId":"70e505fb-1e9f-4e78-df3f-782740798331","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0  0  0  0  3  2  4  7  5  8]\n"," [ 0  0  6  2  4  9 10 11  5 12]\n"," [ 0  0  0  6  2 13 14 15 16  3]]\n"]}],"source":["from keras.preprocessing.sequence import pad_sequences\n","\n","padded_sequences = pad_sequences(tokenized_sentences, maxlen=10)\n","\n","print(padded_sequences)"]},{"cell_type":"markdown","metadata":{"id":"OdhmWKLAooyP"},"source":["## 19.4 Representation of Groups of Words <a name='19.4-representation-of-groups-of-words'></a>"]},{"cell_type":"markdown","metadata":{"id":"acxXXFMPopRA"},"source":["Representation of groups of words in Machine Learning models for text processing includes two categories of approaches:\n","\n","- **Set models** approach, the text is represented as unordered collection of words. Such approaches include *bag-of-words* models.\n","- **Sequence models** approach, where the text is represented as ordered sequences of words. These methods preserve the order of the words in the text. Representatives of these approaches are Recurrent Neural Networks, and Transformer Networks.\n","\n","The order of words in natural language is not necessarily fixed, and sentences with different orders of the words can have the same meaning. Also, different languages use different ways to order the words. As a result, defining the order of the words in text in NLP tasks is not straightforward.\n","\n","### Bag-of-Words Models\n","\n","**Bag-of-words** models discard the information about the order of the words, where the term *bag* implies that the structure of the text is lost. A depiction of a bag-of-words is shown below, where the initial text is separated into word-level tokens, and a bag is created from all words in the text. Also, instead of individual words, these models often employ N-gram representations. This type of models typically consider the frequency of occurrence of each word in the training data, and a classifier is trained by using the word counts as inputs.\n","\n","For instance, to create a spam filtering classifier, two bags-of-words can be created from the words in spam and non-spam emails. Presumably, the spam bag will contain trigger words (such as cheap, buy, stock) more frequently than the bag with words from non-spam emails. A classifier will be trained using the two bags-of-words and learn to differentiate trigger words from regular words. After the training, the classifier will analyze the words in new unseen messages, and predict the probability that these words belong to the spam or non-spam bag-of-words."]},{"cell_type":"markdown","metadata":{"id":"4qBsT78wDv2O"},"source":["<img src=\"images/bag_of_words.png\" width=\"600\">\n","\n","*Figure: Bag-of-words representation.*"]},{"cell_type":"markdown","metadata":{"id":"GGmSvyoAEvHy"},"source":["The early applications of machine learning in NLP relied on bag-of-words models. Modern applications, especially those related to large language models, rely predominantly on sequence models. Before 2018, Recurrent Neural Networks were the preferred models for NLP applications. In recent years, Transformer Networks have replaced Recurrent Neural Networks as more powerful models for NLP tasks."]},{"cell_type":"markdown","metadata":{"id":"iJ2D6GsMsAiB"},"source":["## 19.5 Sequence Models Approach <a name='19.5-sequence-models-approach'></a>"]},{"cell_type":"markdown","metadata":{"id":"R-6vSeeksNq-"},"source":["**Sequence models** process the entire text sequence at once, which allows preserving the order of words in the input text. Typical implementation of sequence models includes the steps of representing the words in text data with integer indices, mapping the integers to vector representations, and passing the vectors to a machine learning model, where the layers in the model will account for the ordering of input vectors.\n","\n","The input vectors to sequence models can be in the form of:\n","\n","- One-hot word vector representation, or\n","- Word embeddings representation."]},{"cell_type":"markdown","metadata":{"id":"6fGeH8L8t67N"},"source":["### One-Hot Word Vector Representation"]},{"cell_type":"markdown","metadata":{"id":"SFjNamLou4pe"},"source":["**One-hot word vector** representation is similar to encoding categorical features with one-hot encoding matrix. That is, the index for each word is converted to one-hot vector, having `1 (hot)` for that word and `0 (cold)` for all other words. An example is shown in the left-hand figure, where we created a zero vector with length of 4, and assigned 1 for the index that corresponds to  every word. Another example if shown in the right-hand figure.\n","\n","<img src=\"images/one_hot_encodding.png\" width=\"800\">\n","\n","*Figure: One-hot word vector encoding.*"]},{"cell_type":"markdown","metadata":{"id":"jGcbgsmjyho6"},"source":["One-hot word vector representation is not an efficient way to represent text, because for large text datasets the input vectors can become quite large. For instance, a training set with 20,000 words will need to use one-hot vectors of size 20,000 to represent each word, and this results in slow training, as well as this type of word representation takes a lot of memory space.\n","\n","Using word embeddings is more efficient, since the vectors for word representation are much smaller than the size of the vocabulary, and more importantly, embedding vectors can capture important semantic meaning of the words. Hence, most modern NLP models rely on word embeddings for word representating words in text."]},{"cell_type":"markdown","metadata":{"id":"7A4VE9IqyAFd"},"source":["### 19.5.1 Word Embeddings <a name='19.5.1-word-embeddings'></a>"]},{"cell_type":"markdown","metadata":{"id":"rI04lGrByAZH"},"source":["**Word embeddings** representation is used to convert each word into a vector (also referred to as embedding vector), in such as way that the vectors of words that have similar semantic meaning have close spatial positions in the embeddings space.\n","\n","The embeddings space consists of the set of vectors, where each word in the vocabulary is represented with one vector. For calculating the distance between the vectors in the embeddings space, typically the cosine similarity is used as a distance metric. For two vectors $u$ and $v$, *cosine similarity* is calculated as the dot (scalar, inner) product of the vectors divided by the norm of the vectors, i.e., $\\dfrac{u\\cdot v}{||u||\\cdot ||v||}$.\n","\n","Typical vectors for representing word embeddings have between 256 to 1,024 dimensions. For instance, the following figure shows the embedding vector for the word 'work'. The embedding vector has many values, and each value represents some aspect of the meaning of that word.\n","\n","<img src=\"images/embedding_vector.png\" width=\"700\">\n","\n","*Figure: Embedding vector for the word 'work'.* Source: [link](https://ig.ft.com/generative-ai/)\n","\n","\n","The embedding vectors of words that have similar meanings are also similar. In the following figure, we can see that the embedding vectors of the words 'football' and 'soccer' are more similar to each other, than the embedding vectors of the words 'sea' or 'we'.\n","\n","<img src=\"images/embeddings_comparison.png\" width=\"500\">\n","\n","*Figure: Embedding vectors for words with similar meanings are also similar.* Source: [link](https://ig.ft.com/generative-ai/)\n","\n","\n","A simple example of word embeddings space is shown below, where similar words are positioned closer to each other. Therefore, the spatial distance between the vectors is dependent on the semantic meaning of the words.\n","\n","<img src=\"images/word_embeddings.png\" width=\"500\">\n","\n","*Figure: Word embeddings space.* Source: [link](https://ig.ft.com/generative-ai/)\n","\n","\n","Two popular methods for generating word embeddings are *word2vec* and *Glove*. These methods use neural networks to learn embedding vectors from a large corpus of text. The resulting vectors learned by these techniques can be imported as pretrained word embeddings and applied to downstream tasks with smaller training datasets.\n","\n","For example, the website  [Embedding Projector](http://projector.tensorflow.org) provides visualizations of word embeddings, and for an entered word displays other words that are adjacent in the embeddings space.\n","\n","To demonstrate the use of word embeddings with Keras, we will implement it for a sentiment analysis task, to classify movie reviews using the IMDB Reviews dataset."]},{"cell_type":"markdown","metadata":{"id":"OpTdRWWgzs0p"},"source":["#### Loading the IMDB Reviews Dataset"]},{"cell_type":"markdown","metadata":{"id":"TMO8oqNCzwsL"},"source":["IMDB Reviews Dataset can be downloaded from the built-in datasets in Keras.  There are 25,000 samples of movie reviews for training and 25,000 samples for validation. Setting `max_features` to 20,000 means we are only considering the first 20,000 words and the rest of the words will have the out-of-vocabulary token. Each movie review has a positive or negative label.\n","\n","The training and validation datasets will be loaded as lists with 25,000 elements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvfoy6M82QI9","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698691446855,"user_tz":360,"elapsed":10831,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"92cdb5d1-d381-4f1a-a4ba-f12c89631e48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n"]}],"source":["max_features = 20000\n","\n","(train_data, train_labels), (val_data, val_labels) = keras.datasets.imdb.load_data(num_words=max_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1698691446855,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"xx-Hxo8m_oc4","outputId":"2c95fc55-df12-48c4-b366-7d0481defe70","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["25000\n","25000\n"]}],"source":["print(len(train_data))\n","print(len(val_data))"]},{"cell_type":"markdown","metadata":{"id":"-DmqzQ-eAU1e"},"source":["Displayed below is one example of a movie review. It is a list of indices, it contains 141 words, and as we can see the words in the dataset are already converted to integer indices."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1698691446856,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"MXKWTG9Pai4C","outputId":"c00a8f2e-7f1f-4960-c785-93679884a7be","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of words in the third review 141\n","[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n"]}],"source":["# Display the third movie review\n","print('Number of words in the third review', len(train_data[2]))\n","print(train_data[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1698691446856,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"Prz3AruWmAb5","outputId":"dd75faab-052e-4d78-d84d-e46ef1329a40","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0])"]},"metadata":{},"execution_count":18}],"source":["# Display the first 10 train labels\n","train_labels[:10]"]},{"cell_type":"markdown","metadata":{"id":"xjd3rQSVaMhR"},"source":["#### Preparing the Dataset"]},{"cell_type":"markdown","metadata":{"id":"qgqbiz6p2liy"},"source":["Let's pad the data using the `pad_sequences` function in Keras. Setting `maxlen` indicates to use the first 200 words in each movie review, and ignore the rest. Most movie reviews in the dataset are shorter than 200 words, however for those that are longer than 200 words some information will be lost. That is a tradeoff between computational expense and model performance.\n","\n","We can see in the next cell that for the third review, which has a length of 141 words, the first 59 words are now 0, and the length is 200."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R85gam_ua92K","tags":[]},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","\n","train_data = pad_sequences(train_data, maxlen=200)\n","val_data = pad_sequences(val_data, maxlen=200)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1698691448231,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"KqHhsOm5BhtH","outputId":"a176c0a6-6ee5-4e48-cf24-ea1658022553","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the third padded review: (25000, 200) \n","\n","[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    1   14   47    8   30   31    7    4  249  108    7\n","    4 5974   54   61  369   13   71  149   14   22  112    4 2401  311\n","   12   16 3711   33   75   43 1829  296    4   86  320   35  534   19\n","  263 4821 1301    4 1873   33   89   78   12   66   16    4  360    7\n","    4   58  316  334   11    4 1716   43  645  662    8  257   85 1200\n","   42 1228 2578   83   68 3912   15   36  165 1539  278   36   69    2\n","  780    8  106   14 6905 1338   18    6   22   12  215   28  610   40\n","    6   87  326   23 2300   21   23   22   12  272   40   57   31   11\n","    4   22   47    6 2307   51    9  170   23  595  116  595 1352   13\n","  191   79  638   89    2   14    9    8  106  607  624   35  534    6\n","  227    7  129  113]\n"]}],"source":["# Display the third movie review\n","print('Shape of the third padded review:', train_data.shape, '\\n')\n","print(train_data[2])"]},{"cell_type":"markdown","metadata":{"id":"qlyWGixEViI_"},"source":["#### Embedding Layer in Keras\n","\n","Keras has `Embedding` layer, which we will use to project the input tokens into vectors in an embedding space. The `Embedding` layer requires at the minimum to specify the number of possible tokens in the data sequences, and the dimensionality of the vectors in the embeddings space. The layer takes integer indices as inputs, and outputs a feature vector. It can be considered as a look-up table, which maps an embedding vector to each integer index.\n","\n","To understand how the Embedding layer works, let's consider a dataset with the maximum number of words set to 100, and out aim is to represent the words with 5-dimensional vectors. In the cell below, the Embedding layer assigned random values to the list of indices 1, 2, and 3, and we can see that to each index a 5-dimensional vector is assigned. However, the embedding vectors are trainable, and when we include the Embedding layer in a model, as we train the model, words that are similar will get closer in the embeddings space."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5338,"status":"ok","timestamp":1698691453565,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"6xIZ5fkxQrB8","outputId":"a317adfa-87e8-49e2-887e-b7aa6f7e86cb","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.04004519,  0.0458275 , -0.04573163, -0.02836338,  0.00029951],\n","       [-0.03247341,  0.00142381, -0.00083622, -0.04686186, -0.02296721],\n","       [ 0.04272026,  0.00845009, -0.04817088, -0.04544125, -0.03160852]],\n","      dtype=float32)"]},"metadata":{},"execution_count":21}],"source":["from keras.layers import Embedding\n","\n","# embedding layer: represent a dataset with a vocabulary of 100 words with 5 dimensional vectors\n","embedding_layer = Embedding(input_dim=100, output_dim=5)\n","\n","embed_integers = embedding_layer(tf.constant([1, 2, 3]))\n","\n","embed_integers.numpy()"]},{"cell_type":"markdown","metadata":{"id":"9SA5YTIfQ1_1"},"source":["#### Define, Compile, and Train the Model\n","\n","Next, we will define a model that uses an `Embedding` layer to project the words in input sequences into 8-dimensional vectors. These vectors will be further processed through dense layers, and the last layer will predict the label of movie reviews. There are two labels: positive and negative movie review, therefore this is a binary classification problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PnBvxyc4UaIA","tags":[]},"outputs":[],"source":["from keras.layers import Flatten, Dense, Dropout\n","from keras import Sequential\n","\n","embedding_dim = 8\n","\n","# Create a model\n","model = Sequential([\n","       Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=200),\n","       Flatten(),\n","       Dense(32, activation='relu'),\n","       Dropout(0.5),\n","       Dense(1, activation='sigmoid')\n","])"]},{"cell_type":"markdown","metadata":{"id":"m3w5rY1jWc07"},"source":["We will compile the model with `binary_crossentropy` loss (two labels: positive and negative review) and `adam` optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pa9wGfvlWYtB","tags":[]},"outputs":[],"source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"l4lU4IG4W1K6"},"source":["Before training the model, we can see the model summary."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1698691453963,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"LNJhXZIMWzmZ","outputId":"cc31c086-357b-4969-a364-55cb805b115d","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 200, 8)            160000    \n","                                                                 \n"," flatten (Flatten)           (None, 1600)              0         \n","                                                                 \n"," dense (Dense)               (None, 32)                51232     \n","                                                                 \n"," dropout (Dropout)           (None, 32)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 211265 (825.25 KB)\n","Trainable params: 211265 (825.25 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143845,"status":"ok","timestamp":1698691597802,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"N_um8S3dW4-j","outputId":"092ea08d-5491-4898-8811-661f97aed4c4","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","782/782 [==============================] - 75s 85ms/step - loss: 0.4589 - accuracy: 0.7602 - val_loss: 0.2947 - val_accuracy: 0.8741\n","Epoch 2/3\n","782/782 [==============================] - 18s 23ms/step - loss: 0.1971 - accuracy: 0.9291 - val_loss: 0.3094 - val_accuracy: 0.8733\n","Epoch 3/3\n","782/782 [==============================] - 13s 17ms/step - loss: 0.0863 - accuracy: 0.9736 - val_loss: 0.3993 - val_accuracy: 0.8626\n"]}],"source":["history = model.fit(train_data, train_labels, validation_data = (val_data, val_labels), epochs=3)"]},{"cell_type":"markdown","metadata":{"id":"R45E02hfskwZ"},"source":["### 19.5.2 Using TextVectorization Layer <a name='19.5.2-using-textvectorization-layer'></a>"]},{"cell_type":"markdown","metadata":{"id":"BaIlzPVGh54c"},"source":["Keras also provides another way to preprocess text by using a TextVectorization layer.\n","\n","This layer performs the following preprocessing steps:\n","\n","* Standardize text by removing punctuations and lowering the text case.\n","* Split sentences into individual tokens.\n","* Convert the tokens into a numerical representation.\n","\n","The arguments in `TextVectorization` layer are:\n","\n","- *max_tokens*: maximum number of tokens in the vocabulary, where vocabulary is comprised of the unique text units (words) in the data.\n","- *standardize*: denotes the standardization specifics to be applied to input data; by default, it is `lower_and_strip_punctuation' meaning to convert to lowercase and remove punctuations.\n","- *split*: denotes what will be considered while splitting the input text; by default it is whitespace `\" \"`.\n","- *output_sequence_length*: the length to which the sequences will be padded or truncated."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfkdnmQkrSYw","tags":[]},"outputs":[],"source":["from keras.layers import TextVectorization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x76D5avVlivO","tags":[]},"outputs":[],"source":["# Sample sentences\n","sentences = ['TensorFlow is a deep learning library!',\n","             'Is TensorFlow powered by Keras API?']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7eYMFefqscFA","tags":[]},"outputs":[],"source":["text_vect_layer = TextVectorization(max_tokens=1000, output_sequence_length=10)"]},{"cell_type":"markdown","metadata":{"id":"aaOetzBexcPA"},"source":["We can use the `adapt()` method to fit the sentences to the TextVectorization layer. The `adapt()` method will preprocess the data, and it will create a vocabulary to be used later to convert text into a numerical representation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdYJMqYGuf4L","tags":[]},"outputs":[],"source":["text_vect_layer.adapt(sentences)"]},{"cell_type":"markdown","metadata":{"id":"UXT4hQTQxiZP"},"source":["Let's pass a sample sentence to inspect the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyQWnWL-xDV0","tags":[]},"outputs":[],"source":["sample_sentence = 'Tensorflow is a machine learning framework!'\n","\n","vectorized_sentence = text_vect_layer([sample_sentence])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1698691599436,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"1bnFLF8xZTC0","outputId":"c7af47c4-3e6e-4f1a-a944-e70464c63979","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Orginal sentence: Tensorflow is a machine learning framework!\n","Vectorized sentence: tf.Tensor([[ 2  3 11  1  6  1  0  0  0  0]], shape=(1, 10), dtype=int64)\n"]}],"source":["print('Orginal sentence:', sample_sentence)\n","print('Vectorized sentence:', vectorized_sentence)"]},{"cell_type":"markdown","metadata":{"id":"No7WUUUhylIG"},"source":["Since the words `'machine'` and `'framework'` were not part of the `sentences` that we passed to the layer, they are both represented by `1` in the vectorized output, since the index 1 is reserved for words that are out of vocabulary(`oov_token`).\n","\n","The output is padded with 0, and the length of the output sequence size is 10.\n","\n","The `TextVectorization` layer performs all required text preprocessing steps at once, and another advantage of this layer is that it can be used inside a model."]},{"cell_type":"markdown","metadata":{"id":"ztGDcuYujNzp"},"source":["### 19.5.3 Sequence Modeling with Recurrent Neural Networks <a name='19.5.3-sequence-modeling-with-recurrent-neural-networks'></a>"]},{"cell_type":"markdown","metadata":{"id":"b6ABwz_Am75n"},"source":["**Recurrent Neural Networks (RNN)** is a  neural network architecture that is designed for handling sequential data. Examples of sequential data are time-series, texts (sequence of words or characters), audio (sequence of sound waves), etc.\n","\n","Working with sequential data requires to preserve the sequence of the information flow in the data. For example, given the sentence `Today, I took my cat for a [....]`, to predict the next word, there should be a way to capture and preserve the flow from the beginning to the end of the sequence.\n","\n","In conventional feedforward networks (such as networks composed of fully-connected or convolutional layers), the information flows from the input layer to the output layer. Conversely, in RNNs, there is a feedback loop at each time step, which creates the *recurrence*. This is shown in the next figure, where at each time step of the RNN model, an input (e.g., word) is processed, then in the next step the succeeding word is processed based on the information from the previous word, etc. This way, the network can learn dependencies between words that are not adjacent.\n","\n","<img src=\"images/rnn.png\" width=\"600\">\n","\n","*Figure: Recurrent Neural Network.*\n"]},{"cell_type":"markdown","metadata":{"id":"2xBfAgEAzEm2"},"source":["There are three major types of RNN layers: conventional (a.k.a. basic, simple, vanilla) RNN, LSTM (Long Short-Term Memory), and GRU (Gated Recurrent Units). They are implemented in Keras and PyTorch, and can be conveniently imported and used for creating models. In Keras, the conventional (basic) RNN is called SimpleRNN, and LSTM and GRU are called as they are written.\n","\n","While SimpleRNN has difficulty in handling long sequences, LSTM and GRU have the ability to store and preserve long-term dependencies over many time steps. Consequently, SimpleRNNs are rarely used at present.\n","\n","Both LSTM and GRU layers use multiple gates to control to flow of information between the time steps. For instance, LSTM layers include an input gate and an output gate to control the input and output information for each time step, a forget gate that removes irrelevant information, and a memory cell that saves important information.\n","\n","Next, we will apply an RNN model with LSTM layers for classification of text data."]},{"cell_type":"markdown","metadata":{"id":"OyjIIjZzRlev"},"source":["#### Loading the Data"]},{"cell_type":"markdown","metadata":{"id":"0DAL7PtzV7gg"},"source":["We are going to use the `ag_news_subset` dataset that is available in TensorFlow datasets. AG is a collection of news articles gathered from more than 2000 news sources. The news articles are classified into 4 classes: World(0), Sports(1), Business(2), and Sci/Tech(3). The total number of training samples is 120,000 and testing 7,600.\n","\n","Let's get the dataset from TensorFlow datasets. In the load function, `with_info=True` will return various information about the dataset (as shown in the next cells), and 'as_supervised=True` indicates that the data will be loaded as 2-element tuples consisting of (input, target) pairs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8fnaQT4CE9G","tags":[]},"outputs":[],"source":["import tensorflow_datasets as tfds\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JIKIZDRQwVV","colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["a102331967874efe9592211666c9645e","ef659d9e1165464b8c532ecdb143f843","fb08a5a2ac5d465498010ad2b31cfbac","17617c4405cc42cfb048921557febf03","c6f32376dd654488a1a2e72aad425b68","731526ab2c68415fb85288e05827f40e","1739e8b5d2424f11aafa7d017c9e7aec","3a1e54ea6efe4350af31fec145576a8f","fef56b802b42457aa102edfe8921ad9c","b5ad7cfc6b1541dcb94f7b2572246f36","e9dad4a3527f447e992e267a5333df7a","32abb59a6c384b8ab05cd012da23af3e","7e37f5e2c27640dab399c1cc71424cfb","3ca979b502014660a01a2cb9519ff222","4eece73b74f7459e94328d43c8cb7513","9ac6fa79e5db41dc989c247775f380c2","eb0347c28aec49b3aab17d43f5f91194","c016efc7071d4dfdbd62cf5015af0bee","f9473bebce324bce9b0222522790c683","19b7e1b9bc994b829437e1b003a13fb4","2c240e5c7d614095857d8962f183c7ea","999fde5ab26642d4bb6ebd0c102a244c","58973cec108b4bcb99b8acadb1f4575f","430f6edebf8c466ba44ef4cea41a4073","e155e595b70e42538c055e85211b2b16","a208d70d07d0412fb86ed50920b17a65","3732737d747b460eafeaa4b53cb32688","57c41c0894e247528a693b874922d2d3","8e9975badd8a4296839b53733bc074b3","b9e7217adb394822b18acbd934efad7c","193f3493ba494baa8d1a8a90eee06b24","ae367b0f22c041f99d08e8ff802bc6c0","be72d285ef5f42d2aeb0ead39c918f51","1e180ca54353442f8b922ae6227d804a","38c881d39f3a4a229295500c756416ba","760b255865e240eb83245996068688ce","e72f463f85bc42bea8c44d24582ba929","198beeaa0ec441ae856d38e985b2957d","214d15da10c74923b28ec8739a9a9f61","49732bf4e9a2416aa631bb59ce69bec9","d08049cb2c16410ab3e508c9c86144de","8b7669a9a4d54e84a9cf6542f0a8f235","8bd3b4d54f1548059ccd0f62e09eaf47","8a07734b60e04641affd72277b008898","d995cd86ea184459bb8e92ba8d9910ec","e9ede8ee28e745728a99a73cf7389368","e848a1e44a5948f1aaac931e42d260ba","7a1cc8b19d74401ea7220419561fc902","cec821a8461f4a0bbe257d9f27a363d9","acbeeac514a24999afb7e015bc748e19","3d93f0e59723474483da1f8cb1471a19","c3132e043ebd4e59a057d762966a1b0b","bb769e09bf4643a287e9c236c18a9f79","3bbe162317894632923e46e3f330379e","2985222b8d5a45089e0d870c04ee2502","53e8b0a8f86d4d7f949527ef2e9efbba","63819a08c59c484ea8cbc648a3524490","f308fd67bd2b4651a51251e898d2a32f","710735ed1bdd4c51be099ec8ad9686fa","b93f9ebb539a4eb3a097f5696cac7793","2ea964f03a424ce3ab97dc446ac25b69","c0af1e8f2a874bb48bc8519d1e2cf360","802211b84ab248e3b8b47a3c59a67342","06d85a87b68447de8d47bd9c41e99713","071c5c3625d34575b1bacb1dfb47acfd","24c007429fe3473f8ebe7d59de1d71c2","a7343c19f8164c4ea8857503b38a4c62","3ab7b61e7a794eccaef465992ee2cf67","a7a8bd7b72c84164ac709252c3b5348b","47ced747a9384ea0a3232791ab9af843","eb8412c3bb9844a7882ddb2a8d7d38be","9d0b98fd70bd4c968060101f59f4fd17","adfe4b87dcdb4d7c9e0fc04291e1a85e","2410b2839964447099c7a6447a789fa3","cadeb351a60540fb84a55ba72bdbad05","dabe3b1a80a3416f9e425a04e65e9f02","1f493933bed44f259964e1161ae23b89","09cdda56ba5b44b4b2f7d8aee8e24fd2","55d2f4ef25264888b62640683613cfdc","105b53fe34c54720b3ce26305146d53a","89b853b142924e6aae8d4accd2f401c7","3bda5d3d8c404601a8f8c17709a209c4","fc97b7cf59b14252a16c59b98721c49b","5468ccc9f12f4a6fbfae7c2232aabbff","af9681ca6df24a0188352ad66ddc29c6","451ae28777cd4f7ebb9eb0ca9962ea94","0f0537c1ba8445c3b768e60e8cb8d5aa","b6b593a20e024ec7bdecf7e6087bbeb8"]},"executionInfo":{"status":"ok","timestamp":1698691629797,"user_tz":360,"elapsed":28685,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"316df678-764d-4f86-9b5b-42c12e6e52e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset 11.24 MiB (download: 11.24 MiB, generated: 35.79 MiB, total: 47.03 MiB) to /root/tensorflow_datasets/ag_news_subset/1.0.0...\n"]},{"output_type":"display_data","data":{"text/plain":["Dl Completed...: 0 url [00:00, ? url/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a102331967874efe9592211666c9645e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Dl Size...: 0 MiB [00:00, ? MiB/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32abb59a6c384b8ab05cd012da23af3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extraction completed...: 0 file [00:00, ? file/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58973cec108b4bcb99b8acadb1f4575f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e180ca54353442f8b922ae6227d804a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train examples...:   0%|          | 0/120000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d995cd86ea184459bb8e92ba8d9910ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incomplete665NZN/ag_news_subset-train.tfrecord*...:  …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e8b0a8f86d4d7f949527ef2e9efbba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test examples...:   0%|          | 0/7600 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7343c19f8164c4ea8857503b38a4c62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incomplete665NZN/ag_news_subset-test.tfrecord*...:   …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09cdda56ba5b44b4b2f7d8aee8e24fd2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset ag_news_subset downloaded and prepared to /root/tensorflow_datasets/ag_news_subset/1.0.0. Subsequent calls will reuse this data.\n"]}],"source":["(train_data, val_data), info = tfds.load('ag_news_subset:1.0.0', #version 1.0.0\n","                                         split=['train', 'test'],\n","                                         with_info=True,\n","                                         as_supervised=True)"]},{"cell_type":"markdown","metadata":{"id":"378IQW1eSVlz"},"source":["We can use `info` to check basic information about the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1698691629798,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"03IBh4d1S1eJ","outputId":"3c3c584b-3719-4bfe-bd66-c74a92f1a39a"},"outputs":[{"output_type":"stream","name":"stdout","text":["['World', 'Sports', 'Business', 'Sci/Tech']\n"]}],"source":["# Displaying the classes\n","class_names = info.features['label'].names\n","print(class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1698691629799,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"uUzqSrGwY7dU","outputId":"8e9aadc4-36a1-4b38-a149-dd34260c71bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training samples: 120000\n","Number of validation samples: 7600\n"]}],"source":["print('Number of training samples:', info.splits['train'].num_examples)\n","print('Number of validation samples:', info.splits['test'].num_examples)"]},{"cell_type":"markdown","metadata":{"id":"_AGay5wWZ_xs"},"source":["We can use `tfds.as_dataframe` to display the first 10 news articles as  Pandas DataFrame.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":318},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1698691630040,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"kGj0TRaXaHG1","outputId":"320ddb82-3fc8-4a27-f6a5-405b1694fd1e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                         description  label\n","0  b'AMD #39;s new dual-core Opteron chip is desi...      3\n","1  b'Reuters - Major League Baseball\\\\Monday anno...      1\n","2  b'President Bush #39;s  quot;revenue-neutral q...      2\n","3  b'Britain will run out of leading scientists u...      3\n","4  b'London, England (Sports Network) - England m...      1\n","5  b'TOKYO - Sony Corp. is banking on the \\\\$3 bi...      0\n","6  b'Giant pandas may well prefer bamboo to lapto...      3\n","7  b'VILNIUS, Lithuania - Lithuania #39;s main pa...      0\n","8  b'Witnesses in the trial of a US soldier charg...      0\n","9  b'Dan Olsen of Ponte Vedra Beach, Fla., shot a...      1"],"text/html":["\n","  <div id=\"df-985f4714-5e40-495c-87a1-a1ed8ae5e77a\" class=\"colab-df-container\">\n","    <style type=\"text/css\">\n","</style>\n","<table id=\"T_08280\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_08280_level0_col0\" class=\"col_heading level0 col0\" >description</th>\n","      <th id=\"T_08280_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_08280_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_08280_row0_col0\" class=\"data row0 col0\" >AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.</td>\n","      <td id=\"T_08280_row0_col1\" class=\"data row0 col1\" >3 (Sci/Tech)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_08280_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_08280_row1_col0\" class=\"data row1 col0\" >Reuters - Major League Baseball\\Monday announced a decision on the appeal filed by Chicago Cubs\\pitcher Kerry Wood regarding a suspension stemming from an\\incident earlier this season.</td>\n","      <td id=\"T_08280_row1_col1\" class=\"data row1 col1\" >1 (Sports)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_08280_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_08280_row2_col0\" class=\"data row2 col0\" >President Bush #39;s quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.</td>\n","      <td id=\"T_08280_row2_col1\" class=\"data row2 col1\" >2 (Business)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_08280_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_08280_row3_col0\" class=\"data row3 col0\" >Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.</td>\n","      <td id=\"T_08280_row3_col1\" class=\"data row3 col1\" >3 (Sci/Tech)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_08280_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_08280_row4_col0\" class=\"data row4 col0\" >London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.</td>\n","      <td id=\"T_08280_row4_col1\" class=\"data row4 col1\" >1 (Sports)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_08280_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n","      <td id=\"T_08280_row5_col0\" class=\"data row5 col0\" >TOKYO - Sony Corp. is banking on the \\$3 billion deal to acquire Hollywood studio Metro-Goldwyn-Mayer Inc...</td>\n","      <td id=\"T_08280_row5_col1\" class=\"data row5 col1\" >0 (World)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_08280_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n","      <td id=\"T_08280_row6_col0\" class=\"data row6 col0\" >Giant pandas may well prefer bamboo to laptops, but wireless technology is helping researchers in China in their efforts to protect the engandered animals living in the remote Wolong Nature Reserve.</td>\n","      <td id=\"T_08280_row6_col1\" class=\"data row6 col1\" >3 (Sci/Tech)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_08280_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n","      <td id=\"T_08280_row7_col0\" class=\"data row7 col0\" >VILNIUS, Lithuania - Lithuania #39;s main parties formed an alliance to try to keep a Russian-born tycoon and his populist promises out of the government in Sunday #39;s second round of parliamentary elections in this Baltic country.</td>\n","      <td id=\"T_08280_row7_col1\" class=\"data row7 col1\" >0 (World)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_08280_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n","      <td id=\"T_08280_row8_col0\" class=\"data row8 col0\" >Witnesses in the trial of a US soldier charged with abusing prisoners at Abu Ghraib have told the court that the CIA sometimes directed abuse and orders were received from military command to toughen interrogations.</td>\n","      <td id=\"T_08280_row8_col1\" class=\"data row8 col1\" >0 (World)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_08280_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n","      <td id=\"T_08280_row9_col0\" class=\"data row9 col0\" >Dan Olsen of Ponte Vedra Beach, Fla., shot a 7-under 65 Thursday to take a one-shot lead after two rounds of the PGA Tour qualifying tournament.</td>\n","      <td id=\"T_08280_row9_col1\" class=\"data row9 col1\" >1 (Sports)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-985f4714-5e40-495c-87a1-a1ed8ae5e77a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-985f4714-5e40-495c-87a1-a1ed8ae5e77a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-985f4714-5e40-495c-87a1-a1ed8ae5e77a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-bc916153-60d8-4c06-a719-f5b041dc85fd\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bc916153-60d8-4c06-a719-f5b041dc85fd')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-bc916153-60d8-4c06-a719-f5b041dc85fd button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_a4ad777e-f303-46e4-9bd1-eb97fcd115ad\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('news_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_a4ad777e-f303-46e4-9bd1-eb97fcd115ad button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('news_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":36}],"source":["news_df = tfds.as_dataframe(train_data.take(10), info)\n","\n","news_df"]},{"cell_type":"markdown","metadata":{"id":"ksGa0Z4JaJDx"},"source":["The columns in the DataFrame are `description` and `label`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1698691630041,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"omL7uadpazuc","outputId":"1c2e6d9d-24c2-4279-905f-d447dfee04ff"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['description', 'label'], dtype='object')"]},"metadata":{},"execution_count":37}],"source":["news_df.columns"]},{"cell_type":"markdown","metadata":{"id":"F-jPA6TEd3sw"},"source":["Now that we understand the data, let's prepare it before we can use LSTMs to classify the news.  "]},{"cell_type":"markdown","metadata":{"id":"EKjyyw21bRzM"},"source":["#### Preparing the Data"]},{"cell_type":"markdown","metadata":{"id":"w-KMfmo0dOWc"},"source":["First, we will shuffle and batch the training data. For the validation data, we don't shuffle, we only batch it.\n","\n","The `buffer_size` below limits the number of data points to be shuffled to 1000. This can be useful when working with large datasets, that can not fit in the memory.\n","\n","The `prefetch()` function below is only added for optimizing the performance, and while the model is being trained, it will prefetch batches for validation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3e-BTCUfmdr"},"outputs":[],"source":["buffer_size = 1000\n","batch_size = 32\n","\n","train_data = train_data.shuffle(buffer_size)\n","train_data = train_data.batch(batch_size).prefetch(1)\n","val_data = val_data.batch(batch_size).prefetch(1)"]},{"cell_type":"markdown","metadata":{"id":"MEIRO6ZlphLl"},"source":["To convert the text data into tokens, in this case we will use the TextVectorizer layer in Keras."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tu8u1i612_sd"},"outputs":[],"source":["max_features = 20000\n","\n","text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_features)"]},{"cell_type":"markdown","metadata":{"id":"JGW-6Q7YXYUU"},"source":["Next, we will apply the `adapt()` method to preprocess the training data. Since the data was loaded as (input, label) tuples, the `lambda` function applies the vectorizer only on the input features (in the column `description`), and not on the `labels`. The vectorization is applied only to the input text, and not to the labels.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4RDwZoxVG5Np"},"outputs":[],"source":["text_vectorizer.adapt(train_data.map(lambda description, label: description))"]},{"cell_type":"markdown","metadata":{"id":"GfiumAc0rriT"},"source":["Let's pass two news articles to `text_vectorizer`. The vectorized sequences will be padded with the maximum sentences, but if we want to have fixed size, we can set the `output_sequence_length` to another value in the layer initialization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hb4d3EqrrMuU"},"outputs":[],"source":["sample_news = ['This weekend there is a sport match between Man U and Fc Barcelona',\n","               'Tesla has unveiled its humanoid robot that appeared dancing during the show!']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1698691645736,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"B7cQZbSIsjhA","outputId":"b21e1b52-449f-4861-fb1b-029c821cfdd9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[   40,   491,   185,    16,     3,  1559,   560,   163,   362,\n","        13418,     7,  7381,  2517],\n","       [    1,    20,   878,    14,     1,  4663,    10,  1249, 11657,\n","          159,     2,   541,     0]])"]},"metadata":{},"execution_count":42}],"source":["vectorized_news = text_vectorizer(sample_news)\n","vectorized_news.numpy()"]},{"cell_type":"markdown","metadata":{"id":"QDiIP1tFtRYs"},"source":["Note that the second sentence was padded with 0. Also the words `Tesla` and `humanoid` have an index of 1 because they were not a part of the training data."]},{"cell_type":"markdown","metadata":{"id":"TUNqJkjpusFA"},"source":["#### Creating and Training the Model"]},{"cell_type":"markdown","metadata":{"id":"e7Vobpziu4Zw"},"source":["We are going to create a Keras model that takes the tokenized text as input and outputs the class of the news articles.\n","\n","The model has the following layers:\n","\n","* `TextVectorization layer` for converting input texts into tokens.\n","* `Embedding layer` for representing the tokens with a trainable feature vector. Because the feature vector is trainable, the words that have similar semantic meanings will be close feature vectors in the embedding space.\n","* `LSTM layer` for processing the sequences. The layer is wrapped into a Bidirectional layer, which will process the sequences from both directions (forward and backward), i.e., one LSTM layer will process the sequences forward, another layer will process the sequences backward, and the outputs of the two LSTMs will be combined.\n","* `Dense layer` for classification purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1698691645737,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"jnRD1ymezwok","outputId":"40d8fc0d-90b0-4f25-952d-19f49db7a352"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["20000"]},"metadata":{},"execution_count":43}],"source":["input_dim = len(text_vectorizer.get_vocabulary())\n","input_dim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApxFc7DouIAX"},"outputs":[],"source":["model = tf.keras.Sequential([\n","    text_vectorizer,\n","    tf.keras.layers.Embedding(input_dim=input_dim, output_dim=64, mask_zero=True),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(4, activation='softmax')\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3dtgRIH1BL_"},"outputs":[],"source":["# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":565572,"status":"ok","timestamp":1698692213875,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"AbJ0GXUi1ToF","outputId":"fc0c4a1f-5dcc-4deb-cf6b-e2756fa05da3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","3750/3750 [==============================] - 139s 33ms/step - loss: 0.3386 - accuracy: 0.8812 - val_loss: 0.2779 - val_accuracy: 0.9050\n","Epoch 2/5\n","3750/3750 [==============================] - 92s 24ms/step - loss: 0.2069 - accuracy: 0.9286 - val_loss: 0.2938 - val_accuracy: 0.9063\n","Epoch 3/5\n","3750/3750 [==============================] - 91s 24ms/step - loss: 0.1426 - accuracy: 0.9495 - val_loss: 0.3396 - val_accuracy: 0.9038\n","Epoch 4/5\n","3750/3750 [==============================] - 91s 24ms/step - loss: 0.0900 - accuracy: 0.9676 - val_loss: 0.4485 - val_accuracy: 0.8954\n","Epoch 5/5\n","3750/3750 [==============================] - 90s 24ms/step - loss: 0.0545 - accuracy: 0.9803 - val_loss: 0.5474 - val_accuracy: 0.8957\n"]}],"source":["# Train the model\n","history = model.fit(train_data, epochs=5, validation_data=val_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3363,"status":"ok","timestamp":1698692217231,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"d7ZdyxK77qfW","outputId":"90683563-5d0c-468b-fb36-df53c69a0c14"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 3s 3s/step\n","[[0.00460763 0.01687396 0.0096032  0.96891516]]\n"]}],"source":["# Predicting the class of new news articles\n","sample_news_1 = ['The self driving car company Tesla has unveiled its humanoid robot that appeared dancing during the show!']\n","\n","# make predictions on the sample_news 1\n","predictions_1 = model.predict(sample_news_1)\n","print(predictions_1)"]},{"cell_type":"markdown","metadata":{"id":"ZdAqAXUzN2Xe"},"source":["The model correctly predicted that the news article is related to tech or science."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1698692217232,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"XOzT9yra8K1o","outputId":"18e38fe3-2dc9-484a-d1c1-fb5811fc740f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: 3\n","Predicted class name: Sci/Tech\n"]}],"source":["# find the index of the predicted class\n","predicted_class_1 = np.argmax(predictions_1)\n","\n","print('Predicted class:', predicted_class_1)\n","print('Predicted class name:', class_names[predicted_class_1])"]},{"cell_type":"markdown","metadata":{"id":"XKxv86w5OWhG"},"source":["One more example is provided in the next cell."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1698692217514,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"NDXFamqkOHAD","outputId":"9ae684bc-9e57-4e16-925a-6c29f0bc5c62"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 36ms/step\n","Predicted class: 1\n","Predicted class name: Sports\n"]}],"source":["# Predicting the class of new news\n","sample_news_2 = ['This weekend there is a match between two big footbal teams in the national league']\n","\n","predictions_2 = model.predict(sample_news_2)\n","\n","predicted_class_2 = np.argmax(predictions_2)\n","\n","print('Predicted class:', predicted_class_2)\n","print('Predicted class name:', class_names[predicted_class_2])"]},{"cell_type":"markdown","metadata":{"id":"vweobvFVe4RB"},"source":["## References <a name='references'></a>\n","\n","1. Complete Machine Learning Package, Jean de Dieu Nyandwi, available at: [https://github.com/Nyandwi/machine_learning_complete](https://github.com/Nyandwi/machine_learning_complete).\n","2. Deep Learning with Python, Francois Chollet, Second Edition, Manning Publications, 2021."]},{"cell_type":"markdown","metadata":{"id":"bDbzcXmWe4RB"},"source":["[BACK TO TOP](#top)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a102331967874efe9592211666c9645e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef659d9e1165464b8c532ecdb143f843","IPY_MODEL_fb08a5a2ac5d465498010ad2b31cfbac","IPY_MODEL_17617c4405cc42cfb048921557febf03"],"layout":"IPY_MODEL_c6f32376dd654488a1a2e72aad425b68"}},"ef659d9e1165464b8c532ecdb143f843":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_731526ab2c68415fb85288e05827f40e","placeholder":"​","style":"IPY_MODEL_1739e8b5d2424f11aafa7d017c9e7aec","value":"Dl Completed...: 100%"}},"fb08a5a2ac5d465498010ad2b31cfbac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a1e54ea6efe4350af31fec145576a8f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fef56b802b42457aa102edfe8921ad9c","value":1}},"17617c4405cc42cfb048921557febf03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5ad7cfc6b1541dcb94f7b2572246f36","placeholder":"​","style":"IPY_MODEL_e9dad4a3527f447e992e267a5333df7a","value":" 1/1 [00:12&lt;00:00, 12.61s/ url]"}},"c6f32376dd654488a1a2e72aad425b68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"731526ab2c68415fb85288e05827f40e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1739e8b5d2424f11aafa7d017c9e7aec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a1e54ea6efe4350af31fec145576a8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"fef56b802b42457aa102edfe8921ad9c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5ad7cfc6b1541dcb94f7b2572246f36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9dad4a3527f447e992e267a5333df7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32abb59a6c384b8ab05cd012da23af3e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e37f5e2c27640dab399c1cc71424cfb","IPY_MODEL_3ca979b502014660a01a2cb9519ff222","IPY_MODEL_4eece73b74f7459e94328d43c8cb7513"],"layout":"IPY_MODEL_9ac6fa79e5db41dc989c247775f380c2"}},"7e37f5e2c27640dab399c1cc71424cfb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb0347c28aec49b3aab17d43f5f91194","placeholder":"​","style":"IPY_MODEL_c016efc7071d4dfdbd62cf5015af0bee","value":"Dl Size...: 100%"}},"3ca979b502014660a01a2cb9519ff222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9473bebce324bce9b0222522790c683","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19b7e1b9bc994b829437e1b003a13fb4","value":1}},"4eece73b74f7459e94328d43c8cb7513":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c240e5c7d614095857d8962f183c7ea","placeholder":"​","style":"IPY_MODEL_999fde5ab26642d4bb6ebd0c102a244c","value":" 11/11 [00:12&lt;00:00,  1.53 MiB/s]"}},"9ac6fa79e5db41dc989c247775f380c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb0347c28aec49b3aab17d43f5f91194":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c016efc7071d4dfdbd62cf5015af0bee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9473bebce324bce9b0222522790c683":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"19b7e1b9bc994b829437e1b003a13fb4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c240e5c7d614095857d8962f183c7ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"999fde5ab26642d4bb6ebd0c102a244c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58973cec108b4bcb99b8acadb1f4575f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_430f6edebf8c466ba44ef4cea41a4073","IPY_MODEL_e155e595b70e42538c055e85211b2b16","IPY_MODEL_a208d70d07d0412fb86ed50920b17a65"],"layout":"IPY_MODEL_3732737d747b460eafeaa4b53cb32688"}},"430f6edebf8c466ba44ef4cea41a4073":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57c41c0894e247528a693b874922d2d3","placeholder":"​","style":"IPY_MODEL_8e9975badd8a4296839b53733bc074b3","value":"Extraction completed...: 100%"}},"e155e595b70e42538c055e85211b2b16":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9e7217adb394822b18acbd934efad7c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_193f3493ba494baa8d1a8a90eee06b24","value":1}},"a208d70d07d0412fb86ed50920b17a65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae367b0f22c041f99d08e8ff802bc6c0","placeholder":"​","style":"IPY_MODEL_be72d285ef5f42d2aeb0ead39c918f51","value":" 4/4 [00:12&lt;00:00, 12.84s/ file]"}},"3732737d747b460eafeaa4b53cb32688":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57c41c0894e247528a693b874922d2d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e9975badd8a4296839b53733bc074b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9e7217adb394822b18acbd934efad7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"193f3493ba494baa8d1a8a90eee06b24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae367b0f22c041f99d08e8ff802bc6c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be72d285ef5f42d2aeb0ead39c918f51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e180ca54353442f8b922ae6227d804a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38c881d39f3a4a229295500c756416ba","IPY_MODEL_760b255865e240eb83245996068688ce","IPY_MODEL_e72f463f85bc42bea8c44d24582ba929"],"layout":"IPY_MODEL_198beeaa0ec441ae856d38e985b2957d"}},"38c881d39f3a4a229295500c756416ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_214d15da10c74923b28ec8739a9a9f61","placeholder":"​","style":"IPY_MODEL_49732bf4e9a2416aa631bb59ce69bec9","value":"Generating splits...: 100%"}},"760b255865e240eb83245996068688ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d08049cb2c16410ab3e508c9c86144de","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b7669a9a4d54e84a9cf6542f0a8f235","value":2}},"e72f463f85bc42bea8c44d24582ba929":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bd3b4d54f1548059ccd0f62e09eaf47","placeholder":"​","style":"IPY_MODEL_8a07734b60e04641affd72277b008898","value":" 2/2 [00:15&lt;00:00,  6.38s/ splits]"}},"198beeaa0ec441ae856d38e985b2957d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"214d15da10c74923b28ec8739a9a9f61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49732bf4e9a2416aa631bb59ce69bec9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d08049cb2c16410ab3e508c9c86144de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b7669a9a4d54e84a9cf6542f0a8f235":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bd3b4d54f1548059ccd0f62e09eaf47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a07734b60e04641affd72277b008898":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d995cd86ea184459bb8e92ba8d9910ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e9ede8ee28e745728a99a73cf7389368","IPY_MODEL_e848a1e44a5948f1aaac931e42d260ba","IPY_MODEL_7a1cc8b19d74401ea7220419561fc902"],"layout":"IPY_MODEL_cec821a8461f4a0bbe257d9f27a363d9"}},"e9ede8ee28e745728a99a73cf7389368":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acbeeac514a24999afb7e015bc748e19","placeholder":"​","style":"IPY_MODEL_3d93f0e59723474483da1f8cb1471a19","value":"Generating train examples...:  94%"}},"e848a1e44a5948f1aaac931e42d260ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3132e043ebd4e59a057d762966a1b0b","max":120000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb769e09bf4643a287e9c236c18a9f79","value":120000}},"7a1cc8b19d74401ea7220419561fc902":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bbe162317894632923e46e3f330379e","placeholder":"​","style":"IPY_MODEL_2985222b8d5a45089e0d870c04ee2502","value":" 112974/120000 [00:12&lt;00:00, 8484.57 examples/s]"}},"cec821a8461f4a0bbe257d9f27a363d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"acbeeac514a24999afb7e015bc748e19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d93f0e59723474483da1f8cb1471a19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3132e043ebd4e59a057d762966a1b0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb769e09bf4643a287e9c236c18a9f79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bbe162317894632923e46e3f330379e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2985222b8d5a45089e0d870c04ee2502":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53e8b0a8f86d4d7f949527ef2e9efbba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_63819a08c59c484ea8cbc648a3524490","IPY_MODEL_f308fd67bd2b4651a51251e898d2a32f","IPY_MODEL_710735ed1bdd4c51be099ec8ad9686fa"],"layout":"IPY_MODEL_b93f9ebb539a4eb3a097f5696cac7793"}},"63819a08c59c484ea8cbc648a3524490":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ea964f03a424ce3ab97dc446ac25b69","placeholder":"​","style":"IPY_MODEL_c0af1e8f2a874bb48bc8519d1e2cf360","value":"Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incomplete665NZN/ag_news_subset-train.tfrecord*...:  85%"}},"f308fd67bd2b4651a51251e898d2a32f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_802211b84ab248e3b8b47a3c59a67342","max":120000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_06d85a87b68447de8d47bd9c41e99713","value":120000}},"710735ed1bdd4c51be099ec8ad9686fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_071c5c3625d34575b1bacb1dfb47acfd","placeholder":"​","style":"IPY_MODEL_24c007429fe3473f8ebe7d59de1d71c2","value":" 101430/120000 [00:00&lt;00:00, 272720.58 examples/s]"}},"b93f9ebb539a4eb3a097f5696cac7793":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"2ea964f03a424ce3ab97dc446ac25b69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0af1e8f2a874bb48bc8519d1e2cf360":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"802211b84ab248e3b8b47a3c59a67342":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06d85a87b68447de8d47bd9c41e99713":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"071c5c3625d34575b1bacb1dfb47acfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24c007429fe3473f8ebe7d59de1d71c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7343c19f8164c4ea8857503b38a4c62":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ab7b61e7a794eccaef465992ee2cf67","IPY_MODEL_a7a8bd7b72c84164ac709252c3b5348b","IPY_MODEL_47ced747a9384ea0a3232791ab9af843"],"layout":"IPY_MODEL_eb8412c3bb9844a7882ddb2a8d7d38be"}},"3ab7b61e7a794eccaef465992ee2cf67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d0b98fd70bd4c968060101f59f4fd17","placeholder":"​","style":"IPY_MODEL_adfe4b87dcdb4d7c9e0fc04291e1a85e","value":"Generating test examples...:   0%"}},"a7a8bd7b72c84164ac709252c3b5348b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2410b2839964447099c7a6447a789fa3","max":7600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cadeb351a60540fb84a55ba72bdbad05","value":7600}},"47ced747a9384ea0a3232791ab9af843":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dabe3b1a80a3416f9e425a04e65e9f02","placeholder":"​","style":"IPY_MODEL_1f493933bed44f259964e1161ae23b89","value":" 0/7600 [00:00&lt;?, ? examples/s]"}},"eb8412c3bb9844a7882ddb2a8d7d38be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"9d0b98fd70bd4c968060101f59f4fd17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adfe4b87dcdb4d7c9e0fc04291e1a85e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2410b2839964447099c7a6447a789fa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cadeb351a60540fb84a55ba72bdbad05":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dabe3b1a80a3416f9e425a04e65e9f02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f493933bed44f259964e1161ae23b89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09cdda56ba5b44b4b2f7d8aee8e24fd2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55d2f4ef25264888b62640683613cfdc","IPY_MODEL_105b53fe34c54720b3ce26305146d53a","IPY_MODEL_89b853b142924e6aae8d4accd2f401c7"],"layout":"IPY_MODEL_3bda5d3d8c404601a8f8c17709a209c4"}},"55d2f4ef25264888b62640683613cfdc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc97b7cf59b14252a16c59b98721c49b","placeholder":"​","style":"IPY_MODEL_5468ccc9f12f4a6fbfae7c2232aabbff","value":"Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incomplete665NZN/ag_news_subset-test.tfrecord*...:   0%"}},"105b53fe34c54720b3ce26305146d53a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_af9681ca6df24a0188352ad66ddc29c6","max":7600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_451ae28777cd4f7ebb9eb0ca9962ea94","value":7600}},"89b853b142924e6aae8d4accd2f401c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f0537c1ba8445c3b768e60e8cb8d5aa","placeholder":"​","style":"IPY_MODEL_b6b593a20e024ec7bdecf7e6087bbeb8","value":" 0/7600 [00:00&lt;?, ? examples/s]"}},"3bda5d3d8c404601a8f8c17709a209c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"fc97b7cf59b14252a16c59b98721c49b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5468ccc9f12f4a6fbfae7c2232aabbff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af9681ca6df24a0188352ad66ddc29c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"451ae28777cd4f7ebb9eb0ca9962ea94":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f0537c1ba8445c3b768e60e8cb8d5aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6b593a20e024ec7bdecf7e6087bbeb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}