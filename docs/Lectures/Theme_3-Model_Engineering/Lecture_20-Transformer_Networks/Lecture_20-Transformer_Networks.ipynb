{"cells":[{"cell_type":"markdown","metadata":{"id":"3JhHIQqNe4Qs"},"source":["# Lecture 20 - Transformer Networks"]},{"cell_type":"markdown","metadata":{"id":"kPG5INqg-_qn"},"source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2023-Python-Programming-for-Data-Science/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_20-Transformer_Networks/Lecture_20-Transformer_Networks.ipynb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2023-Python-Programming-for-Data-Science/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_20-Transformer_Networks/Lecture_20-Transformer_Networks.ipynb)"]},{"cell_type":"markdown","source":["<a id='top'></a>"],"metadata":{"id":"wckYntAu-sjq"}},{"cell_type":"markdown","metadata":{"id":"iEkmemKte4Qv"},"source":["- [20.1 Introduction to Transformers](#20.1-introduction-to-transformers)\n","- [20.2 Self-attention Mechanism](#20.2-self-attention-mechanism)\n","- [20.3 Multi-head Attention](#20.3-multi-head-attention)\n","- [20.4 Encoder Block](#20.4-encoder-block)\n","- [20.5 Positional Encoding](#20.5-positional-encoding)\n","- [20.6 Using a Transformer Model for Classification](#20.6-using-a-transformer-model-for-classification)\n","- [20.7 Fine-tuning a Pretrained BERT Model](#20.7-fine-tuning-a-pretrained-bert-model)\n","- [20.8 Decoder Sub-network](#20.8-decoder-sub-network)\n","- [20.9 Vision Transformers](#20.9-vision-transformers)\n","- [References](#references)"]},{"cell_type":"markdown","metadata":{"id":"gTFgm_bJe4Qw"},"source":["## 20.1 Introduction to Transformers <a name='20.1-introduction-to-transformers'></a>"]},{"cell_type":"markdown","metadata":{"id":"UFMFGCdwiw5k"},"source":["**Transformer Neural Networks**, or simply **Transformers**, is a neural network architecture introduced in 2017 in the now-famous paper [“Attention is all you need”](https://arxiv.org/abs/1706.03762). The title refers to the attention mechanism, which forms the basis for data processing with Transformers.  \n","\n","Transformer Networks have been the predominant type of Deep Learning models  for NLP in recent years. They replaced Recurrent Neural Networks in all NLP tasks, and also, all Large Language Models employ the Transformer Network architecture. As well as, Transformer Networks were recently adapted for other tasks and have outperformed other Machine Learning models for image processing and video processing tasks, protein and DNA sequence prediction, time-series data processing, and have been used for reinforcement learning tasks. Consequently, Transformers are currently the most important Neural Network architecture."]},{"cell_type":"markdown","metadata":{"id":"GF5bogcC7mUa"},"source":["## 20.2 Self-attention Mechanism <a name='20.2-self-attention-mechanism'></a>"]},{"cell_type":"markdown","metadata":{"id":"rWMSWyTqjNym"},"source":["**Self-attention** in NNs is a mechanism that forces a model to attend to portions of the data when making predictions. For instance, in NLP, self-attention mechanism is used to identify words in sentences that have significance for a given query word in the sentence. That is, the model should pay more attention to some words in sentences, and less attention to other words in sentences that are less relevant for a given task.  \n","\n","In the following two sentences, in the left subfigure the word \"it\" refers to \"street\", while in the right subfigure the word \"it\" refers to \"animal\". Understanding the relationships between the words in such sentences has been challenging with traditional NLP approaches. Transformers use the self-attention mechanism to model the relationships between all words in a sentence, and assign weights to other words in sentences based on their importance. In the left subfigure, the mechanism estimated that the **query word** \"it\" is most related to the word \"street\", but the word \"it\" is also somewhat related to the words \"The\" and \"animal. These words are referred to as **key words** for the query word \"it\".The intensity of the lines connecting the words, as well as the intensity of the blue color, signifies the attention scores (i.e., weights). The wider and bluer the lines, the higher the attention scores between two words are.\n","\n","<img src=\"images/attn_1.png\" width=\"700\">\n","\n","*Figure: Attention to words in sentences.*"]},{"cell_type":"markdown","source":["Specifically, Transformer Network compares each word to every other word in the sentence, and calculates attention scores. This is shown in the next figure, where for example, the word \"caves\" has the highest **attention scores** for the words \"glacier\" and \"formed\". The attention scores are calculated as the dot (i.e., inner) product of the input representations of two words. That is, for each Query word $Q$ and Key word $K$, the attention score is $Q\\cdot K$.\n","\n","\n","<img src=\"images/attn_2.png\" width=\"300\">\n","\n","*Figure: Attention scores.*"],"metadata":{"id":"pR2M0cOujAnl"}},{"cell_type":"markdown","source":["Transformers employ word embeddings for representing the individual words in text sequences (where each text sequence can have one or several sentences). Recall from the previous lecture that **word embeddings** are vector representations of words, such that the vectors of words that have similar semantic meaning have close spatial positions in the embeddings space. Therefore, the attention scores are dot products of the embedding vectors for each pair of words in sentences.\n","\n","The obtained attention scores for each word are then first scaled (by dividing the values by $\\sqrt d$) and afterward are normalized to be in the [0,1] range (by applying a softmax function). That is, the attention scores are calculated as $a_{ij}=softmax(\\frac{Q_i\\cdot K_j}{\\sqrt d})$, where $d$ is the dimensionality of the embedding vectors. Scaling the values by $\\sqrt d$ is helpful for improving the flow of the gradients during training. The resulting scaled and normalized attention scores are then multiplied with the initial representation of the words, which in the self-attention module is referred to as **value** or $V$.\n","\n","This is shown in the next figure. The left subfigure shows the attention scores calculated as product of the input representations of the words $Q$ and $K$, which are afterwards multiplied with the input representation $V$ to obtain the output of the module. Note that for text classification, all three terms Query, Key, and Value are the same input representation of the words in sentences. However, the original Transformer was developed for machine translation, where the words in the target language are queries, and the words in the source language are pairs of keys and values. This terminology is also related to search engines, which compare queries to keys, and return values (e.g., the user submits a query, the search engine identifies key words within the query to search for, and it returns the results of the search as values). Self-attention works in a similar way, where each query word is matched to other key words, and a weighted value is returned.\n","\n","The right subfigure below shows how self-attention is implemented in Transformer Networks. Namely, `Matmul` stands for a matrix multiplication layer which calculates the dot product $Q\\cdot K$, which is afterwards scaled by $\\sqrt d$, then there is an optional masking layer , and afterward the final attention scores are obtained by passing it through a `Softmax` layer to obtain $softmax(\\frac{Q_i\\cdot K_j}{\\sqrt d})$. Finally, the attention scores are multiplied with $V$ via another matrix multiplication layer `Matmul` to calculate the output of the self-attention module. The optional masking layer can be used for two purposes: (a) to ensure that attention scores are not calculated for the padding tokens in padded sequences (e.g., 0 is often used as the padding token), but instead are calculated only for the positions in input sequences that have actual words in padded sequences; or (b) to set the attention scores for future tokens to zero, so that the model can only attend to previous tokens, as explained in the section below on decoder sub-networks).\n","\n","<img src=\"images/attn_3.png\" width=\"400\">\n","\n","*Figure: Self-attention in Transformer Networks*"],"metadata":{"id":"UiIXXTnSwvXs"}},{"cell_type":"markdown","source":["In conclusion, self-attention is applied to determine the meaning of the words in a sentence based on the context. That is, Transformers use the attention scores to modify the input vector representations for each word and generate a new representation based on the context of the sentence. During the training of the network, the representations of the words are updated and projected into a new embeddings space that takes the context into account."],"metadata":{"id":"It3c8ihEcKXV"}},{"cell_type":"markdown","source":["## 20.3 Multi-Head Attention <a name='20.3-multi-head-attention'></a>"],"metadata":{"id":"eYLfWGw6pN-Y"}},{"cell_type":"markdown","source":["Transformer Networks include multiple self-attention modules in their architecture. Each self-attention module is called **attention head**, and the aggregation of the outputs of multiple attention heads is called **multi-head attention**. For instance, the original Transformer model had 8 attention heads, while the GPT-3 language model has 12 attention heads.\n","\n","The multi-head attention module is shown in the next figure, where the inputs are first passed through a linear layer (dense or fully-connected layer), next they are fed to the multiple attention heads, and the outputs of all attention heads are concatenated, and passed through one more linear layer.\n","\n","A logical question one can ask is why are multiple attention heads needed? The reason is that multiple attention modules can learn different relationships between the words in sentences. Each module can extract context independently from the other modules, which allows to capture less obvious context and enhance the learning capabilities of the model. For example, one head may capture relationship between the nouns and numerical values in sentences, another head may focus on the relationship between the adjectives in sentences, and another head may focus on rhyming words, etc. And, if one head becomes too specialized in capturing one type of patterns, the other heads can compensate for it and provide redundancy that can improve the overall performance of the model.\n","\n","Also, the computations of each attention head can be performed in parallel on different workers, which allows for accelerating the training and scaling up the models.\n","\n","<img src=\"images/multihead_1.png\" width=\"600\">\n","\n","*Figure: Multi-head attention*"],"metadata":{"id":"-jIbvAu3qj02"}},{"cell_type":"markdown","source":["## 20.4 Encoder Block <a name='20.4-encoder-block'></a>"],"metadata":{"id":"cQJzrp1Kqk1O"}},{"cell_type":"markdown","source":["The **Encoder Block** in Transformer Networks is shown in the next figure. It processes the input word embeddings and extracts representations in text data that can afterwards be used for different NLP tasks.\n","\n","The components in the Encoder Block are:\n","\n","- *Multi-head Attention layer*, which as explained, consists of multiple self-attention modules.\n","- *Dropout layer*, is a regular dropout layer.\n","- *Residual connections*, are skip connections in neural networks, where the input to a layer is added to the processed output of the layer. Residual connections were popularized in the ResNets models, as they were shown to stabilize the training and mitigate the problems of *vanishing and exploding gradients* in neural networks (i.e., they refer to cases when the gradients become too small or too large during training). In the figure, the `Add` term in the layer refers to the residual connection, which adds the input embeddings to the output of the Dropout layer.\n","- *Layer Normalization*, is an operation that is similar to the batch normalization in CNNs, but instead, it normalizes the outputs of each multi-head attention layer independently from the outputs of the other multi-head attention layers, and scales the data to have 0 mean and 1 standard deviation. This type of normalization is more adequate for text data. And, as we learned in the previous lectures, normalization improves the flow of gradients during training. The `Norm` term in the figure refers to the Layer Normalization operation.\n","- *Feed Forward network*, consists of 2 fully-connected (dense) layers that extract useful data representations.\n","- The Encoder Block also contains one more *Dropout layer*, and another *Add & Norm* layer that forms a residual connection for the input to the Feed Forward network and applies a layer normalization operation.\n","\n","Larger Transformer networks typically include several encoder blocks in a sequence. For instance, in the original paper the authors used 6 encoder blocks.\n","\n","<img src=\"images/enc_1.png\" width=\"250\">\n","\n","*Figure: Encoder block*"],"metadata":{"id":"URSQk7VvFH8m"}},{"cell_type":"markdown","source":["The implementation of the Encoder Block in Keras and TensorFlow is shown in the cell following the imported libraries.\n","\n","The Encoder Block is implemented as a custom layer which is a subclass of the `Layer` class in Keras. The `__init__()` constructor method lists the definitions of the layers in the Encoder, and the method `call()` provides the forward pass with the flow of information through the layers.\n","\n","- *Multi-head attention* layer is implemented in Keras, and it can be directly imported. The arguments in the layer are: `num_heads` is the number of attention heads, and `key_dim` is the dimension of the embeddings of the input tokens.\n","- *Dropout* and *Normalization* layers are also directly imported, with arguments `rate` for the dropout rate, and `epsilon` is a small float added to the standard deviation to avoid division by 0.\n","- *Feed forward network* includes 2 dense layers, with the number of neurons set to `ff_dim` and `embed_dim`, respectively.\n","\n","The `call()` method specifies the forward pass of the network, and takes two parameters: `inputs` (the input embeddings to the network) and `training` (an argument which can be True or False). For the dropout layers, during the model training this argument is set to True and dropout is applied, while during inference the argument is set to False and dropout is not applied.\n","\n","Each step in the `call()` method performs the data processing for one layer. Note that the `multi_head_attention` layer has as arguments the `inputs` twice, which is once for the key and once for the value in the self-attention. Also note the residual connections that are implemented in the layer normalization, e.g., the inputs are added to the output of the multi-head attention."],"metadata":{"id":"7PT8eNvBMUwo"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Dense, Embedding, Layer\n","from keras import Sequential, Model"],"metadata":{"id":"31UvwJzcMVxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super().__init__()\n","        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.feed_forward_net = Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),])\n","        self.layer_normalization1 = LayerNormalization(epsilon=1e-6)\n","        self.layer_normalization2 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        multi_head_att_output = self.multi_head_attention(inputs, inputs)\n","        multi_head_att_dropout = self.dropout1(multi_head_att_output, training=training)\n","        add_norm_output_1 = self.layer_normalization1(inputs + multi_head_att_dropout)\n","        feed_forward_output = self.feed_forward_net(add_norm_output_1)\n","        feed_forward_dropout = self.dropout2(feed_forward_output, training=training)\n","        add_norm_output_2 = self.layer_normalization2(add_norm_output_1 + feed_forward_dropout)\n","        return add_norm_output_2"],"metadata":{"id":"wk1rur6XMgPd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 20.5 Positional Encoding <a name='20.5-positional-encoding'></a>"],"metadata":{"id":"CaQtYLw6u2WW"}},{"cell_type":"markdown","source":["We mentioned that Transformers use word embeddings as inputs, however, the embeddings alone don't provide information about the order of words in sentences. Understandably, the order of the words in a sentence is important, and different order of the words can convey a different meaning. To provide such information, Transformer Network introduces **positional encoding** for each word that is added to the input embedding, as shown in the next figure.  \n","\n","<img src=\"images/positional_encoding_1.png\" width=\"300\">\n","\n","*Figure: Positional encoding*"],"metadata":{"id":"idmC_UGSu2oU"}},{"cell_type":"markdown","source":["There are different ways in which positional encoding can be implemented. In the original Transformer paper, the positional encoding is a vector that has the same size as the word embedding vector, and the authors used sine and cosine functions to create position vectors, which are afterwards scaled to be in the range from -1 to 1. Using such positional encoding, each encoding vector corresponds to a unique position in a sequence of words. This type is called *sinusoidal positional encoding*.\n","\n","The following cell implements the addition of positional encoding to word embeddings in Keras. In this case, we will not use the approach for obtaining positional encodings based on sine and cosine functions, but instead we will use a simpler approach and learn the positional encodings in the same way the word embeddings are learned. This type of positional encoding is referred to as *learned positional encodings/embeddings*. Therefore, for both token and positional embeddings we will use the `Embedding` layer in Keras which we introduced in the previous lecture. The arguments in the `Embedding` layer are the input dimension `input_dim` and the dimension of the embedding vectors `output_dim`. For the token embeddings layer, the input dimension is the size of the vocabulary, whereas for the positional embeddings layer the input dimension is the length of the text sequences.\n","\n","\n","In the `call` method, first the length of the text sequences is assigned to `maxlen`. The function `tf.range` is similar to NumPy's `linspace` and creates numbers in the range from `start` to `limit` with a step `delta`. Next, the two separate `Embedding` layers are called, and returned is the sum of the token and positional embeddings."],"metadata":{"id":"kW2zngtqyrDG"}},{"cell_type":"code","source":["class TokenAndPositionEmbedding(Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super().__init__()\n","        self.token_embeddings = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.positional_embeddings = Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, inputs):\n","        maxlen = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        position_embeddings = self.positional_embeddings(positions)\n","        input_embeddings = self.token_embeddings(inputs)\n","        return input_embeddings + position_embeddings"],"metadata":{"id":"sE9FKBCwu3CH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 20.6 Using a Transformer Model for Classification <a name='20.6-using-a-transformer-model-for-classification'></a>"],"metadata":{"id":"cu4fWFqH7Xt_"}},{"cell_type":"markdown","source":["### Model Definition\n","\n","We will now employ the layers that we defined above, to create a Transformer model for text classification.\n","\n","It is a simple model that consists of the following parts:\n","\n","- **Encoder**, which includes an `Input` layer that defines the maximum length of input sequences, `TokenAndPositionEmbedding` layer, and the `TransformerEncoder` layer.\n","- **Classifier**, which consists of a `GlobalAveragePooling1D` layer, and two `Dropout` and `Dense` layers. Global Average Pooling calculates the average value for each word, and it passes those values to the dense layers to classify the text sequences."],"metadata":{"id":"CTj6ItI1dOiu"}},{"cell_type":"code","source":["from keras.layers import Input, GlobalAveragePooling1D\n","\n","maxlen = 200  # Maximum length of input sequences is 200 words\n","embed_dim = 32  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","ff_dim = 32  # Dense layer size in the feed forward network inside transformer\n","vocab_size = 20000  # The size of the vocabulary is 20k words\n","\n","# encoder\n","inputs = Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)(inputs)\n","x = TransformerEncoder(embed_dim, num_heads, ff_dim)(embedding_layer)\n","\n","# classifier\n","x = GlobalAveragePooling1D()(x)\n","x = Dropout(0.1)(x)\n","x = Dense(20, activation=\"relu\")(x)\n","x = Dropout(0.1)(x)\n","outputs = Dense(1, activation=\"sigmoid\")(x)\n","\n","model = Model(inputs=inputs, outputs=outputs)"],"metadata":{"id":"oCS2ZvAU7jC0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The summary of the model is shown below."],"metadata":{"id":"SVGtqsUu-QpA"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sya-beL_-Ljd","outputId":"80d138b1-5ea5-424c-e6b6-265a2227bd00","executionInfo":{"status":"ok","timestamp":1698951370252,"user_tz":360,"elapsed":906,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 200)]             0         \n","                                                                 \n"," token_and_position_embeddi  (None, 200, 32)           646400    \n"," ng (TokenAndPositionEmbedd                                      \n"," ing)                                                            \n","                                                                 \n"," transformer_encoder (Trans  (None, 200, 32)           10656     \n"," formerEncoder)                                                  \n","                                                                 \n"," global_average_pooling1d (  (None, 32)                0         \n"," GlobalAveragePooling1D)                                         \n","                                                                 \n"," dropout_2 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 20)                660       \n","                                                                 \n"," dropout_3 (Dropout)         (None, 20)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 21        \n","                                                                 \n","=================================================================\n","Total params: 657737 (2.51 MB)\n","Trainable params: 657737 (2.51 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["### Loading the Dataset\n","\n","Let's apply the model for sentiment analysis of the movie reviews in the IMDB database. The data is loaded from the Keras datasets, and it contains 25,000 training sequences and 25,000 validation sequences."],"metadata":{"id":"AlDGBEt8-e4J"}},{"cell_type":"code","source":["from keras.preprocessing.sequence import pad_sequences\n","\n","(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","print(len(x_train), \"Training sequences\")\n","print(len(x_val), \"Validation sequences\")\n","x_train = pad_sequences(x_train, maxlen=maxlen)\n","x_val = pad_sequences(x_val, maxlen=maxlen)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNe755zF-h_f","outputId":"213c06f8-a28a-4973-b93a-fa53fdb64885","executionInfo":{"status":"ok","timestamp":1698951383794,"user_tz":360,"elapsed":5885,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 1s 0us/step\n","25000 Training sequences\n","25000 Validation sequences\n"]}]},{"cell_type":"markdown","source":["### Model Training"],"metadata":{"id":"GEXdQPdg-X4m"}},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","\n","model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1mmvdi3a-YQ_","outputId":"d7dcaf54-f695-47e6-e7a6-8066c560eb52","executionInfo":{"status":"ok","timestamp":1698951477320,"user_tz":360,"elapsed":89940,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","782/782 [==============================] - 69s 76ms/step - loss: 0.3943 - accuracy: 0.8097 - val_loss: 0.2872 - val_accuracy: 0.8773\n","Epoch 2/2\n","782/782 [==============================] - 20s 26ms/step - loss: 0.1968 - accuracy: 0.9239 - val_loss: 0.3153 - val_accuracy: 0.8746\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7920b23f9000>"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## 20.7 Fine-tuning a Pretrained BERT Model <a name='20.7-fine-tuning-a-pretrained-bert-model'></a>"],"metadata":{"id":"AqW48ZemAIbO"}},{"cell_type":"markdown","metadata":{"id":"qxOWDkU-Z3jj"},"source":["**BERT** (Bidirectional Encoder Representations from Transformers) is a Transformer Network that can be used for variety of NLP tasks such as question answering, text classification, machine translation, etc.\n","\n","In this section we will use a pretrained version of BERT and will fine-tune it for classification of news articles in the AG database (that we used in the previous lecture).\n","\n","TensorFlow Hub is a repository of pretrained machine learning models, and it offers several versions of [BERT](https://tfhub.dev/google/collections/bert/1) such as: [Small BERT](https://tfhub.dev/google/collections/bert/1), [Albert](https://tfhub.dev/google/collections/albert/1), and [BERT Expert](https://tfhub.dev/google/collections/experts/bert/1). The different versions of BERT are optimized for different use cases. In our case, we will use [SmallBERT](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3).\n","\n","To use this model we will need to install the TensorFlow Text library for text processing."]},{"cell_type":"code","source":["!pip install -q tensorflow_text\n","\n","import tensorflow_text as text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BgRpU8iyENG6","outputId":"c349f43a-6e78-4f56-937b-7a4501bb120a","executionInfo":{"status":"ok","timestamp":1698951487442,"user_tz":360,"elapsed":5545,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["The BERT model in TensorFlow Hub has a corresponding text preprocessing model for converting  texts into tokens."],"metadata":{"id":"sMEEA9-yE-8E"}},{"cell_type":"code","source":["import tensorflow_hub as hub\n","import numpy as np\n","\n","bert_handle = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2'\n","preprocessing_model = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"],"metadata":{"id":"drbAfm7HFC59"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IYjYF5jDEEdk"},"source":["The output of the `preprocessing model` has 3 elements:  \n","\n","- `input_word_ids`: token ids of the input sequences.\n","- `input_mask`: has value 1 for all input tokens before padding, and value 0 for the padding tokens.\n","- `input_type_ids`: has different values for segments in text; e.g., if there are 3 sentences in the input text, the tokens in the same sentences will have the same index."]},{"cell_type":"markdown","metadata":{"id":"VGHlY82i_Gde"},"source":["Let's wrap `preprocessing_model` into a `hub.KerasLayer` and test it on a sample sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8iJ0dhE_WHh"},"outputs":[],"source":["preprocess_layer = hub.KerasLayer(preprocessing_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NpLEBPAK_sEn","outputId":"e990afee-7424-4f02-b939-c41b5b5ce1c3","executionInfo":{"status":"ok","timestamp":1698951501845,"user_tz":360,"elapsed":422,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Keys: dict_keys(['input_mask', 'input_word_ids', 'input_type_ids'])\n","Shape: (1, 128)\n","Word Ids: tf.Tensor([  101  6627 11256  1024  1996  6627  5016  6207  2003  2551], shape=(10,), dtype=int32)\n","Input Mask: tf.Tensor([1 1 1 1 1 1 1 1 1 1], shape=(10,), dtype=int32)\n","Type Ids: tf.Tensor([0 0 0 0 0 0 0 0 0 0], shape=(10,), dtype=int32)\n"]}],"source":["sample_news = ['Tech rumors: The tech giant Apple is working on a self driving car']\n","\n","preprocessed_news = preprocess_layer(sample_news)\n","\n","print('Keys:', preprocessed_news.keys())\n","# length of the input sequence\n","print('Shape:', preprocessed_news[\"input_word_ids\"].shape)\n","print('Word Ids:', preprocessed_news[\"input_word_ids\"][0,:10])\n","print('Input Mask:', preprocessed_news[\"input_mask\"][0, :10])\n","print('Type Ids:', preprocessed_news[\"input_type_ids\"][0, :10])"]},{"cell_type":"markdown","metadata":{"id":"BwdYMcP3vb4K"},"source":["### Loading the Dataset"]},{"cell_type":"markdown","metadata":{"id":"nSyKsBxnXEY3"},"source":["The news articles in the AG dataset are classified into 4 categories: World, Sports, Business, and Sci/Tech."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JIKIZDRQwVV","colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["a8692a35a3ed47e0b79297429b0b7533","fadf0867006e4897a7d24e5ef6909060","13d7a1287fe741e1995fb3289dcac900","a6bf212026604d7e976cce4b9fc5b9f6","27f56e0ef16f45049b1df85150dbfd57","4b66b4bb14b847f28b078e35dba8d8f4","3a74e9fd9d95414fad35885dbff8332a","b236ee46fcec4d33a84c48fd54d7d63b","8c718dd0b32e4776b4fd6b436c5c9d26","3cc5f36fc2504001acd3ec7bf50bcc99","3f93c6d6981e466d800101acd61f9928","5e41ed62f2aa47ed976d6582ac76dca2","0d314c1f13c24a949e239dad669b052a","58fe9d2789d34c1d8f0e1cbb50ce9da5","ff87a26092944058a140491ee6b9d2e3","dd05a294aa22400983211d75c729eb78","7160c70142ba420181b657fd5508a6d9","02c726b3564145f7b76fa779c89c85ee","e1f19d1773b74922b6e8e96962d4b079","941a4cf632a54f028c3eab52659262d8","b1fbee7aad324369973571cb56dd170f","567f415da15f4db3b90aa4e6c9c0d146","ae54af7ba3304d23bafd0acf6346cb0c","96cf0b53d0c14a8a8fb298106ce7764c","f979f8943cc7433485bc77d5a9f3abbf","855f989bdffc46c198dda4c05f1c6b11","1d7a77c910514a488e9861da3ddd277a","65de04f440ac493ea431ca1ca3e25acf","31f93846cd174fd2a6a6ab15c1434331","3cacebb69361450da61026b350a078a7","f4000dd43e5f416995aca4f6e336a6f3","0cacd8e2301549069221376a0e274250","6fb3b283ad404f27b69993c458e834bf","3bd388c10296421eb01abfb021e56fff","eae33891c2194fd2b6ccf4b0f71be945","09ff776eff28467fa8081ba97e95304d","b31b919815034d7d875ca6c640d71bc2","1de3114075d74188a55e8b2291435162","ea22c56301a84fe89f771b5951042b44","8bc50cd12bca4a1fac5f1fe780cb2e80","946be6cf142f427c8402502df3bb57a3","e2533ab2cf174aecaff3d691eaeebfc1","0e311627ef16443289a1263697829b31","c3589ad94c66446483cff52b1de9035f","fe32e36d6f354dd2a0ce0505f00fcc23","869322681ba0462d9eb1f3279b877f33","c4be296a3a55424d9fe8e77d0213abc0","517f2a2bb5c34f2fb9f6245980485ad4","d1214052fac141a2bae4af11c2a145c0","9b15a52f018f4e68a6720a96cca3bcfa","2dcf5a23684143608dafcf47e7ecc040","b8cf3265667a4a35bac06c51b2148cae","7ac75837054744beab22b55404723b04","3a4ccea583c24b47a15fb63dab94b1a3","cb0e899fa9c0412b86f001d3e7e4bac8","be6d5d5898674ca69ef75f6c6d822441","80008141999e4cf4bc8ced95f77a0f15","a9bd2593faf04228ac4fd53a49837034","43525510fa3f48f89990569b95f0b956","0eac75dd109048c999766e61019f5519","bfcc8d0c667f4106b3d398f42ec0fd82","7df2ac3a292c49609f562aa6824859af","c8795162d50a447f934c77b743ae35c3","deaef5bc0b7f42cdb36f646c6d19c9ec","a4ac82f061644279bd7d6b19c10638ce","5d1930cb71534345ae9290df53fdc0e2","b7765c3c9b5d4489adee098c771180e6","2378c7944d5f409f8ba53577a70e3e3b","6b1c2269c7234e0a8be0e4ede47e7ea2","b1209cb979f64b379939fa252c4c68d2","8e81c989712f4dd283cff02bd2c34f3e","a93a360ff7a14072bd60d06469db023a","c1cf3b27247a4724864b32d959ef47d8","1281674e14994f9e973bc092bdd82700","30288e1cffd443af907a35f8661906b0","40f10cac5e6443a593c733a5f5207208","5528a57f64254c15b250845618b5e013","0d0f828999934ceeadda89812b8afd4a","07fe1e7223214a329e048d12b57110a3","d39ff0fdaf66411d849acfc66cd411d4","38a56fac8a60493eba32ab70efa8e38e","7195b928e27345fe821f8a54cab8b85d","5a856dfa564042bfad2bba38e80fb30e","06e496c861014acd9860890eaf3ea95d","f49633262bf644d199531c4b084bb4da","0ae0c01ad4f447da8274137110d23f0e","f04a4afe1fcc4d34ae865218c25c709c","6feb11d142284d08b3e70fd6f86408d3"]},"outputId":"2873df6f-eaaf-4082-9206-ac4c0206fdc4","executionInfo":{"status":"ok","timestamp":1698951532768,"user_tz":360,"elapsed":27957,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset 11.24 MiB (download: 11.24 MiB, generated: 35.79 MiB, total: 47.03 MiB) to /root/tensorflow_datasets/ag_news_subset/1.0.0...\n"]},{"output_type":"display_data","data":{"text/plain":["Dl Completed...: 0 url [00:00, ? url/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8692a35a3ed47e0b79297429b0b7533"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Dl Size...: 0 MiB [00:00, ? MiB/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e41ed62f2aa47ed976d6582ac76dca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extraction completed...: 0 file [00:00, ? file/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae54af7ba3304d23bafd0acf6346cb0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bd388c10296421eb01abfb021e56fff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train examples...:   0%|          | 0/120000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe32e36d6f354dd2a0ce0505f00fcc23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteZ0FFY4/ag_news_subset-train.tfrecord*...:  …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be6d5d5898674ca69ef75f6c6d822441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test examples...:   0%|          | 0/7600 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7765c3c9b5d4489adee098c771180e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteZ0FFY4/ag_news_subset-test.tfrecord*...:   …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d0f828999934ceeadda89812b8afd4a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset ag_news_subset downloaded and prepared to /root/tensorflow_datasets/ag_news_subset/1.0.0. Subsequent calls will reuse this data.\n"]}],"source":["import tensorflow_datasets as tfds\n","\n","(train_data, val_data), info = tfds.load('ag_news_subset:1.0.0', #version 1.0.0\n","                                         split=('train', 'test'),\n","                                         with_info=True,\n","                                         as_supervised=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03IBh4d1S1eJ","outputId":"4ba12445-ba1f-483b-ed51-fbd12db946c3","executionInfo":{"status":"ok","timestamp":1698951532769,"user_tz":360,"elapsed":13,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n","Number of training samples\" 120000\n","Number of test samples\" 7600\n"]}],"source":["# Dataset information\n","class_names = info.features['label'].names\n","print('Classes:', class_names)\n","\n","print('Number of training samples\"', info.splits['train'].num_examples)\n","print('Number of test samples\"', info.splits['test'].num_examples)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3e-BTCUfmdr"},"outputs":[],"source":["buffer_size = 1000\n","batch_size = 32\n","\n","# prepare the data\n","train_data = train_data.shuffle(buffer_size)\n","train_data = train_data.batch(batch_size).prefetch(1)\n","val_data = val_data.batch(batch_size).prefetch(1)"]},{"cell_type":"markdown","metadata":{"id":"k1_z6U5WA9Pq"},"source":["### Model Definition with BERT"]},{"cell_type":"markdown","metadata":{"id":"wyPUdJ8aBQmZ"},"source":["The model defined below includes an Input layer, a preprocessing layer to convert the text data into token embeddings, and a layer for the BERT model.\n","\n","Afterward, the output is passed through a classifier head, which includes two dense layers and dropout layers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5sqJYq7wBCWO"},"outputs":[],"source":["# input layer\n","input_text = Input(shape=(), dtype=tf.string)\n","\n","# preprocesing model\n","preprocessing_layer = hub.KerasLayer(preprocessing_model)(input_text)\n","# Bert model, set trainable to True\n","bert_encoder = hub.KerasLayer(bert_handle, trainable=True)(preprocessing_layer)\n","\n","# For fine-tuning use pooled output\n","pooled_bert_output = bert_encoder['pooled_output']\n","\n","# clasifier\n","x = Dense(16, activation='relu')(pooled_bert_output)\n","x = Dropout(0.2)(x)\n","final_output = Dense(4, activation='softmax')(x)\n","\n","# Combine input and output\n","news_model = Model(input_text, final_output)"]},{"cell_type":"markdown","metadata":{"id":"rMP5y7SwKJS9"},"source":["### Model Training"]},{"cell_type":"markdown","metadata":{"id":"oDYULiuLKPcD"},"source":["Let's compile and train the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lh2sQNQ5JlWz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd36df01-c67a-401c-f950-054b8ea65712","executionInfo":{"status":"ok","timestamp":1698953715264,"user_tz":360,"elapsed":2162299,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","3750/3750 [==============================] - 735s 193ms/step - loss: 0.3221 - accuracy: 0.8924 - val_loss: 0.2472 - val_accuracy: 0.9178\n","Epoch 2/3\n","3750/3750 [==============================] - 715s 191ms/step - loss: 0.2130 - accuracy: 0.9302 - val_loss: 0.2213 - val_accuracy: 0.9272\n","Epoch 3/3\n","3750/3750 [==============================] - 714s 190ms/step - loss: 0.1623 - accuracy: 0.9473 - val_loss: 0.2526 - val_accuracy: 0.9242\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x791f163977f0>"]},"metadata":{},"execution_count":17}],"source":["# compile\n","news_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n","                        loss='sparse_categorical_crossentropy',\n","                        metrics=['accuracy'])\n","# train\n","news_model.fit(train_data, epochs=3, validation_data=val_data)"]},{"cell_type":"markdown","metadata":{"id":"DGX0X6PVOE8K"},"source":["### Model Evaluation\n","\n","Finally, let's predict the class of two news articles."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vx4pDaAeOPJi","outputId":"e65d9ced-3038-4aea-a5a7-4dfd2b84891d","executionInfo":{"status":"ok","timestamp":1698953716361,"user_tz":360,"elapsed":1137,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 426ms/step\n","Predicted class: 3\n","Predicted class name: Sci/Tech\n"]}],"source":["sample_news_1 = ['Tesla, a self driving car company is also planning to make a humanoid robot. This humanoid robot appeared dancing in the latest Tesla AI day']\n","\n","predictions_1 = news_model.predict(np.array(sample_news_1))\n","\n","predicted_class_1 = np.argmax(predictions_1)\n","\n","print('Predicted class:', predicted_class_1)\n","print('Predicted class name:', class_names[predicted_class_1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RrCqt3PORiJ","outputId":"1b32e212-514c-4d66-fa30-352998859cb5","executionInfo":{"status":"ok","timestamp":1698953716361,"user_tz":360,"elapsed":4,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 22ms/step\n","Predicted class: 1\n","Predicted class name: Sports\n"]}],"source":["sample_news_2 = [\"In the last weeks, there has been many transfer suprises in footbal. Ronaldo went back to Old Trafford, \"\n","                \"while Messi went to Paris Saint Germain to join his former colleague Neymar.\"\n","                \"We can't wait to see these two clubs will perform in upcoming leagues\"]\n","\n","predictions_2 = news_model.predict(np.array(sample_news_2))\n","\n","predicted_class_2 = np.argmax(predictions_2)\n","\n","print('Predicted class:', predicted_class_2)\n","print('Predicted class name:', class_names[predicted_class_2])"]},{"cell_type":"markdown","source":["## 20.8 Decoder Sub-network <a name='20.8-decoder-sub-network'></a>"],"metadata":{"id":"J9KU9cmkAMwM"}},{"cell_type":"markdown","source":["The Transformer Network in the original paper was designed for machine translation. Differently from the text classification task where for an input text sentence the model predicts a class label, in machine translation for an input text sentence in a source language the model predicts the corresponding text sentence in a target language. Therefore, both the input and output of the model are text sequences. These type of models are called **sequence-to-sequence models**, or oftentimes this term is abbreviated to **seq2seq models**. Beside machine translation, other NLP tasks that employ seq2seq models include question answering, text summarization, dialog generation, and others.\n","\n","The architecture of Transformer Networks designed to handle seq2seq tasks consists of encoder and decoder sub-networks.\n","\n","- **Encoder sub-network** takes a source text sequence as an input, and extracts a useful representation of the text data.\n","- **Decoder sub-network** takes a target text sequence as an input, as well as it receives the intermediate representation from the encoder sub-network. The decoder combines the information from the target sequence and the encoded source sequence, and learns to predict the next word (token) in the target sequence.\n","\n","During the evaluation step, the model does not have access to the target sequence. It is just fed with a source sequence, and the model tries to predict the next word in the target sequence. Afterward, the predicted target sequence is fed back to the decoder, and the next word is again predicted. This step is repeated until the decoder generates an end-of-sequence token.\n","\n","This is shown in the next figure, where the French sequence \"Je suis etudiant\" is translated into \"I am a student\". The decoder outputs one word at each time step until the end-of-sequence is reached.\n","\n","<img src=\"images/transformer_decoding_2.gif\" width=\"700\">\n","\n","*Figure: Decoder block*\n","\n","Such models that predict future values based on past observations under the assumtion that the current value is dependent on previous values are called **autoregressive models**. Autoregressive text generation involves iteratively generating one token at a time, by predicting the next word or token based on the preceding words in the sequence. This approach allows the model to produce coherent and relevant responses by chatbots."],"metadata":{"id":"14Ix6YwoxmyL"}},{"cell_type":"markdown","source":["The architecture of the decoder is similar to the encoder and it is shown in the next figure. The upper part of the decoder is practically the same as the encoder, and it consists of a multi-head attention module with residual connections and layer normalization, followed by a feed-forward network with residual connections and layer normalization. The output of the encoder is passed to the multi-head attention module.\n","\n","The main difference from the encoder is the *masked multi-head attention* module in the lower part of the decoder. This module is inserted between the target sequence (i.e., the output sequence of the decoder) and the multi-head attention module. Masked multi-head attention module applies masking to the next words in the target sequence, so that the network does not have access to those words. That is, during training, if the model needs to predict the 4th word in a sentence, masks will be applied to all words after the 3rd word, so that the model has access only to the words 1, 2, and 3, in order to predict the 4th word. This step ensures that the model uses only the previous steps to predict the word in the next step in the target sequence. This type of mask is also referred to as **causal attention mask**.\n","\n","Finally, the output representations from the decoder are inputted to a linear (dense) layer and a softmax layer, that outputs the probability for the next word in the vocabulary learned from the training dataset.\n","\n","And also note the marks `Nx` in the figure. They indicate that the shown encoder and decoder blocks are repeated multiple times in the network. In the original Transformer Network, there are 6 encoder blocks, and similarly there are 6 decoder blocks. Introducing multiple blocks in the encoder and decoder sub-networks increases the learning ability as it allows the model to learn more abstract representations.\n","\n","<img src=\"images/transformer.png\" width=\"700\">\n","\n","*Figure: Transformer Network*"],"metadata":{"id":"zDTIt98p6Pzm"}},{"cell_type":"markdown","source":["Note that Recurrent Neural Networks are also a type of seq2seq models. Transformer Networks have several advantages over RNN, due to the ability to inspect entire text sequences at once, capture context in long sequences, are parallelizable, and are more powerful in general. Conversely, RNN have access only to the next token in a sequence (have difficulty finding correlations in long sequences because the information needs to pass through many processing steps), can not perform parallel computations (are slow to train), and the gradients can become unstable."],"metadata":{"id":"14HTvHQyFCqr"}},{"cell_type":"markdown","source":["## 20.9 Vision Transformers <a name='20.9-vision-transformers'></a>"],"metadata":{"id":"b_PWJPleANCx"}},{"cell_type":"markdown","source":["After the initial success of Transformer Networks in NLP, recently they have been adapted for computer vision tasks as well. The initial Transformer model for vision tasks proposed in 2021 was called **Vision Transformer (ViT)**.\n","\n","The architecture of ViT is very similar to the Transformers used in NLP. However, Transformer Networks were designed for working with sequential data, while images are spatial data types. To consider each pixel in an image as a sequential token would be impractical and too time-consuming. Therefore, ViT splits images into a set of smaller image patches (16x16 pixels), and it uses the sequence of image patches as inputs to the model (i.e., each image patch is considered a token). Each image patch is first flattened to one-dimensional vector, and those vectors are afterward passed through a dense layer to learn lower-dimensional embeddings for each patch. Positional embeddings and class embeddings are added, and the sequences are fed to a standard transformer encoder. Class embeddings are vectors that correspond to different classes in the dataset. The encoder block in ViT is identical to the encoder in the original Transformer Network. The steps are depicted in the figure below.\n","\n","<img src=\"images/vision_transformer.gif\" width=\"700\">\n","\n","*Figure: Vision Transformer*"],"metadata":{"id":"vj37AnHbLuWa"}},{"cell_type":"markdown","source":["The authors trained 3 versions of ViT, called Base (12 encoder blocks, 768 embeddings dimension, 86M parameters), Large (24 encoder blocks, 1,024 embeddings dimension, 307M parameters), and Huge (32 encoder blocks, 1,280 embeddings dimension, 632M parameters).\n","\n","Various other versions of vision transformers were introduced recently, which include MaxViT (Multi-axis ViT), Swin (Shifted Window ViT), DeiT (Data-efficient image Transformer), T2T-ViT (Token-to-token ViT), and others. These models achieved higher accuracy on many vision tasks in comparison to Convolutional Neural Networks. The following figure shows the accuracy on ImageNet.\n","\n","<img src=\"images/imagenet_accuracy.png\" width=\"500\">\n","\n","*Figure: Accuracy on the ImageNet dataset*"],"metadata":{"id":"Msq8PaT_O-hh"}},{"cell_type":"markdown","metadata":{"id":"vweobvFVe4RB"},"source":["## References <a name='references'></a>\n","\n","1. The Illustrated Transformer, Jay Alammar, available at: [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/).\n","2. Keras Examples, Text classification with Transformer, available at: [https://keras.io/examples/nlp/text_classification_with_transformer/](https://keras.io/examples/nlp/text_classification_with_transformer/).\n","3. Using Pretrained BERT for Text Classification, Jean de Dieu Nyandwi, available at: [https://github.com/Nyandwi/machine_learning_complete/blob/main/9_nlp_with_tensorflow/5_using_pretrained_bert_for_text_classification.ipynb](https://github.com/Nyandwi/machine_learning_complete/blob/main/9_nlp_with_tensorflow/5_using_pretrained_bert_for_text_classification.ipynb).\n","4. Deep Learning with Python, Francois Chollet, Second Edition, Manning Publications, 2021.\n","5. TensorFlow Tutorials, Neural Machine Translation with a Transformer and Keras, available at [https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer).\n","6. How the Vision Transformer (ViT) Works in 10 Minutes: An Image is Worth 16x16 Words, Nikolas Adaloglou, available at [https://theaisummer.com/vision-transformer/](https://theaisummer.com/vision-transformer/).\n"]},{"cell_type":"markdown","metadata":{"id":"bDbzcXmWe4RB"},"source":["[BACK TO TOP](#top)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a8692a35a3ed47e0b79297429b0b7533":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fadf0867006e4897a7d24e5ef6909060","IPY_MODEL_13d7a1287fe741e1995fb3289dcac900","IPY_MODEL_a6bf212026604d7e976cce4b9fc5b9f6"],"layout":"IPY_MODEL_27f56e0ef16f45049b1df85150dbfd57"}},"fadf0867006e4897a7d24e5ef6909060":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b66b4bb14b847f28b078e35dba8d8f4","placeholder":"​","style":"IPY_MODEL_3a74e9fd9d95414fad35885dbff8332a","value":"Dl Completed...: 100%"}},"13d7a1287fe741e1995fb3289dcac900":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b236ee46fcec4d33a84c48fd54d7d63b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c718dd0b32e4776b4fd6b436c5c9d26","value":1}},"a6bf212026604d7e976cce4b9fc5b9f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cc5f36fc2504001acd3ec7bf50bcc99","placeholder":"​","style":"IPY_MODEL_3f93c6d6981e466d800101acd61f9928","value":" 1/1 [00:11&lt;00:00, 11.69s/ url]"}},"27f56e0ef16f45049b1df85150dbfd57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b66b4bb14b847f28b078e35dba8d8f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a74e9fd9d95414fad35885dbff8332a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b236ee46fcec4d33a84c48fd54d7d63b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"8c718dd0b32e4776b4fd6b436c5c9d26":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3cc5f36fc2504001acd3ec7bf50bcc99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f93c6d6981e466d800101acd61f9928":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e41ed62f2aa47ed976d6582ac76dca2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d314c1f13c24a949e239dad669b052a","IPY_MODEL_58fe9d2789d34c1d8f0e1cbb50ce9da5","IPY_MODEL_ff87a26092944058a140491ee6b9d2e3"],"layout":"IPY_MODEL_dd05a294aa22400983211d75c729eb78"}},"0d314c1f13c24a949e239dad669b052a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7160c70142ba420181b657fd5508a6d9","placeholder":"​","style":"IPY_MODEL_02c726b3564145f7b76fa779c89c85ee","value":"Dl Size...: 100%"}},"58fe9d2789d34c1d8f0e1cbb50ce9da5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1f19d1773b74922b6e8e96962d4b079","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_941a4cf632a54f028c3eab52659262d8","value":1}},"ff87a26092944058a140491ee6b9d2e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1fbee7aad324369973571cb56dd170f","placeholder":"​","style":"IPY_MODEL_567f415da15f4db3b90aa4e6c9c0d146","value":" 11/11 [00:11&lt;00:00,  1.06 MiB/s]"}},"dd05a294aa22400983211d75c729eb78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7160c70142ba420181b657fd5508a6d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02c726b3564145f7b76fa779c89c85ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1f19d1773b74922b6e8e96962d4b079":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"941a4cf632a54f028c3eab52659262d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1fbee7aad324369973571cb56dd170f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"567f415da15f4db3b90aa4e6c9c0d146":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae54af7ba3304d23bafd0acf6346cb0c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96cf0b53d0c14a8a8fb298106ce7764c","IPY_MODEL_f979f8943cc7433485bc77d5a9f3abbf","IPY_MODEL_855f989bdffc46c198dda4c05f1c6b11"],"layout":"IPY_MODEL_1d7a77c910514a488e9861da3ddd277a"}},"96cf0b53d0c14a8a8fb298106ce7764c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65de04f440ac493ea431ca1ca3e25acf","placeholder":"​","style":"IPY_MODEL_31f93846cd174fd2a6a6ab15c1434331","value":"Extraction completed...: 100%"}},"f979f8943cc7433485bc77d5a9f3abbf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cacebb69361450da61026b350a078a7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4000dd43e5f416995aca4f6e336a6f3","value":1}},"855f989bdffc46c198dda4c05f1c6b11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cacd8e2301549069221376a0e274250","placeholder":"​","style":"IPY_MODEL_6fb3b283ad404f27b69993c458e834bf","value":" 4/4 [00:11&lt;00:00, 11.92s/ file]"}},"1d7a77c910514a488e9861da3ddd277a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65de04f440ac493ea431ca1ca3e25acf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31f93846cd174fd2a6a6ab15c1434331":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cacebb69361450da61026b350a078a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f4000dd43e5f416995aca4f6e336a6f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0cacd8e2301549069221376a0e274250":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fb3b283ad404f27b69993c458e834bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bd388c10296421eb01abfb021e56fff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eae33891c2194fd2b6ccf4b0f71be945","IPY_MODEL_09ff776eff28467fa8081ba97e95304d","IPY_MODEL_b31b919815034d7d875ca6c640d71bc2"],"layout":"IPY_MODEL_1de3114075d74188a55e8b2291435162"}},"eae33891c2194fd2b6ccf4b0f71be945":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea22c56301a84fe89f771b5951042b44","placeholder":"​","style":"IPY_MODEL_8bc50cd12bca4a1fac5f1fe780cb2e80","value":"Generating splits...: 100%"}},"09ff776eff28467fa8081ba97e95304d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_946be6cf142f427c8402502df3bb57a3","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2533ab2cf174aecaff3d691eaeebfc1","value":2}},"b31b919815034d7d875ca6c640d71bc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e311627ef16443289a1263697829b31","placeholder":"​","style":"IPY_MODEL_c3589ad94c66446483cff52b1de9035f","value":" 2/2 [00:11&lt;00:00,  4.98s/ splits]"}},"1de3114075d74188a55e8b2291435162":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ea22c56301a84fe89f771b5951042b44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bc50cd12bca4a1fac5f1fe780cb2e80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"946be6cf142f427c8402502df3bb57a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2533ab2cf174aecaff3d691eaeebfc1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e311627ef16443289a1263697829b31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3589ad94c66446483cff52b1de9035f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe32e36d6f354dd2a0ce0505f00fcc23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_869322681ba0462d9eb1f3279b877f33","IPY_MODEL_c4be296a3a55424d9fe8e77d0213abc0","IPY_MODEL_517f2a2bb5c34f2fb9f6245980485ad4"],"layout":"IPY_MODEL_d1214052fac141a2bae4af11c2a145c0"}},"869322681ba0462d9eb1f3279b877f33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b15a52f018f4e68a6720a96cca3bcfa","placeholder":"​","style":"IPY_MODEL_2dcf5a23684143608dafcf47e7ecc040","value":"Generating train examples...:  94%"}},"c4be296a3a55424d9fe8e77d0213abc0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8cf3265667a4a35bac06c51b2148cae","max":120000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ac75837054744beab22b55404723b04","value":120000}},"517f2a2bb5c34f2fb9f6245980485ad4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a4ccea583c24b47a15fb63dab94b1a3","placeholder":"​","style":"IPY_MODEL_cb0e899fa9c0412b86f001d3e7e4bac8","value":" 112761/120000 [00:10&lt;00:00, 11467.57 examples/s]"}},"d1214052fac141a2bae4af11c2a145c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"9b15a52f018f4e68a6720a96cca3bcfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dcf5a23684143608dafcf47e7ecc040":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8cf3265667a4a35bac06c51b2148cae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ac75837054744beab22b55404723b04":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a4ccea583c24b47a15fb63dab94b1a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb0e899fa9c0412b86f001d3e7e4bac8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be6d5d5898674ca69ef75f6c6d822441":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80008141999e4cf4bc8ced95f77a0f15","IPY_MODEL_a9bd2593faf04228ac4fd53a49837034","IPY_MODEL_43525510fa3f48f89990569b95f0b956"],"layout":"IPY_MODEL_0eac75dd109048c999766e61019f5519"}},"80008141999e4cf4bc8ced95f77a0f15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfcc8d0c667f4106b3d398f42ec0fd82","placeholder":"​","style":"IPY_MODEL_7df2ac3a292c49609f562aa6824859af","value":"Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteZ0FFY4/ag_news_subset-train.tfrecord*...:  76%"}},"a9bd2593faf04228ac4fd53a49837034":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8795162d50a447f934c77b743ae35c3","max":120000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_deaef5bc0b7f42cdb36f646c6d19c9ec","value":120000}},"43525510fa3f48f89990569b95f0b956":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4ac82f061644279bd7d6b19c10638ce","placeholder":"​","style":"IPY_MODEL_5d1930cb71534345ae9290df53fdc0e2","value":" 91278/120000 [00:00&lt;00:00, 353132.57 examples/s]"}},"0eac75dd109048c999766e61019f5519":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"bfcc8d0c667f4106b3d398f42ec0fd82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7df2ac3a292c49609f562aa6824859af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8795162d50a447f934c77b743ae35c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deaef5bc0b7f42cdb36f646c6d19c9ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4ac82f061644279bd7d6b19c10638ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d1930cb71534345ae9290df53fdc0e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7765c3c9b5d4489adee098c771180e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2378c7944d5f409f8ba53577a70e3e3b","IPY_MODEL_6b1c2269c7234e0a8be0e4ede47e7ea2","IPY_MODEL_b1209cb979f64b379939fa252c4c68d2"],"layout":"IPY_MODEL_8e81c989712f4dd283cff02bd2c34f3e"}},"2378c7944d5f409f8ba53577a70e3e3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a93a360ff7a14072bd60d06469db023a","placeholder":"​","style":"IPY_MODEL_c1cf3b27247a4724864b32d959ef47d8","value":"Generating test examples...:   0%"}},"6b1c2269c7234e0a8be0e4ede47e7ea2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1281674e14994f9e973bc092bdd82700","max":7600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_30288e1cffd443af907a35f8661906b0","value":7600}},"b1209cb979f64b379939fa252c4c68d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40f10cac5e6443a593c733a5f5207208","placeholder":"​","style":"IPY_MODEL_5528a57f64254c15b250845618b5e013","value":" 0/7600 [00:00&lt;?, ? examples/s]"}},"8e81c989712f4dd283cff02bd2c34f3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"a93a360ff7a14072bd60d06469db023a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1cf3b27247a4724864b32d959ef47d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1281674e14994f9e973bc092bdd82700":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30288e1cffd443af907a35f8661906b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40f10cac5e6443a593c733a5f5207208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5528a57f64254c15b250845618b5e013":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d0f828999934ceeadda89812b8afd4a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07fe1e7223214a329e048d12b57110a3","IPY_MODEL_d39ff0fdaf66411d849acfc66cd411d4","IPY_MODEL_38a56fac8a60493eba32ab70efa8e38e"],"layout":"IPY_MODEL_7195b928e27345fe821f8a54cab8b85d"}},"07fe1e7223214a329e048d12b57110a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a856dfa564042bfad2bba38e80fb30e","placeholder":"​","style":"IPY_MODEL_06e496c861014acd9860890eaf3ea95d","value":"Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteZ0FFY4/ag_news_subset-test.tfrecord*...:   0%"}},"d39ff0fdaf66411d849acfc66cd411d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f49633262bf644d199531c4b084bb4da","max":7600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ae0c01ad4f447da8274137110d23f0e","value":7600}},"38a56fac8a60493eba32ab70efa8e38e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f04a4afe1fcc4d34ae865218c25c709c","placeholder":"​","style":"IPY_MODEL_6feb11d142284d08b3e70fd6f86408d3","value":" 0/7600 [00:00&lt;?, ? examples/s]"}},"7195b928e27345fe821f8a54cab8b85d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"5a856dfa564042bfad2bba38e80fb30e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06e496c861014acd9860890eaf3ea95d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f49633262bf644d199531c4b084bb4da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ae0c01ad4f447da8274137110d23f0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f04a4afe1fcc4d34ae865218c25c709c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6feb11d142284d08b3e70fd6f86408d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}